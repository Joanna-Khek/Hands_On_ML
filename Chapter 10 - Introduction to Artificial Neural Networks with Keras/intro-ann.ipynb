{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Perceptron is one of the simplest ANN architectures.\n",
    "- It is composed of a single layer of threshold logic unit (TLUs.)\n",
    "- TLU computes a weighted sum of its inputs, then applies a step function to that sum and outputs the result.\n",
    "- When all neurons in a layer are connected to every neuron in the previous layer, it is called a fully connected layer.\n",
    "- The fully connected layer is computed this way\n",
    "$h_{W,b}(X) = \\phi(XW + b)$  \n",
    "- The Percetron is fed one training instance at a time and for each instance it makes its prediction. \n",
    "- For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.\n",
    "- The decision boundary of each output neuron is linear so Perceptrons are incapable of learning complex patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is Gradient Descent using an efficient technique for computing the gradients automatically in just two passes through the network (one forward, one backward).\n",
    "- Backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter.\n",
    "- This means that it can find out how each connection weight and each bias term should be tweaked in order to reduce the error.\n",
    "- Once it has these gradients, it just performs a regular Gradient Descent step and the whole process is repeated until the network converges to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handles one mini-batch at a time and goes through the full training set multiple times. Each pass is called an epoch\n",
    "- Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer\n",
    "- The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch).\n",
    "- The result is passed on to the next layer, its outpuet is computed and passed to the next layer and so on until we get the output of the last layer (output layer)\n",
    "- The algorithm measures the network's output error (i.e uses loss function that compares the desired output and actual output of network and returns some measure of error)\n",
    "- Algorithm compares how much each output connection contributed to the error using the help of chain rule, which makes this step fast.\n",
    "- Algorithm then measures how much of these error contributions came from each connection in the layer below, again using chain rule until it reaches the input layer.\n",
    "- This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
    "- Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the gradients it just computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection. Then it finally tweaks the connection weights to reduce the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure10.8](images/figure10.8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we chain several linear transformations, all we get is linear transformation.    \n",
    "Example: $f(x) = 2x + 3$ and $g(x) = 5x - 1$.     \n",
    "$f(g(x)) = 2(5x - 1) + 3 = 10x + 1$      \n",
    "\n",
    "- If we don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer and we can't solve very complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regression MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we are just predicting a single value, we will only need a single output neuron\n",
    "- Loss function to use during training is typically the mean squared error (MSE) but if we have a lot of outliers in the training set, we can use mean absolute error (MAE) or Huber loss, which is a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table10.1](images/_table10.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **binary classification problem**, we only need a single output neuron using the logistic activate function (output will be a number between 0 and 1 which we can interpret as the probability of the positive class)\n",
    "- For **multilabel binary classification tasks**, example: spam or ham and simultaneously predicts whether it is an urgent or nonurgent email, we need two output neurons both using the logistic activation function. The first would output the probability that the mail is spam and second would output the probability that it is urgent.\n",
    "- For **multiclass classification tasks**, we will have one output neuron per class and use softmax activation function for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table10.2](images/_table10.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras is a high-level deep learning API that allows us to easily build, train, evaluate and execute all sorts of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np \n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In fashion MNIST, we have 70,000 grayscale images of 28 x 28 pixels each with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to split the training set into train set and validation set\n",
    "- Since we are going to train the neural network using Gradient Descent, we must scale the input features.\n",
    "- We will scale the pixel intensities down to the 0-1 range by dividing them by 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the label of first instance\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x345.6 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Creating Model using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Build first layer and add it to the model.\n",
    "# It is a flatten layer whose role is to convert each input image into a 1D array. (reshape(-1, 1))\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# Add a dense hidden layer with 300 neurons. Use ReLU activation function\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "\n",
    "# The last layer we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive)\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a list of layers when creating the `Sequential` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first hidden layer has 784x300 connection weights + 300 bias terms, which adds up to 235500 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAIECAYAAAAep4eDAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdf2wb530/8PfFTrou21dMEJBpnMhYUUgw0FVtsnXy2iGL7bWzu6NXTL/oRun+kNwTEKN2rQG1QcIwZDgtQC0GXEACKaDwBJiUZAyYiCUYZmmw/6jYAB2odkNhYTBAzQhA/hMeAqzNz+f7h/ucj+SROpLHO/L4fgFE5OeO9zx3z+U+vOeeex5FCCFAREREfrX2mNclICIiovZisCciIvI5BnsiIiKfY7AnIiLyuf2VCVtbW/jHf/xHL8pCRERELVpbW6tKq7qz/9///V/cunXLlQJR93nw4AHPD5uy2Syy2azXxaAudOvWLTx48MDrYlCXqXd9rrqzl6x+GRCtrq5ifHyc54cNo6OjAPj/EjVOURScO3cOY2NjXheFuoi8PlvhM3siIiKfY7AnIiLyOQZ7IiIin2OwJyIi8jkGeyIiIp9zNNhns1nMzMxAURT83d/9HS5cuIBwOOxkFr4Qi8UQi8W8LoaneAyIiNxT89W7Rm1ubuLo0aPI5/NYWFjAU089hX/+539ualu6riMQCMA8IZ9Vmpt0Xcevf/1r/OpXv0Imk8H6+ron5XCK18ezE/AYEFGvcCzYy3eJ+/v7AQDvvfceFEVpalt37961leameDwOALhy5UrL25qbm2t5G63y+njyGBARucexYL+4uOjIdnRdRzKZ3DPNbTI4ORHsvdYJx9NrPAZE1EtafmavKErZHXzlvyvJi6xcLxaLoVgsGsvj8TgymUzZtqzSpGKxiPn5eSiKgnA4jM3NTSM9nU4bfQYymYyxzu7ubqu73bTKclmlWZW1WCwik8kY68hjODMzg52dHWNb8vhY1YlMq3c83dDJx4B9CYjIl0SFlZUVYZG8JwBV37NK0zRNABCFQkHk83kBQGia1tS2CoWCUFVVpFIpIYQQGxsbAoDI5XJCVVXjO1tbW0IIUTO/VvezEeZyWaXVKqtcbl6nVCoZx/PevXtCiIfHpHL7clvmtGb3o9nzw6yTj0E0GhXRaLSl/ZNGRkbEyMiII9ui3gJArKyseF0M6jJ1rs+rrgf7aDRaFmztfs8qLZVKWa4nL9Z2t9OIVr/fSLnsBKdcLicAiHg83vK27HAi2Nsto1VaJxwDuxjsqVkM9tSMesHe9ffs5+bmsLCwgN3dXczPz7e0rZs3bwKobqL1w3N1u4aGhgAAs7OzHpfEOzwGRET1eTKoTjKZxOuvvw5VVVvajnzmKoSo+hAREdFDjvXGtyudTuP06dPI5/PGa3qt2tnZwcDAgCPb6laapnldBM/xGBARWXP9zj4SiQCAI4E+kUgAAJaXl6HrOoBHvfN7heyFfuLECY9L4h0eAyKi+hwJ9tvb28bf8sJrfp3O/Ldsut/d3S17XcpqHXPgtko7efIkgIfP6AOBABRFQSgUwujoaNn25A8B+d/K/Owyf9/8dyOsjkujZU2n08Y6y8vLUFW17JGIvMOVxzebzRrLZmZmAFgfT7d08jHgq3dE5EsN9OazBNPrUPU+kuw5HY1GRaFQMHrn5/P5muvUShPi4StV0WjUeEVLbscq/1plamU/G2W3XPXSzK8WJhIJUSqVyvLI5/PG8vX1dSGEMF5R3Ot47sWJ3vidfAz46h11ArA3PjWhXm98RYjy3myrq6sYHx9nJ7cOJN828LJuvD4/OuEY2DU6Ogrg0VDSRHYpioKVlRWMjY15XRTqInWuz2uc4paIiMjnGOy7RK0+EL2Ex4CIqDk9H+zNA/LU+7i1nVpCoZDl372kF46BnfOl19446WTz8/M1O+s69f++HXbz4rnTGbw4b3o+2AuLAXmsPm5tx+72e1EvHYNa+1gsFnHp0iU8+eSTZZNJWXHyx2a77e7uYmZmxpjYSE5oVUlOhCQnSZJvZXiV37FjxzA5OWnZ0uTFeVovT547nXPueHLeNNCbj4jnRwOa6Y2POm95lEoloapq2SRAcn6IWm8QyEmBGnnjwm2lUsl4Y8K8TzJNisfjxpsYQljPieBFfltbW0JV1ao3QqR6dVoLGuyNv1cePHc679xpx3njykQ41Bt4ftjndLCPx+OWF2b5HTn7o9XyTlZ5oRSisYmSVFX1PD9N02oGjk4I9jx3OvPccfq8YbAnx/D8sM/JYC/vsjY2Niy/I+8krC7a9VoK5J0JfjdegbyLKxQKIpVKGRen9fV142JlHhNDrivzV1XVsoyNAqqnopZ5VE6BLO+evMxPTq9tdRfsdbB3+typd97I/Hju2MvP6fOGwZ4cw/PDPieDvbxgVl4s5XeEEMbgUpUXlFr1paqqSCQSQoiHF11VVY1mRTkgkdUFy3xhk9+TgUJevFq5iJZKJcumUfM+bm1tlQ2Q1Aon8pPHxu6d316cDPZOnzv1zhu5nOeOvfycPm8Y7MkxPD/sczLYywtHre8IIcqC9L1796qWm1ndUWxtbZXd4dlpnpR3eJXrtDIK4cbGRt1nmZqmGXnUWsft/ORF36pJ1utg7+S5Y+e8qVUenjvV6zh93jQV7Pnhh5/WP04F+1rpcpkkm2xVVTUuyFbfkxchM3nhkc2vVnlWppnv4io/zTJ3JKsUj8dFKpUSpVJJRKPRuhdat/Nrpu5qAZwL9k6eO3bOm1p58tyxzs/J86ap4XJXVlZAVGlrawvXrl3j+WHDm2++ieeff76h4XJrDQdcb5hgRVHK0re3t/HlL38ZqqpieXkZgUDA9vbM6VbrVKY5PXxxOp3G+++/j+npactlkUgEpVIJfX192NnZweDgIBKJhOX6bufXTN3V0uhwuXudH/WWNXLu2N1Hnjv283PyvKk3XC6b8akhPD/sc7IZv1a6XFZJPqet1YQr76oqnyMCj56rWuVZmSb/bW76bVYul6vbhFuZt7yjbPZ8dDq/ZuquXt5u39lL9c4dO+dNrTx57jR2fjRTvnp39j0/qA5RN4jH4wDsT62sqipSqRSuXLliufzUqVMAgPv37xtpcttyAh87EokEAGB5edn4fjOjtBWLRdy+fRtzc3NG2vb2tjEdMYCyKYwBoK+vzzLdy/yi0WjDZWk3J88dp84bgOeOmSvnTQO/DIh4fjTAjd74ew18UuvOXnbIMj+fTaVSxt2Z3C4A4zmj+e7E/IqeTDN/zOWsHGCkkuyVbbUdcy9l2TlMdgSTHcMqX9dyOz8hurM3fjPnzl7njXm7rZ47e9Wj3IZTdel2fkKwNz51MJ4f9rXjPXtzZyCrC46VWgOHFAoFkUgkjO/KzkRW266VJsTDC5YMDJqmVQWVaDQqNE2rWQ7Z6cvqU9nEu7GxYayvaZpl4HU7PyEeXcw7+T17p86deueN1bZrpQlR/9zZqx6FcLYu3c5PCOfPGwZ7cgzPD/vaMYJeo8N7dpJGRyvrpvyi0WjHj6DXrecOzxv7+MyeyAempqZw584dZLNZr4vSsGw2i4sXL/oyv+3tbWxvb2NqasqV/JrRrecOzxvnMNgTdYm+vj4sLS3h6tWr2N7e9ro4tm1ubuLpp5/G8PCw7/Lb2dnB4uIilpaWjE5Ynagbzx2eN87yNNg7Ped7q3RdL8u/08pH1XXUbdu3q9a5FgwGsby8jNu3b3tQquYcOXIEAwMDvswvk8ng8uXLCAaDVcu8uF7Uy7Pbzh2eN87yNNgLIVAqlYx/l0olxwZYaMbdu3fL/i2EQKFQMP7tdfmouo66bft7EUKUfaz09fXh/PnzLpeMrJw/f97ygg3Yq0un2M2L505n8OK88bwZ39yE4WUzmK7rSCaTVenmCunkZrpeUKuOumX7RERe8TzY11IsFpFOpxEOhwE8bPZQFAXhcBi7u7vGOplMxlgnmUxCURTMzMxgZ2cHACyb363S4vE4MplM2fJGyWAhvx+LxYxBIsx5mgeNMC+T+yX3TS4Lh8PY3Nys2mdd1zEzM4NYLNZwWd2m6zrS6bSxr8lkEsViEUBrdeTGORCLxbriGBMR1dRA1/22gcUrBnamSZTLzeuUSiXj3cZ79+6VDdwgye1U5mmVVi+9ksy3UChUlVW+T1k577Hc18r5oGtN/Vh5XHK5nOU226XZ86PetJit1JEb50A0Gm1qJq5mXr0jEqLxV++IhOiC9+wbCbK1LvZmuVxOAI+mDbR7UW812MtBGWp9T47QZB44IpfLlU0NKcTeUz/K7ToxRWOjmjk/nJpOtZE0p8+BZjDYU7MY7KkZPfee/dDQEABgdnbW1Xzn5uawsLCA3d1dy/Gdjx07BgD4t3/7NyPt9u3b+PM///Oy9W7evAmguqm5cqzqbulDIGd9M/d/OHToEIBH++o0r84BIqJO5Mtg76VkMonXX3/dcsKDoaEhaJqG06dPQ9d16LqO//mf/0F/f3/ZevK5sajolSm69E2AxcXFqjT5Q0XuKxERtY+vg72maa7kI2c7SqfTOH36NH7yk5/UfF9Tluntt9/G3bt38d3vfrfmdmUHs24nf/jIDnlm7a4jt84BIqJO5stgL4PkiRMn2p5XNpvFyy+/DACIRCIAUHWnbibv7iORCJLJpOVoTU5N/dgpnJwW0y43zwEiok7nebA3z7Fs/tt8FyjTay0HHt5Vy3WWl5ehqqpxRynv7mQAMI8PbTUHsTmwWt2NStlsFocPHzaeP8vv7+7ult2VV25D3s3Xmtv45MmTAB4+ow8EAlAUBaFQCKOjo3XL06mOHz8OVVVx9epVo/xvv/02NE3DkSNHALRWR1K7zgG+ekdEXa+B3nyOQ42pAq2W7ZVmfi0tkUiU9VTP5/PGMjlvsHy1zdxDXPbgjkajNedatvrIvCq/L3vnV075KfOvnBLRrNbUj+Z83Z4NSojmz4+9psVsto6EaO85IARfvSP3gb3xqQn1euMrQpT3+lpdXcX4+HjXdAaTPdW7pbzAwzvPH/7wh1hYWPC6KA3rxPOjU88B+YhCvo1AZJeiKFhZWcHY2JjXRaEuUuf6vOZ5M34vWl1dbduzaiIiokpdHezNz687/Vl2LBYrGxZXPqum1nTTOUBE5JX9XhegFaFQqOzvTmvGNZM99BOJBKanpz0ujX900zlAROSVrg723XRhn56eZpBvg246B4iIvNLVzfhERES0NwZ7IiIin2OwJyIi8jkGeyIiIp+r2UFvdXXVzXJQl9ja2gLA88OOBw8eAOCxoubI/9eI7Kp3ztQcQY+IiIi6j9UIelXBnoj8qxOHOyaituNwuURERH7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+t9/rAhBRexSLRfz0pz8tS/vlL38JAPjxj39clv70009jenratbIRkbsUIYTwuhBE5LyPP/4Yzz77LN577z08/vjjNdf74IMP8L3vfQ+Li4sulo6IXLTGZnwin9q/fz8ikQj27duHDz74oOYHAE6dOuVxaYmonRjsiXwsEongo48+qrvOs88+i69//esulYiIvMBgT+Rjhw8fxvPPP19z+RNPPIHJyUk89hgvBUR+xv/DiXxMURS8+uqrNZ/Zf/jhh4hEIi6XiojcxmBP5HP1mvI///nP4ytf+YrLJSIitzHYE/ncl770JQwODlalP/HEE/jud7/rQYmIyG0M9kQ9YHJysqop/8MPP8TExIRHJSIiNzHYE/WAV199FR9//LHxb0VRMDQ0hIGBAQ9LRURuYbAn6gEHDx7Eiy++CEVRAAD79u1jEz5RD2GwJ+oRr732Gvbt2wcA+OSTTzA2NuZxiYjILQz2RD1ibGwMn376KRRFwde+9jUcOHDA6yIRkUsY7Il6xLPPPouXX34ZQgg24RP1GN9MhCOfRRIRETlhZGQEa2trXhfDCWu+muL27NmzOHz4sNfF8KXx8XEeXxu2trZw7do1rKyseF0US7/5zW+QSCTw/e9/3+ui+N6bb74JADh37pzHJaFmyPrzC18F+8OHD7PTUZuMj4/z+Np07dq1jj5Of/VXf4XnnnvO62L4nrwj7ORzgWrzyR29gc/siXoMAz1R72GwJyIi8jkGeyIiIp9jsCciIvI5BnsiIiKf6/lgn81mMTMzA0VRMDMzg3//939HOp1GOBz2umi+E4vFEIvFvC4GEVHP8dWrd43a3NzE0aNHkc/nsbCwgHQ6jW984xtNbUvXdQQCAZjHKLJKc5Ou6/j1r3+NX/3qV8hkMlhfX/ekHJ3E6zohIvJCTwd7+R5lf38/AGBiYgITExNNjcZ39+5dW2luisfjAIArV654Wg5pbm7O6yJ4XidERF7o6WC/uLjoyHZ0XUcymdwzzW0yuHZKsPdaJ9QJEZEXevKZvaIoZXfvlf+2IgOFXDcWi6FYLAJ4eAedyWTKtmWVJhWLRczPz0NRFITDYWxubpYtM/cZyGQyxnq7u7vOHAAPVO6XVVqtfS0Wi8hkMsZ6sh5mZmaws7MD4NExtqpXmVarTtiXgIh8T/gEALGystLwd6wOgVW6pmkCgCgUCiKfzwsAQtO0ut+xSisUCkJVVZFKpYQQQmxsbAgAIpfLCSGEUFXV+N7W1pYQQljm58R+NrqNRo+vmXm/rNLq7atcx7xeqVQy6uTevXuiUChUbV9uy5xmdSyi0aiIRqNN75vZyspKy8ea/GFkZESMjIx4XQxqks/qb7Un7+yb8cwzz0DTNASDQeMZfzOPATY3N5HJZDAxMQEAOHLkCADg1q1bAFDWiW54eBgAWsqvU1h1DrS7r8LUmU6u19fXB03TADxsEQgGg1Xbl9vay9zcXEf0JyAiahcGe5vm5uawsLCA3d1dzM/PN72dmzdvAqhuYuZz9cYNDQ0BAGZnZz0uCRFRZ2Owb0AymcTrr78OVVWb3oZ8ZiyEqPoQERG1Q0/3xm9EOp3G6dOnkc/nbTcP17Ozs4OBgQEHSkayOZ+IiKzxzt6mSCQCwP5z4FoSiQQAYHl5GbquA3jUO58aI3vinzhxwuOSEBF1tp4N9tvb28bfMmgAMF6nq/xbNt3v7u5ari+XmwO3VdrJkycBPHxGHwgEoCgKQqEQRkdHq/KUPwbkfyuX22H+rvlvt1kd12b2NZ1OG+stLy9DVVXjOMs7fFk/2WzW+N7MzAwA6zrhq3dE5Hc9GewVRcGXv/xl49+Dg4NGR7lQKGSkm/+WvbWTySQCgQCi0Sg0TcNvf/vbsuXXr1/H5ORkzbRgMIh8Po9oNArgYYAyPxow5xkIBMr+W7nczn6avyt/XHjB6rg2s6+HDh1COBxGIBBAf38/lpeXjWUXLlyAqqoYHBxEJpPB8PAwVFVFKpXC5cuXAVjXCRGR3ynCJz3DFEXBysoKxsbGvC6KL3l9fOWPlE4/XVdXVzE+Pt7x5aT2k611clhu6i4+q7+1nryzJyIi6iUM9tTxavWj8CN21uwc8/Pznvdz4bnQPK/rr9Mw2Hch84A89T5+Uasfhd8Ui0VcunQJTz75ZNkcDFa6qb53d3cxMzNjzGdgngvCTM5/IOdHkJ0xvcrv2LFjmJyc9OQHph/PBV3Xa16rKo99o+eCnC/DzMv660heDdTrNLQ4djvVx+NrT7Nj45dKJaGqatnY/6lUSgCoOW6/nA+gUCi0VOZ2KpVKYn193fhb7pNMk+LxeNkcEblcTgAQ8Xjc0/y2traEqqqiVCo1VA4hmh9b3a/nwtbWVtk8F+aPudyNngtyudX/d17UX4daZbAnW3h87Wk22MfjccsLubyIyYmTrJZ3ssogK4T9SaMACFVVPc9P07SGf3QI0Xyw8Ou5kEqlRD6fL0srFApV+9pI3ZRKJRGNRutO9uV2/XUoToRD5LVisYjZ2Vm88sorlsvj8TgikUhDzdq6riOdThvNpMlksmx8A7vTKNebjtmOWkNLV456GI/HATwaG0GWo9EJitqR3+joKGZnZ11pDnb6XKh3Hsj83DoXjhw5UjUo2ebmJkZGRqr2EbBXN0tLSzhz5kzdfN2sv47m9c8Np4B3nm3F42tPM3f26+vrAkDVXY8Qj+7W5N2LbNqsXF5JVVWRSCSEEI+mVZbNmXanFt5rOuZmlEoly2Z18z5ubW2JVCrlSJO0E/nJY2O1jXqauTN0+lyodx7I5V6dC0KImtN226mbjY0No8yoc2fvZv11MDbjkz08vvY0E+zlhc2KTDcH6Xv37lUtN5MXYvMFUj4vlRdrq4tjZZp8Tly5Tq3nxnZsbGzUfYaqaZqRRzPPWduRn/zB0GhTcDPBwslzwc55IL/nxbmQy+VqPpIQon7dFAoF40dMrX2Q3Ky/Drbqq0F1zp49i8OHD3tdFF8aHx/n8bVha2sL165da2hQnXoDBimKYqQXi0WEQiGoqoqlpSUEg8Gy5dLMzAwWFxfL0nVdRyAQgKqqWF9ft8yzMi0cDhuzNFZq9rIRDodx8eJFDA8PVy2bn5/HgQMHcPz4ccTjcWxvb2N5eRl9fX1N5eVkfs0M6tTMoCxOngt2zoNaebpxLsRiMZw5cwbBYLBq2V51k0wmMT09XbO8ldyqvw625qs7e3746ZRPM+durWVmsuexvFu1+l6t7ZnTrdapTGtmX+pJpVJld2OVywAYd3D37t0TAGqu73Z+zRyLZu4MnTwX7JwHtdZr97lg1TFP2qtu1tfXqx5z7FU+t+qvg7EZn+zh8bWnmWb8Ri7wQjx6rluryVc28VY+5wQePYdt5AJvbipuVi6Xq9vkW5m3DF7NBhin8+vEYC9E/XPBznlQK892ngtCPAzotZ7371U3zfzQZrBnb3wiz8nex3ZH+5KT+1y5csVy+alTpwAA9+/fN9LktmXTpB1OTcdcLBZx+/btst7U29vbxkyEQHUvetlcW6t3vRf5ycmr2snJc8Gp8wBwfmruO3fuYGhoyHLZXnUjhKj6SOa/K7lRfx3Ny58aTgLvPNuKx9ceJ3vj7zVQSq07e9mBS1VV47upVMq4m5Pbhamp1Hz3JL9jXs/8MZezcgCUSrIXt9V2zL2jZWcy2WFLdiTb2Ngo257b+QnRGb3xmzkX9joPzNtt9VzYq17M9uqY10jdSLI8VtgbXwjBO3si7/3Zn/0ZAODdd9810hRFKZsK2GoI1Lm5Ocs70b6+PiwtLUFV1bLv/uhHPzK2J9WbWniv6ZgBoFQqQdO0mkO5Xrp0qWbHrsHBQePvI0eOYGNjA3fu3IGiKLhx4wY2NjZw5MiRsu+4nR/wqF5kPbWTk+fCXueB3J7UyrmwV72Y3bp1y/I4S43UjR1u1l8n81VvfE5x2z48vvY0O8WtbA49f/58O4rVduFw2Ojd7bf8YrEYAoFAw3XTbG/ubj4X3D4P7HC7/joUp7gl6gRTU1O4c+eOMWpYN8lms7h48aIv89ve3sb29jampqZcyQ/o3nPB7fPADi/qr1Mx2BN1ANnkevXqVWxvb3tdHNs2Nzfx9NNPW77H3u357ezsYHFxEUtLSy2969+objwX3D4P7PCq/joVg30LKseVJmpFMBjE8vIybt++7XVRbDty5AgGBgZ8mV8mk8Hly5ctB31pt247F9w+D+zwsv460X6vC9CJ7M4HrWkaFhcXm8pDjmRlfrZrlUbtPy6ddNz7+vq68lmtH3ldDzwXWsNjV4539haEECiVSmX/Nn82NjYAAAsLC03ncffuXVtp1P7jwuNORH7HYF9DvWc8zb4CIum6jmQyuWcatf+48LgTUS9gsG+QnQkVZACRc0jHYrGyuZTj8bjxLrBcxypNqjWPdCNzUXthr7m0Zbp5XyvTrI5LsVhEJpMx9lse65mZGezs7LS8feDh6zp23hkmIuoGDPYNsBtAf/jDH+L06dMoFArI5/O4cuUKLl26ZCw3D+MpHw1YpQEPA/rU1BQOHDgAIQTOnj2Lo0ePGq+TRCIRZDIZZLNZqKqKfD6PTCaDN954w6G9bt7k5CTef/99CCFQKBSQyWQwNTVlDLdZKBSqvpPP58v+bXVcQqGQMQtXNpvF9PS08dhlcHDQCPjNbp+IyG8Y7G2Qd3wHDx60tf4zzzwDTdMQDAaNEaaa7ci3ubmJTCaDiYkJAI8eIdy6dats8Ar5ykur+TlFlvvkyZMAHvYuvnjxIjKZDN5++20jrZJ5dLZazAFZ7ndfXx80TQMA40692e0DD38EmH8IEBF1MwZ7G+QdX+VdYS1zc3NYWFjA7u5u0xNFSDdv3gRQ3fxcaxKUTiFHnTIH3EOHDgF4tE9OkxNrzM7OtmX7RETdisG+AXbvCoGHz5Fff/31pmbtMpN3qZVvBHR6c7NVy4Ls9Fhr7HIiImoPBvsG2Qmy6XQap0+fxk9+8hPHBpowdzzrBvJHjrlDniSb29ul3dsnIuo2DPZtEIlEADTWElCL0/NIu8XJubTtkj+ITpw40ZbtExF1Kwb7GmRgqvzbzHzXav5b3tXu7u6W3ZFbrWMO3FZpsoPblStXEAgEjOkuR0dHy7Yny2guq9VdtVuOHz8OVVVx9epVoxxvv/02NE0rG6dA3oXL42Se/GNmZgaA9XGR0uk0gIf7vby8DFVVyx6dNLt9vnpHRH7CYG9BUZSyOZ1lkK1kngva/LfsxZ1MJhEIBBCNRqFpGn77299WrXP9+nVMTk7WTKs3j7Tduai9YGcubQC4cOECVFXF4OAgMpkMhoeHoaoqUqkULl++DMD6uEiHDh1COBxGIBBAf38/lpeXHd0+EZEfcD57sqXTjq+dwY280Ox89uQ/PpsPvef4rP44nz0REZHfMdhT16nVV4KIiKwx2FPXqdVXgoiIrHE+e+o6fB5ORNQY3tkTERH5HIM9ERGRzzHYE3IG7l4AACAASURBVBER+RyDPRERkc/5alCd4eFhPP/8814XxZdu3brF42vDgwcPkM1mMTIy4nVRyGNyaObh4WGPS0LNyGazGB4e9s2gOr4J9u2aXIXITwqFAv7rv/4LR48e9booRB3v8OHD+MEPfuB1MZzgn2BPRHvjcL5EPYnD5RIREfkdgz0REZHPMdgTERH5HIM9ERGRzzHYExER+RyDPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIM9ERGRzzHYExER+RyDPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIM9ERGRzzHYExER+RyDPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIM9ERGRzzHYExER+RyDPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIM9ERGRzzHYExER+RyDPRERkc8x2BMREfkcgz0REZHPMdgTERH53H6vC0BE7fHuu+/ib/7mb/DRRx8Zaf/3f/+Hvr4+/PEf/3HZul/5ylfwT//0T24XkYhcwmBP5FPPPfccPvzwQ/z3f/931TJd18v+PTEx4VaxiMgDbMYn8rHXXnsN+/fX/02vKApOnTrlUomIyAsM9kQ+FolE8Mknn9RcrigKXnrpJfzRH/2Ri6UiIrcx2BP52AsvvIDh4WE89pj1/+r79u3Da6+95nKpiMhtDPZEPjc5OQlFUSyXffrppxgbG3O5RETkNgZ7Ip8bHR21TN+3bx/+8i//EqFQyOUSEZHbGOyJfO6ZZ57B0aNHsW/fvqplk5OTHpSIiNzGYE/UA1599VUIIcrSHnvsMXz729/2qERE5CYGe6Ie8Ld/+7d4/PHHjX/v378f3/rWt9DX1+dhqYjILQz2RD3gD//wD6GqqhHwP/nkE7z66qsel4qI3MJgT9QjvvOd7+Djjz8GAHz2s5/FiRMnPC4REbmFwZ6oRxw/fhxPPvkkAGBkZASf/exnPS4REbmlZ8fGX11d9boIRK770z/9U/zHf/wHXnjhBf4/QD3nhRdewOHDh70uhicUUdlFt0fUGmSEiIj8aWRkBGtra14XwwtrPXtnDwArKyscPaxFq6urGB8fr3qti6rJwW28vNh8+umn+PGPf4wLFy54VoZu1Qn1R82rNbhUr+Aze6Ie8thjj+Ef/uEfvC4GEbmMwZ6ox+w15S0R+Q+DPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIN9C4rFItLpNMLhsNdF6XqxWAyxWMzrYhAR+RKDfQsuXbqESCSCTCbjdVEaViwWEYvFoCgKFEVBOp32ukie03Wdgy0RkS8x2LdgYWHB6yI0pVgs4v79+5ibm4MQAqlUCpFIBPPz856VaW5uDnNzc57lDwB37971NH8ionZhsO9B9+/fx/DwsPHviYkJAMDs7KxXRfKcrutIJpNeF4OIqC0Y7Bug6zrS6TQURUE4HMbOzo7lesViEfPz88Z6m5ubRrr5GX8mkzHW2d3dLduG/H4ymUSxWKxqXq6Vhx3mQC/3CwCi0ajtbTjJqu+DnWNVLBaRyWSMdZLJJBRFwczMTFndyEcV5mNYmRaPx43HMeZ09iUgIj/gUFoNmJycxIEDB1AqldDX12f5nLtYLGJqagqnTp2CEAKbm5s4evQocrkcYrGYEVCy2SxUVUU+n8fBgwdx4MAB47HA/Pw8RkdHcf78eei6jng8bjuPoaGhhvZpd3fXuKOdnJxs5rC0bGpqqqrfgzmt1rEKhULG+tlsFtPT0xgbG8MPf/hDDA4O4t69exgYGEChUChbF4CxLWlubg5XrlwBAI7zT0T+I3oUALGysmJ7/fX1dQFA3Lt3z0grlUoCgDAfxlQqJSoPKwARjUaNv62Wm9MAiEKhYPy7UCg0lIdd+XzeyBuAiMfjDX1fCCFWVlaqytIMO8fFKs1qnVwuV7U/zW7LSSMjI2JkZKRt26f2Yv11tx6vv1U249v01ltvAQAGBgaMtL6+vqr1bt68CaC6mVjeNdqhaRpCoRDS6TR0XUcwGCy723QiDwDo7++HEAK5XA7RaBSzs7O+eG4tWzd6uQ8CEZEZg71Ni4uLttaTTc9CiKqPXefOnYOqqohEIggEAlW95J3Iw2xoaMhowj99+nRT2yAios7FYN8mtTrv2TEwMID19XXkcjlomobZ2VnL1+JaycMqT7/RNM3rIhARdQQGe5sSiQQAYHt729Z6y8vLRi932XPeLkVRoOs6hoaGsLCwgFwuV9Yk7UQeleR2UqlU09voFPJH0IkTJzwuCRFRZ2Cwt+mb3/wmgIevYslXv8yvu83MzAAATp48CeDh8/NAIABFURAKhTA6OopisWisL4Or/C+AsuXxeNzI56mnnirrkV8vDzvC4TDm5+eN7cse/9Fo1Hjn3k3m/ZZ/N3KsABhvRui6juXlZaiqClVVjeXyLl/+EMhms8YyWXdyffMPJ756R0R+wGBvU39/P/L5PA4cOICDBw9iZmYGX/ziF6GqKlKpFC5fvgwACAaDyOfzxjvrmqYhn8+jv7+/7PWvQCBQ9l8AZcvPnDmDtbU1KIqCtbU1nD9/3lhWLw87pqenMTs7i4MHD0JRFCwtLeFb3/qWZyPYmfdb/t3IsQKAQ4cOIRwOIxAIoL+/H8vLy2XLL1y4AFVVMTg4iEwmg+Hh4aq6k/t//fp1z15DJCJqB0U026uryymKgpWVFYyNjXldlK62urqK8fFxz95Nl28idMNpLFte1tbWPC4JNYP11916vP7WeGdPRETkcwz21LWsnvX7WaudMHvd/Px8Wb8Pt7H+WuN1/XU7BnufMQ+0U+/jB1bP+v2qWCzi0qVLePLJJ406rNVxsFvqW04pbPWpHIpazoEg50fYa0pmOU+C2bFjxzA5OenJD0M/1h9gf6psWX/hcLjmlOB7reNl/fmC+6P2dQY0OFwuWXNquNxe0OxwnaVSSaiqKra2tox/yyGTaw2RLIdYNg+73Gm2trbKhms2f8zljsfjAoDI5XJCCOvhkM3kcqvzcmtrS6iqKkqlUsPlZf2VKxQKxj4J8WgY78p6SaVSxjEvlUpC0zSRSCQaXkcIb+rPJ1Z79irNYO8MBnv7mr3YxONxy6AgA1oqlbL8XqfXSyqVEvl8viytUChU7atV4AYgVFWt2mapVBLRaLTuPAeapjU1DwTrr5w50EuVx13Ov2FeV/4Ykz/e7Kxj5nb9+QTHxifqZMViEbOzs3jllVcsl8fjcUQikT2btSXzNM3mKZTN+dmdhrmVaZYB4MiRI1Wvi25ubmJkZKRqH4FHYyPIcli9Krq0tIQzZ87UzXd0dBSzs7OuNAf7uf7sTJX9s5/9DADw3HPPGWmf+9znAADvvPOO7XXM3Kw/X/H654ZXwDt7R/DO3r5m7izkbIuVd8BCPLrzk3eylXdBVvWiqqrRPFooFISqqmXNoqqqGndn8k5L3nlpmmZsR35X3pVubGzUvBNrhDkPM7mPW1tbIpVKWTZvb2xsGGVGnTt7uT/r6+sNlY31V1s+nzf2wzwzqKZplvsBU8uMnXUq83Kr/nyEzfjUGgZ7+5q52MgLqBWZLp8JV15oK78nL+jmQCmfm5ubkq0CZWWaU9Msm+VyuZpN2kI8CgrRaLTqmW2hUCh7xlsv2MupqRttCmb9Was3VXatejCn21nHzM3685HVnh5UZ3h4GM8//7zXRelqDx48QDabrWp6pWrZbBbDw8MNDepRb9AgRVGM9GKxiFAoBFVVsbS0hGAwWLYceDgs8OLiYlmarusIBAJQVRXr6+s186xMq9erutlLSiwWw5kzZxAMBquWzc/P48CBAzh+/Dji8Ti2t7exvLxsTDOdTCYxPT1ds7yVmhmMqZlBWXqp/ra3t3Hr1i1cuXIFiUQC09PTNfffnG5nnUpu1Z+PcFAdIj8IBoPI5XLIZDKYmpqyfB/ZappmGSxrXfhrcXqaZfn81SrQp9NpzM7O4vjx4+jr68Pk5CQymQxWV1eNssi5K7pVt9cfYD1Vtnl+ikpyvgo765AD2t540KHAZnxHsBnfvmaaEVGnOdoqXT4jtmo+lk3Flc+7UfE81yrPyjT5b3OzcytSqVTN58WVectmXJkm/671sbNNO1h/9pjzSiQSVWWWzf7ysYudderlYVevN+Pzzp6og8me6HZHDpOT+1y5cqVq2alTpwAA9+/fN9Lkdu3OmCg5Pc3ynTt3MDQ0ZLms8s5P3s3KdFHn7lTUuVM19xpvl16pv8ryyKmyZYuLuczvvvtu2TI761hxo/78hMGeqIMNDAwAqA4WVlMBSxMTE5YXwuPHj0NVVVy9etX43ttvvw1N03DkyJGq7dWbWnivaZblK13b29t77uP29jZefvnlmsvPnj0L4NE0xvIVPJneKPkK2le/+tWmvt8IP9efnamy+/v7kUgkcOPGDei6Dl3XcePGDSQSCeO1SzvrmLlZf77iZbuCl8BmfEewGd++ZpoR5Uhq5gFHYLOp2uq1JdlrXX4vlUqV9Wy32m6tvMyvW2maVvZ6WTQaFZqmWZahUjQa3XOkuI2NDaM3vqZpYmNjo+769Y6L7MHe6Oh0rL9y8pGD/MTjccuBdszrqqpas+7srCOEu/XnI73dG59T3LbO6yluu0mzvYFl0+r58+cdL1O7hcNho5d4p4jFYggEAg0fT9ZfZ3C7/nyCvfGJOt3U1BTu3LljNF93i2w2i4sXL3pdjDLb29vY3t7G1NSUa3my/pzjRf35BYM9UYfr6+vD0tISrl69ausZeCfY3NzE008/XTWkqpd2dnawuLiIpaUlo5OfG1h/zvCq/vyCwd4B9aaSnZ+fRyaT4TzMbSKnSe3W7dsVDAaxvLyM27dve10UW44cOWJ0TusUmUwGly9ftnyXv91Yf63zsv78gMHeAUIIFAoF49+lUsl4BejYsWNIJpOch7lN7t6929Xbb0RfX19XPvftFOfPn/c0ULD+WuN1/XU7BnuHmE9CcxPT0NAQlpaWAKDmyFjUHF3XkUwmu3b7RERuYbB3QTAYxNmzZ5HJZKruFGtNM9nIVJXAo/di5ZSX5qbnVqeybId6U3WaH4NIVmnxeNwY9lMuKxaLyGQyxnFLJpNQFAUzMzPY2dlpefvAw97AsVisHYeFiKgtGOxd8tJLLwEA3nrrLSOtWCxiamoKBw4cgBACZ8+exdGjR43eppFIBJlMBtlsFqqqIp/PI5PJ4I033ijb9vz8PEZHRyGEwNjYGK5fv24rDy9NTk7i/fffNx6BmMcENz8SkfL5fFWaeT5z+dgkFAoZk3xks1lMT0+jVCoBAAYHB7Gzs9PS9omIupIXb/d3ArRhUB3sMV5z5fK9ppm02l6tNPMAE3IgDzt5tKqZQXXsTNXZyL7bScvlcsbAH61uq1k9PqhH12P9dbcerz+Oje+lmzdvAqhuQrYaF7seTdMQCoWQTqeh6zqCwaBxF+pUHk6Sg1qY+zkcOnQIwKPyOk2Ouz47O9uW7RMRdTIGe5fIjnnmMa+dmmby3LlzUFUVkUgEgUCgbDKLdkxl2Sonp+okIqK9Mdi75Be/+AUA4JVXXqlaJjuONWtgYADr6+vI5XLQNA2zs7NVs1e1moeT5GxlVq8itnv+as6PTUS9iMHeBcViEdeuXYOqqsbsVIBz00wqigJd1zE0NISFhQXkcjmjubpdU1m2wsmpOu2SP3ZOnDjRlu0TEXUyBnuHmN+fN/9tHsdZvm8v1Ztm0u5UlVI8HjdeyXvqqaeMebT3msrSC3am6pR34DJIm8cVn5mZMf42txJU/oCRU6Lquo7l5WWoqmqs38r2+eodEXUbBnsHKIqCQCBg/FsGVUVRcPv2bVy8eBHr6+tVoz8Fg0Hk83njOb6macjn8+jv70coFCrbnvm/AMqWA8CZM2ewtrYGRVGwtrZmjNRVLw+vyLHCVVVFKBQyOg3+6Ec/Mta5cOECVFXF4OAgMpkMhoeHoaoqUqkULl++bKwnX4+7fv06Jicny/I5dOgQwuEwAoEA+vv7sby87Oj2iYi6Bae45RS3LenEKW7lj4dOKhPQ81Nsdj3WX3fr8frjFLdERER+x2BPvmLuy8CJh4iIHmKwJ18x92Wo7NdARNSr9ntdACInddpzeiKiTsA7eyIiIp9jsCciIvI5BnsiIiKfY7AnIiLyOQZ7IiIin+vpEfSIiKh3jIyM9OwIej376t3KyorXRSBy3dbWFq5du8bzn3rSCy+84HURPNOzd/ZEvagT5zIgorbj2PhERER+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9kRERD7HYE9ERORzDPZEREQ+x2BPRETkc/u9LgARtcdvf/tbvPvuu2VphUIBAHD//v2y9H379uHgwYOulY2I3KUIIYTXhSAi57333nsIhUL46KOP9lz3xIkT+Nd//VcXSkVEHlhjMz6RTz311FP4xje+gcce2/t/84mJCRdKREReYbAn8rFXX30VezXefeYzn8G3v/1tl0pERF5gsCfysXA4jN/7vd+ruXz//v0Ih8P4gz/4AxdLRURuY7An8rHf//3fx7e//W08/vjjlss/+eQTfOc733G5VETkNgZ7Ip87depUzU56Tz75JP76r//a5RIRkdsY7Il87hvf+Ab6+vqq0h9//HGMj4/jM5/5jAelIiI3MdgT+dzjjz+OiYkJPPHEE2XpH330EU6dOuVRqYjITQz2RD0gEongww8/LEt75pln8PLLL3tUIiJyE4M9UQ/4i7/4C4RCIePfjz/+OCYnJ7Fv3z4PS0VEbmGwJ+oBjz32GCYnJ42m/I8++giRSMTjUhGRWxjsiXrExMSE0ZT/wgsv4E/+5E88LhERuYXBnqhHvPTSS/jCF74AAPj7v/97KIricYmIyC09O+vd6Oio10Ugcp1sxv/5z3/O/weo5xw+fBg/+MEPvC6GJ3r2zv7WrVt48OCB18Xoeg8ePMCtW7e8LkZXyGazyGaznpahv78fgUAA/+///T9Py9GNOqH+qHnZbBZbW1teF8MzPXtnDwDnzp3D2NiY18XoaqurqxgfH8fa2prXRel48k7a62N1+/ZtHDt2zNMydKNOqT9qTq+3ZPXsnT1Rr2KgJ+o9DPZEREQ+x2BPRETkcwz2REREPsdgT0RE5HMM9i0oFotIp9MIh8NeF6XrxWIxxGIxr4tBRORLDPYtuHTpEiKRCDKZjNdFaVkymez5EdV0Xe/5Y0BE/tTT79m3amFhAYuLi14Xo2Xb29s4ffq0p2WYm5vzNH8AuHv3rtdFICJqC97Z9zhd1zkCHh4eh2Qy6XUxiIjagsG+AbquI51OQ1EUhMNh7OzsWK5XLBYxPz9vrLe5uWmkm5/xZzIZY53d3d2ybcjvJ5NJFIvFqublWnk0amlpCWfOnGnqu06x6vtg51gVi0VkMhljHfkoYmZmpqxuFEUxPrXS4vG48TjGnM6+BETkBwz2DZicnMSdO3dQKpWwvr6O//zP/6xap1gsYmpqCgcOHIAQAmfPnsXRo0exvb2Nqakp4xl/NpuFqqrI5/PIZDJ44403jG3Mz89jdHQUQgiMjY3h+vXrtvNoxObmJr72ta8hGAw2d0AcYj4uVmm1jlUoFEI4HDbWmZ6eRqlUAgAMDg4aAb9QKFTlmc/ny/5tfowghIAQwvH9JCLyjOhRAMTKyort9dfX1wUAce/ePSOtVCoJAMJ8GFOplKg8rABENBo1/rZabk4DIAqFgvHvQqHQUB52FAoFkUgkapbBrpWVlaa+V8nOcbFKs1onl8sJACIej7e8LSeNjIyIkZGRtm2f2ov11916vP5WeWdv01tvvQUAGBgYMNL6+vqq1rt58yaA6mbiK1eu2M5L0zSEQiGk02nouo5gMFh2p+lEHv/yL/+C6elp2+t3k6GhIQDA7OysxyUhIuoMDPY22e11L5uixe+ags0fu86dOwdVVRGJRBAIBDA/P+9oHplMBt/85jdtl4eIiLobg32b1Oq8Z8fAwADW19eRy+WgaRpmZ2erAn4reYTDYRw8eLBmxzW/0DTN6yIQEXUEBnubEokEAOzZCU6ut7y8DF3XATzqOW+XoijQdR1DQ0NYWFhALpcra5JuNY96LQKNtEB0Kvkj6MSJEx6XhIioMzDY2ySbvWOxmPHql/l1t5mZGQDAyZMnATx8fh4IBKAoCkKhEEZHR1EsFo31ZZCW/wVQtjwejxv5PPXUU4jH48ayenl0I/N+y78bOVYAkE6njXWWl5ehqipUVTWWy7t8+UMgm80ay2TdyfXNP5z46h0R+QGDvU39/f3I5/M4cOAADh48iJmZGXzxi1+EqqpIpVK4fPkyACAYDCKfzyMajQJ4GGTy+Tz6+/sRCoWM7QUCgbL/AihbfubMGaytrUFRFKytreH8+fPGsnp5dCPzfsu/GzlWAHDo0CGEw2EEAgH09/djeXm5bPmFCxegqioGBweRyWQwPDxcVXfy9bvr169jcnLSwT0kIvKWIvzQbtsERVGwsrKCsbExr4vS1VZXVzE+Pu5Z87/sY9ANp7FseVlbW/O4JNQM1l936/H6W+OdPRERkc8x2FPXsnrWT0RE1Rjsfcb8Ol29jx9YPev3s0bf6qBy8/PzZZ083cb6a43X9dftGOx9xuq1ulYG+OlkftynWorFIi5duoQnn3zS+MFW6y2BbvpxVywWEYvFjHLKtyoqyQmP5FwIzaxz7NgxTE5OetIK5Nf603Ud2WwWyWSybCKrSt1ef77Q5vF4OxYaHBufrDk1Nn4vaHZs7lKpJFRVFVtbW8a/5fwIteZDkPMpmOdY6DSFQsHYJyEezflgntNApquqKkqlkiiVSkLTtLJ5HeyuI4QQW1tbxnqNYv1Vi0ajIhqN1p1XotvrzydWe/YqzWDvDAZ7+5q92MTjccugIC+wqVTK8nudXi/mQC9VBo18Pi8AlK0rJzrK5XK21zHTNK3qB4UdrL/aagV7P9SfT3AiHKJOViwWMTs7i1deecVyeTweRyQSqdn8XUnXdaTTaaN5OJlMVnV0TKfTRpNsJpOBoigIh8PGIE/mdefn543l5kGm7BgeHq4qGwBj/AgA+NnPfgYAeO6554y0z33ucwCAd955x/Y6ZqOjo5idnXWlOdjP9WdHt9efnzDYE3Wwn//85wCAL3zhC5bLz58/j2g0ikgksudQzgAwOTmJ999/H0IIFAoFZDIZTE1NGYF2amoKkUgEmUwG2WwWqqoin88jk8ngjTfeMLZTLBYxNTWFAwcOQAiBs2fP4ujRo7bKYGV3d9cYJdI8oNGdO3cAoGzAqGAwCODRhFB21jGTx1Ie23bqlfqrpdvrz1e8bVnwDtiM7wg249vXTDOifB5qRabLZ8IAxL1796qWSxsbG1XPgbe2tqqakmHRJFuZJp85V65T6xl0PbIZV37MTbRWZalMt7OOWalUsuwbsBfWX22N1kG31J+P9PYze374cfvT6MVGfq/WMkl26FJV1QgGld/TNK0qTV44VVWtm2dlmgxOVp9m5XI5IzjKzlm1tmlOt7NOI8tqaSZY9Er9NVoH3VJ/PrK6Hz3s7NmzOHz4sNfF6GpbW1u4du0aVlZWvC5Kx3vzzTfbtu1gMIhcLocvf/nLmJqaqpobAAAWFxer0vr6+gBYN5fWI9cXDr7yODQ0hM9+9rO4cuUKTp8+jenpaaiqWrNscnIjO+t0Oj/Un5Veqb9u0NPB/vDhwxwb3wHXrl3jcbSh3WNyDw0NYX19HeFwuGyWREleVIvFovFMVGr2orqzs4OBgYGmvmulcltWZZYdzV588UXb63QDP9RfpV6qv07HDnpEHUxe9O2OHCZn8rty5UrVslOnTgEA7t+/b6TJ7TY6PXIikQAALC8vG9twYoQ4ua1UKgXg0dTS5jK/++67ZcvsrGPF3Ou/XXqt/ip1e/35CYM9UQeTd12VwUK+dmT1+tHExITlhfD48eNQVRVXr141vvf2229D0zQcOXKkansyT3PecvnJkycBAFeuXEEgEICiKAiFQkbQka901evdHQ6HMT8/b9zF6bqOeDyOaDSKiYkJAA97aCcSCdy4cQO6rkPXddy4cQOJRMLovW1nHTOZ31e/+tWaZXOKn+uvMh+r/ez2+vMVr3sNeAVgb3wnsDe+fc10EJIdt8wDjsBmpypzpy3z9hKJhPG9VCpVNhqZ1XZr5ZXP540OdZqmiXw+byyLRqNC0zTLMkjr6+tl243H45YD7ZjXVVVVbGxsNL2OEI96sDc6Oh3rr5rVvljtT7fWn4+scj57Pmtuidfz2XeTZufTlk2r58+fd7xM7RYOh7G+vu51McrEYjEEAoGGjyfrrzO4XX8+wfnsiTrd1NQU7ty5g2w263VRGpLNZnHx4kWvi1Fme3sb29vbmJqaci1P1p9zvKg/v2CwJ+pwfX19WFpawtWrVx0f4axdNjc38fTTT1cNieulnZ0dLC4uYmlpyXhlzQ2sP2d4VX9+wWDvgHrzxs/PzyOTyXAe5jbRdb2tU4C2e/t2BYNBLC8v4/bt214XxZYjR4609ZWuZmQyGVy+fLnqtTU3sP5a52X9+QGDvQPE78aplkqlkjHH+rFjx5BMJjkPc5vcvXu3q7ffiL6+vq587tspzp8/72mgYP21xuv663YM9g4xn4TmJqahoSEsLS0BQNmEFdQ6XdeRTCa7dvtERG5hsHdBMBjE2bNnkclkqu4Ua00z2chUlcCj92LllJfmpmc3prJsVL2pOs2PQSSrtHg8bgyzKZcVi0VkMhnjuCWTSSiKgpmZGezs7LS8+n1OlgAAE49JREFUfeBhb+BYLNaOw0JE1BYM9i556aWXAABvvfWWkVZvmkm7U1UCDwP96OgohBAYGxvD9evXbeXhpXpTdZofiUj5fL4qbW5uzvhbPjYJhUIIh8PGcZuenkapVAIADA4OYmdnp6XtExF1JS/e7u8EaMOgOthjJqbK5XtNM2m1vVpp5gEm5EAedvJoVTOD6tiZqrORfbeTlsvljIFbWt1Ws3p8UI+ux/rrbj1ef6u8s/fQzZs3AVQ3IVuNi12PpmkIhUJIp9PQdR3BYNC4C3UqDyfJQS3M/RwOHToE4FF5nTY0NAQAmJ2dbcv2iYg6GYO9S2THPPOY1+ZpJis/jTh37hxUVUUkEkEgECibzMKpPJzk5FSdRES0NwZ7l/ziF78AALzyyitVy2THsWYNDAxgfX0duVwOmqZhdna2avaqVvNwkqqqAKwnAWn3/NWcH5uIehGDvQuKxSKuXbsGVVWN2akA56aZVBQFuq5jaGgICwsLyOVyRnO1W1NZNsLJqTrtkj92Tpw40ZbtExF1MgZ7h9Sa5tE8jrN8316qN82k3akqpXg8bryS99RTTxnzaO81laUX7EzVKe/AZZA2jys+MzNj/G1uJaj8AZNOpwE8PG7Ly8tQVdVYv5Xt89U7Iuo2DPYOUBQFgUDA+LcMqoqi4Pbt27h48SLW19erRn8KBoPI5/PGc3xN05DP59Hf349QKFS2PfN/AZQtB4AzZ85gbW0NiqJgbW3NGKmrXh5ekWOFq6qKUChkdBr80Y9+ZKxz4cIFqKqKwcFBZDIZDA8PQ1VVpFIpXL582VhPvh53/fp1TE5OluVz6NAhhMNhBAIB9Pf3Y3l52dHtExF1C05xyyluW9KJU9zKHw+dVCag56fY7Hqsv+7W4/XHKW6JiIj8jsGefMXcl4ETDxERPcRgT75i7stQ2a+BiKhX7fe6AERO6rTn9EREnYB39kRERD7HYE9ERORzDPZEREQ+x2BPRETkcz3dQW9ra8vrInQ9eQxXV1c9Lknne/DgAQAeq27F+utuDx48wPPPP+91MTzT0yPoERFR7xgZGenZEfR69s6+R3/jUI/rxOGNiaj9+MyeiIjI5xjsiYiIfI7BnoiIyOcY7ImIiHyOwZ6IiMjnGOyJiIh8jsGeiIjI5xjsiYiIfI7BnoiIyOcY7ImIiHyOwZ6IiMjnGOyJiIh8jsGeiIjI5xjsiYiIfI7BnoiIyOcY7ImIiHyOwZ6IiMjnGOyJiIh8jsGeiIjI5xjsiYiIfI7BnoiIyOcY7ImIiHyOwZ6IiMjnGOyJiIh8jsGeiIjI5xjsiYiIfI7BnoiIyOcY7ImIiHyOwZ6IiMjnGOyJiIh8jsGeiIjI5xjsiYiIfI7BnoiIyOf2e10AImqPYrGIn/70p2Vpv/zlLwEAP/7xj8vSn376aUxPT7tWNiJylyKEEF4Xgoic9/HHH+PZZ5/Fe++9h8cff7zmeh988AG+973vYXFx0cXSEZGL1tiMT+RT+/fvRyQSwb59+/DBBx/U/ADAqVOnPC4tEbUTgz2Rj0UiEXz00Ud113n22Wfx9a9/3aUSEZEXGOyJfOzw4cN4/vnnay5/4oknMDk5icce46WAyM/4fziRjymKgldffbXmM/sPP/wQkUjE5VIRkdsY7Il8rl5T/uc//3l85StfcblEROQ2Bnsin/vSl76EwcHBqvQnnngC3/3udz0oERG5jcGeqAdMTk5WNeV/+OGHmJiY8KhEROQmBnuiHvDqq6/i448/Nv6tKAqGhoYwMDDgYamIyC0M9kQ94ODBg3jxxRehKAoAYN++fWzCJ+ohDPZEPeK1117Dvn37AACffPIJxsbGPC4REbmFwZ6oR4yNjeHTTz+Foij42te+hgMHDnhdJCJyCYM9UY949tln8fLLL0MIwSZ8oh7DiXB+Z3V1FePj414Xg4iIHMLwZljjFLcVVlZWvC5C13vzzTcBAOfOnfO4JJ1vfHwcZ8+exeHDh13J7ze/+Q0SiQS+//3vu5Kf37ldf2TP1tYWrl275nUxOgqDfQV2Wmrd2toaAB5LO8bHx3H48GFXj9Vf/dVf4bnnnnMtPz/zov7IHgb7cnxmT9RjGOiJeg+DPRERkc8x2BMREfkcgz0REZHPMdgTERH5HIO9w4rFItLpNMLhsNdF6WqxWAyxWMzrYhAR+QKDvcMuXbqESCSCTCbjdVGasr29DUVRjM/MzIzXRfKUruvG5DFERN2K79k7bGFhAYuLi14Xo2nvvPNO2b9PnDjhSTnm5uY8ybfS3bt3vS4CEVHLGOypzLPPPsshJn9H13Ukk0mvi0FE1DI247dI13Wk02koioJwOIydnZ2qdYrFIubn5411Njc3y5aZn/FnMhljvd3d3bLtyG0kk0kUi8Wy5uV6edi1u7uLcDiMWCyGbDbb8PedUqvfg51jVSwWkclkjHWSyaTxOELWjfkxhWSVFo/Hjccx5mXsT0BEXUeQEEKIlZUV0czhUFVVaJomSqWSEEKIVColABjbKhQKQlVVkUqlhBBCbGxsCAAil8sZ35frb21tCSGEyOfzAoDQNM3IJx6Pi3w+L4QQolQqiWg0ajsPu9bX142yABCqqopCodDwMRkZGREjIyMNf08yH5Na6bWOlbn8cp1SqSQ0TRMAxL1790ShUKjavtxOZZ5WadFoVESj0ab3r3L7KysrjmyL3Mf660zNXs99bJVH43eaOTlkcLx3756RViqVygKEDP5mAMqChZ0gA6As8MqAZTcPu0qlksjlcsaPiUQi0fA2Wg32Qlgfk1rpVseqcp1cLicAiHg8bns79crhFAaL7sb660wM9lVW2YzfgrfeegsAMDAwYKT19fWVrXPz5k0A1c3EV65caSgvTdMQCoWQTqeh6zqCwaDxbN2pPGT5h4aGMDc3h0Qi0bVvFVQaGhoCAMzOznpcEiIi9zHYt8BOr3sZLIUQVZ9GnDt3DqqqIhKJIBAIYH5+3vE8Ko2Njfkm2BMR9TIGe5dYddxrxMDAANbX15HL5aBpGmZnZ8sCvhN5VOrr64OmaY5u02t+2x8iIjsY7FuQSCQAPByIZq91lpeXoes6gEc95xuhKAp0XcfQ0BAWFhaQy+WMJmmn8qik6zpGR0db2kankD+EvBo3gIjISwz2LfjmN78J4OGrWPLVL/MrbzMzMzh58iSAh8/PA4EAFEVBKBQygmixWDTWl4Fa/rdyeTweN/J56qmnEI/HAWDPPOxIp9NlZd/d3cXdu3dx5MgR29twinmfa/2917ECHu6TXGd5eRmqqkJVVQCP7vDljwDzq4bmUQPl+uYfT3z1joi6DYN9C/r7+5HP53HgwAEcPHgQMzMz+OIXvwhVVZFKpXD58mUEg0Hk83lEo1EAD4NMPp9Hf38/ACAUChnbCwQCZf+tXH7mzBmsra1BURSsra3h/PnzALBnHnY8+eSTOHr0KBRFQSwWw3vvvWcEOreZ97nW33sdKwA4dOgQwuEwAoEA+vv7sby8bCy7cOECVFXF4OAgMpkMhoeHy+pNkiP5Xb9+HZOTkw7tIRGRuxTRai8un1hdXcX4+DhHj3OAbFFYW1vzJH/5NkI31KWiKFhZWcHY2JjXRaEmsP46E6/nVdZ4Z09ERORzDPbkK7We8fuRE50we9n8/HxZnw+3sf5q87pu/IjBvgeYB9up9/GDWs/4/aZYLOLSpUtQVRWbm5tGHdbqONgt9a3rOrLZLJLJZNXcCJXkHAjhcLjmeBD11jl27BgmJyc9+VHI+uvcuvEt90ft60wcXtE5TgyX2yvQxHCrpVJJqKpqjP0v0+SwybWGSZZDLDcz34Fb5LwD2GOY4lQqJVRVFaVSyZj7oHJoZzvrbG1tGes0g/VXzsn6a6VueD2vwrHxJZ4czmGwt6+ZYBGPx2sGBHmRlZMiWS3vBvWChZy0yBws5dwHcvInO+tImqYZcyY0U07WX7VW609qtm54Pa/CsfGJukmxWMTs7CxeeeWVmuvE43FEIhFjnIG9mKdpNk+hLPOzOwWzXL/VqZb38rOf/QwA8Nxzzxlpn/vc5wAA77zzju11pNHRUczOzrrSZMz669y68TsGe6Iu8vOf/xwA8IUvfKHmOufPn0c0GkUkEqk7uqM0OTmJ999/H0IIFAoFZDIZTE1NQdd1TE1NIRKJIJPJIJvNQlVV5PN5ZDIZvPHGG2XbKRaLmJqawoEDByCEwNmzZ3H06FFbZWjEnTt3AKBsHIlgMAjg0TwRdtaR5LGUx7adWH+dWze+523LQudgs49z2IxvHxpsBpbPQ+ttT4hHz4VRMQVz5Xc3NjaqngNvbW2VNSXD5tS/Tk61bLX9vZaZ0+2sI8lpqZtpLmb91d6PVupParZueD2vsspBdX5HDsKwsrLidVG63ptvvgng4Ux9VJ885+wOyrLXgEGKohjLisUiQqEQVFXF0tISgsFg2XLg4dDAi4uLZWm6riMQCEBVVayvr1vmaZVWr1d8o5eZevtZa5k53c46dvPbq5ysv8b204264aA6Vdb40+d35C9Bfvhx+9PInaH8Tr3lZrLjk+zVXLm81vbM6Vbr2E1rVr1tyTteq+9ommZ7Hbv57VVO1l+1Vuuv1XLxzr4KO+hVEhZzwvPT2GdkZAQjIyOel6MbPu02NDSE9fV1ZDIZY+IkM/NEP5WanQ7Y6amWK1mVWXY2e/HFF22v0w16tf7IeQz2RF1EXvAbGV1MTvBz5cqVqmWnTp0CANy/f99Ik9tudHrjdk21XEnONmku87vvvlu2zM46leREUu3E+uvcuvE7BnuiLjIwMADAOliYX7eqNDExYXnBPH78OFRVxdWrV43vvf3229A0DUeOHGloWuG9plqWr3Tt1bvbvH2r/ezv70cikcCNGzeg6zp0XceNGzeQSCSMHt521pHkXeVXv/rVuuVyAuuvc+vG9wQJIfiMx0nsjW8fGnzmK0dRMw9IIrdT+bGiqqrlNhOJhPG9VCpljFpmtc16+eTzeaPHuaZpIp/PG8ui0ajQNM2yDPX2o9a+rK+vG8+zNzY2ml5H9l5vZmQ61l85p+uv2brh9bwKe+NL7L3pHK+nuO0mzUyRKptVz58/365itVU4HMb6+rrXxTDEYjEEAoGmjifrr72arRtez6twiluibjM1NYU7d+4gm816XZSGZbNZXLx48f+3d/8+qbNhGMcvkndHHeA/0DgxuLhKnEwYNXFw0wQ2jYwwGBOnoiYOEmEjUZJ3g8TJOLjoYgKrm4mLTPAX9Aye9vDbAq0t9fuZPCU8fdInOTfQp/fl9zRszWZTzWZT+/v7P3ZO1s8ZP9YmzCj2wJyJRqMql8s6OztzvbuZlx4fH7W0tKT19XW/pyLpa9d5sVhUuVxWNBr9sfOyft/za23CjGIPzKFYLKZKpaKHhwe/p+JYMpm0N6gFQb1e18nJid2q9SexfuP5uTZhRbH3wLjM+EKhoHq9PtGjN3Cu0+l4mvXt9fiTiEajc3vfNwiOj499LSas32h+r00YUew9YJpfgRSWdrttN1HZ3NxUqVTS3t4eSU4eeHp6muvxAcALFHuPdH8q7b7nlEgkVC6XJclOpoI7Op2OSqXS3I4PAF6h2PsgFovp8PBQ9Xp96DfFUZnSk2RTW++3sq37f3r+idzqSY3L5e6+FWLpP2YYhh3k0X281WqpXq/b161UKikSiSiTyditQWcZP5/PK5/Pe3ZdAGBWFHufrK2tSZLu7+97jo/LlHaaTV0oFLS9vS3TNLWzs6OrqyvH5/DTuFzu7tsilvf3955/n56e2n+bXb3n4/G4nej18vKig4MDtdttSdLKyore3t5mGh8AAs+HTj6B5EXHJTlIuOp//btM6WHv6T+mvo5TVtcup+eY1TQd9NzK5R51zYcdtxLFrKzsWcaflibswIZgYf2CiQ56A0i9C5rb21tJgz8hDwvBGCWdTisej6tararT6SgWi/V8C3XjHG6zuu1173VYXV2V9G++bkskEpKkbDbryfgAEBQUe59YG/P6wy2se8LmDHGoR0dHSqVS2t3d1cLCwkBqlRvncFuxWBw4Zm1stOYLAJgOxd4nr6+vkqSNjY2hr8+SKb28vKxaraZGo6F0Oq1sNjs0ptLr3OpJeJHL7ZTX4wOA3yj2Pmi1Wrq8vFQqlVIymex5zY1M6Ugkok6no0QioevrazUajZ6fqn8qt3oSbuZyO2V92Nna2vJkfAAICoq9R0ZlOncHO1jP23cblyk9STa1YRj243iLi4syDMPROfzyXS639O8buFWku4NEMpmMpN5fCIZ9eKlWq5K+rlulUlEqlbLfM+34PHoHIPB82hkYOG7u3tSITGf93fndn2Xdb1SmdP9Y4459fn6ahmH07DZ3cg43TJtnPy6X25pzKpUyJZm1Ws00za9877u7O3sXv7XDPpfL9ezst8ZsNBr2GDc3N66Mn8vlpn6SQezmnmusXzCxG38AefYW8o/dE8Q8e+uJg6Ct7zR56AgO1i+Y+P98AHn2AACEHcUeode9l4HwIQC/EcUeoRePx4f+DQC/xX9+TwDwGvftAPx2fLMHACDkKPYAAIQcxR4AgJCj2AMAEHJs0OvjZ8vYsLDazHItnbm4uAhUAyJMhvULno+PD7+nEDh00Pvr+flZ5+fnfk8DAOASPoTZ/qfYAwAQbrTLBQAg7Cj2AACEHMUeAICQo9gDABByfwCYebVnVcQc0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conda install graphviz\n",
    "#conda install pydot\n",
    "#conda install pydotplus\n",
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the parameters of a layer can be accessed using its `get_weights()` and `set_weights()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3\n"
     ]
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03550194,  0.026763  , -0.05496952, ...,  0.06234607,\n",
       "        -0.01958454, -0.07247707],\n",
       "       [ 0.05312347, -0.06168449, -0.00690103, ..., -0.00810606,\n",
       "         0.04796738,  0.07112816],\n",
       "       [ 0.07343674,  0.00355121,  0.01129284, ...,  0.01459645,\n",
       "         0.03866401,  0.00992532],\n",
       "       ...,\n",
       "       [ 0.07023481,  0.04740307, -0.05007914, ..., -0.03447373,\n",
       "        -0.03305513, -0.00741641],\n",
       "       [ 0.07079388,  0.04774422,  0.04132418, ...,  0.06253916,\n",
       "        -0.06233387, -0.01359921],\n",
       "       [-0.06346244,  0.04384978, -0.06936462, ..., -0.0673992 ,\n",
       "         0.03135884, -0.03409648]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First input connection to each of the 300 neurons in hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03550194,  0.026763  , -0.05496952, -0.0396114 ,  0.00171014,\n",
       "       -0.02661934, -0.04328419,  0.02588104,  0.01886033, -0.04168353,\n",
       "        0.06313936, -0.01816562,  0.034371  ,  0.02634424,  0.01544217,\n",
       "        0.03367422, -0.05955818, -0.06337114,  0.06066568,  0.02936454,\n",
       "        0.03856376,  0.02425412, -0.01198527, -0.05954003,  0.06618066,\n",
       "        0.07400754,  0.021465  , -0.06669442,  0.03395548, -0.03818245,\n",
       "        0.02867067,  0.0180715 ,  0.04577805,  0.04957814, -0.03635976,\n",
       "       -0.00797799, -0.05280275,  0.02550439, -0.00486012,  0.01385266,\n",
       "       -0.07294268, -0.05895633,  0.00421606,  0.00224176,  0.04102567,\n",
       "       -0.03788963, -0.04114794,  0.00877522, -0.04323959, -0.03931945,\n",
       "       -0.03294398,  0.06915732,  0.00067221,  0.05712625, -0.01060534,\n",
       "       -0.01557034, -0.03405895,  0.06227823, -0.00563996, -0.03311468,\n",
       "        0.05404492, -0.02828035,  0.00419722, -0.05862967,  0.06637879,\n",
       "       -0.02196395, -0.07389785, -0.00421086,  0.00044861, -0.00715937,\n",
       "       -0.00249735,  0.0649377 , -0.05060418, -0.01500889,  0.03880113,\n",
       "        0.04021198,  0.06954886, -0.00329958, -0.0171533 ,  0.02625506,\n",
       "        0.06470853,  0.06298339, -0.02927956, -0.01806068, -0.05939233,\n",
       "       -0.04530815,  0.03479622, -0.06616798, -0.00893643,  0.01354244,\n",
       "        0.03498513, -0.0023405 , -0.04410039, -0.01746036, -0.03199233,\n",
       "        0.00465401,  0.00942251,  0.0115198 ,  0.01267006, -0.05443297,\n",
       "       -0.00395522,  0.02527293,  0.04577244, -0.04779805, -0.00726887,\n",
       "       -0.01577917, -0.00277641,  0.01367731,  0.00838315,  0.06822351,\n",
       "        0.06348763,  0.02907917, -0.00542328,  0.01853911,  0.00914948,\n",
       "       -0.03699692, -0.02353166, -0.0560011 ,  0.03974412,  0.04936085,\n",
       "       -0.00998336, -0.03165716,  0.00586858, -0.03353097,  0.02423438,\n",
       "        0.0242717 , -0.04022299,  0.07355197,  0.06682017,  0.00135443,\n",
       "       -0.0516389 , -0.04805573,  0.00999924,  0.01290003,  0.01718868,\n",
       "       -0.02352061,  0.03471658, -0.06582014,  0.04805145, -0.07216172,\n",
       "       -0.04006187,  0.05444896, -0.03716054, -0.06348412,  0.01467805,\n",
       "       -0.05409887, -0.00475218, -0.02898202, -0.02387185, -0.06219502,\n",
       "        0.02110106,  0.03355057,  0.05834307,  0.03257567, -0.04160117,\n",
       "        0.07317619, -0.0061702 ,  0.00524066, -0.07178774, -0.03441376,\n",
       "       -0.0390658 , -0.06945296, -0.00357559, -0.04623563, -0.05728073,\n",
       "       -0.00884273, -0.04506766,  0.06629109,  0.01707897,  0.04300693,\n",
       "       -0.05400559,  0.04439643, -0.01560612,  0.07333171,  0.02020223,\n",
       "        0.05927992, -0.05289816,  0.00840665,  0.07333629, -0.01329154,\n",
       "       -0.00429756,  0.00398272, -0.03951676, -0.03821559, -0.01964333,\n",
       "       -0.01147751, -0.06398103,  0.06842127,  0.00062732,  0.0070793 ,\n",
       "       -0.03794153,  0.0328267 , -0.06464455, -0.03629862, -0.00159396,\n",
       "        0.05694133, -0.06656759,  0.06363173, -0.04006254,  0.05551189,\n",
       "        0.07181945, -0.05033058, -0.01350729,  0.04796858,  0.06197417,\n",
       "        0.00711309, -0.01311725, -0.03065456,  0.0378506 ,  0.01343313,\n",
       "        0.06407583, -0.04825252,  0.02050721, -0.05824102,  0.05016217,\n",
       "       -0.06400213, -0.06836227, -0.03921449, -0.05457001, -0.01593124,\n",
       "        0.02660094,  0.03419438,  0.06867158,  0.01178012, -0.04740314,\n",
       "       -0.07422753,  0.04065147, -0.03904996,  0.02846163,  0.0194448 ,\n",
       "       -0.06797949,  0.06221256, -0.01167563, -0.01810089, -0.06736807,\n",
       "        0.00407909, -0.04160713, -0.06098219, -0.00913404,  0.05104688,\n",
       "       -0.00286283, -0.01183789,  0.01735789,  0.00616366,  0.0401268 ,\n",
       "       -0.00250164, -0.01077342, -0.00383613, -0.04225455,  0.02248871,\n",
       "       -0.02380909, -0.01804151,  0.06755829, -0.06067722, -0.03988793,\n",
       "       -0.02697731,  0.04446933, -0.03946772,  0.02063844,  0.03983794,\n",
       "       -0.03151403, -0.01030436,  0.07379396, -0.0730527 ,  0.00295303,\n",
       "       -0.06775121, -0.0244353 , -0.06363107,  0.05248943, -0.03624619,\n",
       "        0.06711121, -0.02116869,  0.00295868,  0.07316482, -0.06663384,\n",
       "       -0.0736345 ,  0.01981295, -0.04416133,  0.06832655, -0.01384805,\n",
       "       -0.00306631, -0.01715462, -0.02183257,  0.00696304,  0.03639728,\n",
       "        0.04819327,  0.02938231, -0.05150276, -0.04869167, -0.06133432,\n",
       "        0.04120602,  0.05370668, -0.02608697,  0.00578064, -0.04205865,\n",
       "        0.02656194, -0.00970839,  0.06234607, -0.01958454, -0.07247707],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Compiling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Losses: https://keras.io/api/losses/    \n",
    "- Optimizers: https://keras.io/api/optimizers/     \n",
    "- Metrics: https://keras.io/api/metrics/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sprase Categorical Crossentropy: because we have sparse labels (for each instance, there is just a target class index from 0 to 9). If we had one target probability per class for each instance, then we need to use `categorical_crossentropy` loss instead. \n",
    "\n",
    "- If we are doing binary classification, we would use `sigmoid` (logistic) activation function and use `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 30, 'steps': 1719}\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the training set was very skewed, with some classes being overrepresented and others underrepresented, it would be useful to set the `class_weight` argument\n",
    "- We can give larger weight to underrepresented classes and lower weight to overrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training accuracy and validation accuracy streadily increase during the training while training loss and validation loss decrease. Not sign of overfitting.\n",
    "\n",
    "The validation loss is still going down so the model has no yet converged. We should continue training.\n",
    "\n",
    "If not satisfied with model performance, we can go back to tune the hyperparameters one by one in this order:   \n",
    "- Learning rate\n",
    "- Optimizer\n",
    "- Number of layers, number of neurons, activation function for each layer, batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3423 - accuracy: 0.8755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3422611951828003, 0.8755000233650208]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Using model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98],\n",
       "       [0.  , 0.  , 0.98, 0.  , 0.02, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_new))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Building a regression MLP using Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike classification, the output layer has a single neuron since we only want to predict a single value.\n",
    "- It uses no activation function\n",
    "- The loss function is the mean squared error.\n",
    "- Since dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.9078 - val_loss: 0.5514\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5422 - val_loss: 0.4789\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5360 - val_loss: 0.4581\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4806 - val_loss: 0.4405\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4591 - val_loss: 0.4493\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4538 - val_loss: 0.4214\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4375 - val_loss: 0.4146\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4337 - val_loss: 0.4118\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4317 - val_loss: 0.4061\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4189 - val_loss: 0.3996\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4158 - val_loss: 0.3961\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4125 - val_loss: 0.3965\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4527 - val_loss: 0.3928\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4217 - val_loss: 0.3875\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4095 - val_loss: 0.3861\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3985 - val_loss: 0.3899\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3986 - val_loss: 0.3885\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3965 - val_loss: 0.3838\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3891 - val_loss: 0.3742\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3920 - val_loss: 0.3751\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 1s 3ms/step - loss: 0.3919\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Building complex models using functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input object. This is a specification of the kind of input the model will get.\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# Create a dense layer with 30 neurons, using ReLU activation. We tell Keras how it should connect the layers together using function like representation.\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# Create a concatenate layer that concatenates the input and output of the second hidden layer.\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "\n",
    "# Create output layer with a single neuron and no activation function.\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# Create a Keras Model, specifying which inputs and outputs to use\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 30)           270         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 30)           930         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['input_1[0][0]',                \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            39          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 2.0193 - val_loss: 0.7286\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7378 - val_loss: 0.6639\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6860 - val_loss: 0.6280\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6509 - val_loss: 0.5989\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6248 - val_loss: 0.5786\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6027 - val_loss: 0.5614\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5859 - val_loss: 0.5476\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5718 - val_loss: 0.5350\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5603 - val_loss: 0.5254\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5498 - val_loss: 0.5164\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5409 - val_loss: 0.5085\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5324 - val_loss: 0.5014\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5252 - val_loss: 0.4961\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5195 - val_loss: 0.4889\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5129 - val_loss: 0.4835\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5080 - val_loss: 0.4804\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5040 - val_loss: 0.4758\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4989 - val_loss: 0.4710\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4966 - val_loss: 0.4686\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4938 - val_loss: 0.4647\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4897\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training lasts several hours, we should not only save our model at the end of training, but also save checkpoints at regular intervals during training to avoid losing everything if computer crashes. We can tell the `fit()` method to save checkpoints by using callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Save checkpoints using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.8513\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.7858\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6827\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6284\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5895\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5574\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5343\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5164\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5030\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4929\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])  \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# save checkpoints\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model/my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use validation set during training, we can set `save_best_only=True` when creating the `ModelCheckpoint`. This way, it will save the model when its performance on the validation set is the best so far. Hence, we do not need to worry about training for too long and overfitting the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 1.7874 - val_loss: 0.7342\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7680 - val_loss: 0.6618\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6789 - val_loss: 0.6258\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6450 - val_loss: 0.5972\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.6183 - val_loss: 0.5751\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5962 - val_loss: 0.5555\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5773 - val_loss: 0.5400\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5613 - val_loss: 0.5259\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5471 - val_loss: 0.5131\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5351 - val_loss: 0.5038\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])  \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model/my_keras_model.h5\",\n",
    "                                                save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"model/my_keras_model.h5\") # roll back to the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `Early Stopping` callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the `patience` argument), and it will optionally roll back to the best model. EarlyStopping will keep track of the best weights and restore them for us at the end of the training so there is no need to restore the best model saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both callbacks to save checkpoints and interrupt training early when there is no more progress to save time.\n",
    "\n",
    "https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 1.9392 - val_loss: 0.9171\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.8627 - val_loss: 0.7158\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7436 - val_loss: 0.6635\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6960 - val_loss: 0.6275\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6617 - val_loss: 0.6004\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.6342 - val_loss: 0.5781\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6109 - val_loss: 0.5576\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5909 - val_loss: 0.5404\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5736 - val_loss: 0.5273\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5585 - val_loss: 0.5143\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5452 - val_loss: 0.5026\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5334 - val_loss: 0.4928\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5231 - val_loss: 0.4841\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5140 - val_loss: 0.4780\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5060 - val_loss: 0.4702\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4988 - val_loss: 0.4641\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4924 - val_loss: 0.4593\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4865 - val_loss: 0.4553\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4815 - val_loss: 0.4505\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4766 - val_loss: 0.4470\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4727 - val_loss: 0.4439\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4689 - val_loss: 0.4410\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4653 - val_loss: 0.4380\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4619 - val_loss: 0.4353\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4589 - val_loss: 0.4328\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4561 - val_loss: 0.4312\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4536 - val_loss: 0.4289\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4509 - val_loss: 0.4277\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4482 - val_loss: 0.4258\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4462 - val_loss: 0.4236\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4436 - val_loss: 0.4220\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4416 - val_loss: 0.4207\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4394 - val_loss: 0.4199\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4375 - val_loss: 0.4174\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4353 - val_loss: 0.4159\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4336 - val_loss: 0.4142\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4317 - val_loss: 0.4130\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4302 - val_loss: 0.4119\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4278 - val_loss: 0.4108\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4267 - val_loss: 0.4098\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4252 - val_loss: 0.4083\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4237 - val_loss: 0.4068\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4219 - val_loss: 0.4064\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4203 - val_loss: 0.4051\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4186 - val_loss: 0.4045\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4172 - val_loss: 0.4026\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4160 - val_loss: 0.4018\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4146 - val_loss: 0.4011\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4128 - val_loss: 0.4003\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4120 - val_loss: 0.3991\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4105 - val_loss: 0.3975\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4090 - val_loss: 0.3971\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4080 - val_loss: 0.3960\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4067 - val_loss: 0.3958\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4057 - val_loss: 0.3942\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4043 - val_loss: 0.3934\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4032 - val_loss: 0.3934\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4013 - val_loss: 0.3922\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4009 - val_loss: 0.3915\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3996 - val_loss: 0.3903\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3985 - val_loss: 0.3894\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3971 - val_loss: 0.3894\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3962 - val_loss: 0.3908\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3953 - val_loss: 0.3876\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3942 - val_loss: 0.3864\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3932 - val_loss: 0.3855\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3920 - val_loss: 0.3884\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3916 - val_loss: 0.3841\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3905 - val_loss: 0.3840\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3895 - val_loss: 0.3826\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3885 - val_loss: 0.3829\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3874 - val_loss: 0.3814\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3867 - val_loss: 0.3803\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3860 - val_loss: 0.3798\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3848 - val_loss: 0.3787\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3839 - val_loss: 0.3786\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3833 - val_loss: 0.3784\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3824 - val_loss: 0.3774\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3812 - val_loss: 0.3758\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3806 - val_loss: 0.3751\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3798 - val_loss: 0.3750\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3791 - val_loss: 0.3737\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3779 - val_loss: 0.3736\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3769 - val_loss: 0.3719\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3762 - val_loss: 0.3726\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3752 - val_loss: 0.3709\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3746 - val_loss: 0.3706\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3740 - val_loss: 0.3694\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3729 - val_loss: 0.3689\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3724 - val_loss: 0.3683\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3715 - val_loss: 0.3679\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3703 - val_loss: 0.3673\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3699 - val_loss: 0.3661\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3693 - val_loss: 0.3656\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3686 - val_loss: 0.3661\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3678 - val_loss: 0.3642\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3671 - val_loss: 0.3640\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3664 - val_loss: 0.3638\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3656 - val_loss: 0.3621\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3647 - val_loss: 0.3616\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])  \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive visualization tool that allows us to:\n",
    "- view the learning curves during training\n",
    "- compare multiple runs\n",
    "- visualize computation graph\n",
    "- analyze training statistics\n",
    "- view images generated by model\n",
    "- visualize complex multidimensional data projected down to 3D and automatically clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to modify the program so that it outputs data to special binary log files\n",
    "- These binary log files are called event files.\n",
    "- Each binary data record is called a summary\n",
    "\n",
    "The TensorBoard server will monitor the log directory and automatically pick up the changes and update the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 3s 6ms/step - loss: 2.0383 - val_loss: 0.9810\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.8437 - val_loss: 0.7598\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7205 - val_loss: 0.6836\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6598 - val_loss: 0.6347\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6180 - val_loss: 0.5993\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5864 - val_loss: 0.5726\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5626 - val_loss: 0.5518\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5443 - val_loss: 0.5347\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5292 - val_loss: 0.5212\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5166 - val_loss: 0.5098\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5058 - val_loss: 0.5005\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4968 - val_loss: 0.4915\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4892 - val_loss: 0.4841\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4820 - val_loss: 0.4780\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4760 - val_loss: 0.4734\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4709 - val_loss: 0.4675\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4662 - val_loss: 0.4631\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4617 - val_loss: 0.4589\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4581 - val_loss: 0.4550\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4543 - val_loss: 0.4517\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4510 - val_loss: 0.4487\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4475 - val_loss: 0.4454\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4449 - val_loss: 0.4425\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4420 - val_loss: 0.4404\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4398 - val_loss: 0.4378\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4376 - val_loss: 0.4355\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4352 - val_loss: 0.4336\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4331 - val_loss: 0.4312\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4311 - val_loss: 0.4293\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4293 - val_loss: 0.4275\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])  \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# Tensorboard()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_train, y_train),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the tensorboard server using the code below and view the UI here: http://localhost:6006/#scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-381a5144af7336fa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-381a5144af7336fa\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard](images/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Fine-Tuning Neural Networks Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practices regarding tuning neural network hyperparameters:    \n",
    "Leslie N. Smith - https://arxiv.org/abs/1803.09820"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Simplified Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Temp\\ipykernel_9420\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 1.4550 - val_loss: 0.6899\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.7141 - val_loss: 0.6232\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6347 - val_loss: 0.5705\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5882 - val_loss: 0.5354\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5552 - val_loss: 0.5133\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5318 - val_loss: 0.4924\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5149 - val_loss: 0.4798\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5009 - val_loss: 0.4755\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4922 - val_loss: 0.4633\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4853 - val_loss: 0.4561\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4788 - val_loss: 0.4535\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4714 - val_loss: 0.4520\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4677 - val_loss: 0.4430\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4627 - val_loss: 0.4408\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4593 - val_loss: 0.4363\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4563 - val_loss: 0.4347\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4509 - val_loss: 0.4305\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4479 - val_loss: 0.4280\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4443 - val_loss: 0.4264\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4403 - val_loss: 0.4248\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4392 - val_loss: 0.4191\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4353 - val_loss: 0.4181\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4332 - val_loss: 0.4156\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4297 - val_loss: 0.4138\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4297 - val_loss: 0.4107\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4261 - val_loss: 0.4093\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4232 - val_loss: 0.4073\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4215 - val_loss: 0.4066\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4216 - val_loss: 0.4042\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4181 - val_loss: 0.4044\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4013\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4136 - val_loss: 0.4016\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.3990\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4099 - val_loss: 0.3994\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.3962\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4065 - val_loss: 0.3942\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4055 - val_loss: 0.3938\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4032 - val_loss: 0.3943\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4021 - val_loss: 0.3921\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4008 - val_loss: 0.3910\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3991 - val_loss: 0.3901\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3988 - val_loss: 0.3885\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3968 - val_loss: 0.3891\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3964 - val_loss: 0.3875\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3935 - val_loss: 0.3880\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3927 - val_loss: 0.3871\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3912 - val_loss: 0.3854\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3922 - val_loss: 0.3851\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3894 - val_loss: 0.3823\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3875 - val_loss: 0.3836\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3882 - val_loss: 0.3823\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3872 - val_loss: 0.3785\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3850 - val_loss: 0.3783\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3850 - val_loss: 0.3776\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3825 - val_loss: 0.3769\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3819 - val_loss: 0.3765\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3803 - val_loss: 0.3753\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3818 - val_loss: 0.3767\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3794 - val_loss: 0.3751\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3783 - val_loss: 0.3735\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3770 - val_loss: 0.3731\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3764 - val_loss: 0.3739\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3761 - val_loss: 0.3730\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3744 - val_loss: 0.3717\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3736 - val_loss: 0.3709\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3733 - val_loss: 0.3702\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3719 - val_loss: 0.3689\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3716 - val_loss: 0.3691\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3699 - val_loss: 0.3668\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3705 - val_loss: 0.3670\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3691 - val_loss: 0.3673\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3683 - val_loss: 0.3671\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3681 - val_loss: 0.3668\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3670 - val_loss: 0.3659\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3663 - val_loss: 0.3653\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3657 - val_loss: 0.3644\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3652 - val_loss: 0.3662\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3650 - val_loss: 0.3633\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3631 - val_loss: 0.3635\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3631 - val_loss: 0.3615\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3628 - val_loss: 0.3650\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3622 - val_loss: 0.3605\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3613 - val_loss: 0.3613\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3602 - val_loss: 0.3623\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3600 - val_loss: 0.3600\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3625 - val_loss: 0.3600\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3596 - val_loss: 0.3592\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3588 - val_loss: 0.3590\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3612 - val_loss: 0.3589\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3579 - val_loss: 0.3598\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3575 - val_loss: 0.3580\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3570 - val_loss: 0.3599\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3602 - val_loss: 0.3579\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3567 - val_loss: 0.3587\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3557 - val_loss: 0.3586\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3548 - val_loss: 0.3581\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3554 - val_loss: 0.3555\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3550 - val_loss: 0.3563\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3535 - val_loss: 0.3552\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3542 - val_loss: 0.3537\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3585\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `GridSearchCV` or `RandomizedSearchCV` to explore the hyperparameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1,100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 6ms/step - loss: 4.7552 - val_loss: 3.9149\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 3.5173 - val_loss: 2.9470\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 2.7233 - val_loss: 2.3304\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.2179 - val_loss: 1.9417\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.8989 - val_loss: 1.6983\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6986 - val_loss: 1.5471\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.5739 - val_loss: 1.4543\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.4964 - val_loss: 1.3971\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.4477 - val_loss: 1.3615\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.4167 - val_loss: 1.3391\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3965 - val_loss: 1.3245\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3828 - val_loss: 1.3149\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3732 - val_loss: 1.3081\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3662 - val_loss: 1.3032\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3609 - val_loss: 1.2994\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3567 - val_loss: 1.2965\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3533 - val_loss: 1.2941\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3504 - val_loss: 1.2920\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3479 - val_loss: 1.2902\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3458 - val_loss: 1.2886\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3439 - val_loss: 1.2872\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3422 - val_loss: 1.2858\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3406 - val_loss: 1.2845\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3391 - val_loss: 1.2833\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3377 - val_loss: 1.2820\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3363 - val_loss: 1.2808\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3350 - val_loss: 1.2795\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3337 - val_loss: 1.2783\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3324 - val_loss: 1.2770\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3311 - val_loss: 1.2756\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3297 - val_loss: 1.2742\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3284 - val_loss: 1.2727\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3270 - val_loss: 1.2712\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3255 - val_loss: 1.2695\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3239 - val_loss: 1.2678\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3222 - val_loss: 1.2659\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3204 - val_loss: 1.2638\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3184 - val_loss: 1.2615\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3163 - val_loss: 1.2591\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3140 - val_loss: 1.2565\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3116 - val_loss: 1.2536\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3089 - val_loss: 1.2504\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3059 - val_loss: 1.2468\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3025 - val_loss: 1.2429\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2988 - val_loss: 1.2384\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2945 - val_loss: 1.2334\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2897 - val_loss: 1.2280\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2845 - val_loss: 1.2219\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2787 - val_loss: 1.2152\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2724 - val_loss: 1.2078\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.2654 - val_loss: 1.1997\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.2575 - val_loss: 1.1906\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2485 - val_loss: 1.1807\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.2387 - val_loss: 1.1696\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 1.2281 - val_loss: 1.1578\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2165 - val_loss: 1.1449\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2036 - val_loss: 1.1307\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 1.1896 - val_loss: 1.1155\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 1.1743 - val_loss: 1.0993\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.1578 - val_loss: 1.0820\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1399 - val_loss: 1.0635\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.1208 - val_loss: 1.0442\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.1010 - val_loss: 1.0244\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0803 - val_loss: 1.0041\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0591 - val_loss: 0.9836\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0374 - val_loss: 0.9629\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0155 - val_loss: 0.9426\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9938 - val_loss: 0.9229\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9727 - val_loss: 0.9039\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9521 - val_loss: 0.8857\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9327 - val_loss: 0.8686\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.9141 - val_loss: 0.8526\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8966 - val_loss: 0.8377\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8803 - val_loss: 0.8237\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8650 - val_loss: 0.8106\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8508 - val_loss: 0.7984\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8375 - val_loss: 0.7869\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8252 - val_loss: 0.7761\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8136 - val_loss: 0.7654\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8025 - val_loss: 0.7551\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7918 - val_loss: 0.7451\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7814 - val_loss: 0.7353\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7716 - val_loss: 0.7259\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7622 - val_loss: 0.7168\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7533 - val_loss: 0.7080\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7447 - val_loss: 0.6994\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7365 - val_loss: 0.6912\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7285 - val_loss: 0.6834\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7207 - val_loss: 0.6757\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7131 - val_loss: 0.6685\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7058 - val_loss: 0.6614\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6986 - val_loss: 0.6545\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6914 - val_loss: 0.6478\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6846 - val_loss: 0.6414\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6781 - val_loss: 0.6354\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6719 - val_loss: 0.6297\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6659 - val_loss: 0.6245\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6604 - val_loss: 0.6195\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6551 - val_loss: 0.6147\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6501 - val_loss: 0.6103\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.6266\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 2.2417 - val_loss: 1.3185\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.2650 - val_loss: 0.9519\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9821 - val_loss: 0.8179\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8514 - val_loss: 0.7443\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7775 - val_loss: 0.6975\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7305 - val_loss: 0.6659\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6980 - val_loss: 0.6430\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6740 - val_loss: 0.6255\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6560 - val_loss: 0.6124\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6417 - val_loss: 0.6020\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6301 - val_loss: 0.5930\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6204 - val_loss: 0.5854\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6122 - val_loss: 0.5788\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6051 - val_loss: 0.5733\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5990 - val_loss: 0.5683\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5934 - val_loss: 0.5635\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5885 - val_loss: 0.5593\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5840 - val_loss: 0.5555\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5798 - val_loss: 0.5525\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5760 - val_loss: 0.5490\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5724 - val_loss: 0.5457\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5691 - val_loss: 0.5432\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5661 - val_loss: 0.5407\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5632 - val_loss: 0.5381\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5606 - val_loss: 0.5356\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5580 - val_loss: 0.5333\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5556 - val_loss: 0.5314\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5532 - val_loss: 0.5289\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5511 - val_loss: 0.5271\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5489 - val_loss: 0.5252\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5469 - val_loss: 0.5234\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5448 - val_loss: 0.5213\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5429 - val_loss: 0.5199\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5410 - val_loss: 0.5182\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5391 - val_loss: 0.5164\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5373 - val_loss: 0.5150\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5354 - val_loss: 0.5136\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5336 - val_loss: 0.5118\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5099\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5301 - val_loss: 0.5090\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5284 - val_loss: 0.5072\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5269 - val_loss: 0.5062\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5252 - val_loss: 0.5045\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5236 - val_loss: 0.5034\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5022\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5207 - val_loss: 0.5009\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5191 - val_loss: 0.5000\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5178 - val_loss: 0.4987\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5165 - val_loss: 0.4973\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5152 - val_loss: 0.4961\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5140 - val_loss: 0.4954\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5128 - val_loss: 0.4943\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5116 - val_loss: 0.4930\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5103 - val_loss: 0.4917\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5090 - val_loss: 0.4908\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5077 - val_loss: 0.4896\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5064 - val_loss: 0.4884\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5053 - val_loss: 0.4874\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5041 - val_loss: 0.4866\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5030 - val_loss: 0.4856\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5020 - val_loss: 0.4846\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5010 - val_loss: 0.4841\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5001 - val_loss: 0.4830\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4990 - val_loss: 0.4823\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4981 - val_loss: 0.4819\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4974 - val_loss: 0.4806\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4964 - val_loss: 0.4796\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.4956 - val_loss: 0.4792\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4948 - val_loss: 0.4787\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4940 - val_loss: 0.4778\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.4935 - val_loss: 0.4775\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4931 - val_loss: 0.4772\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4927 - val_loss: 0.4768\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4922 - val_loss: 0.4763\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4919 - val_loss: 0.4762\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4914 - val_loss: 0.4758\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4910 - val_loss: 0.4755\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4908 - val_loss: 0.4752\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4904 - val_loss: 0.4753\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4901 - val_loss: 0.4747\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4897 - val_loss: 0.4743\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4894 - val_loss: 0.4742\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4892 - val_loss: 0.4740\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4885 - val_loss: 0.4734\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4888 - val_loss: 0.4734\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4883 - val_loss: 0.4734\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4880 - val_loss: 0.4735\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4878 - val_loss: 0.4728\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4874 - val_loss: 0.4724\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4873 - val_loss: 0.4725\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4870 - val_loss: 0.4724\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4867 - val_loss: 0.4722\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4862 - val_loss: 0.4714\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4863 - val_loss: 0.4717\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4860 - val_loss: 0.4711\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4858 - val_loss: 0.4715\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4855 - val_loss: 0.4715\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4852 - val_loss: 0.4711\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4850 - val_loss: 0.4704\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4848 - val_loss: 0.4704\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 4.5439 - val_loss: 3.5710\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 3.0268 - val_loss: 2.3147\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.9673 - val_loss: 1.4920\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3400 - val_loss: 1.0859\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0659 - val_loss: 0.9406\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9680 - val_loss: 0.8908\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9283 - val_loss: 0.8657\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9042 - val_loss: 0.8466\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8853 - val_loss: 0.8304\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8692 - val_loss: 0.8162\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8553 - val_loss: 0.8039\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8434 - val_loss: 0.7935\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8330 - val_loss: 0.7840\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8238 - val_loss: 0.7758\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8158 - val_loss: 0.7686\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8085 - val_loss: 0.7623\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8020 - val_loss: 0.7564\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7959 - val_loss: 0.7511\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7904 - val_loss: 0.7463\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7855 - val_loss: 0.7420\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7811 - val_loss: 0.7381\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7771 - val_loss: 0.7347\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7734 - val_loss: 0.7315\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7700 - val_loss: 0.7285\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7669 - val_loss: 0.7260\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7640 - val_loss: 0.7238\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7613 - val_loss: 0.7216\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7588 - val_loss: 0.7193\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7564 - val_loss: 0.7172\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7541 - val_loss: 0.7152\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7519 - val_loss: 0.7134\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7498 - val_loss: 0.7117\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7478 - val_loss: 0.7100\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7458 - val_loss: 0.7080\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7439 - val_loss: 0.7062\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7420 - val_loss: 0.7047\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7401 - val_loss: 0.7029\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7384 - val_loss: 0.7015\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7366 - val_loss: 0.7000\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7349 - val_loss: 0.6985\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7332 - val_loss: 0.6971\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7315 - val_loss: 0.6958\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7299 - val_loss: 0.6942\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7282 - val_loss: 0.6927\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7266 - val_loss: 0.6913\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7250 - val_loss: 0.6898\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7234 - val_loss: 0.6885\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7219 - val_loss: 0.6872\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7202 - val_loss: 0.6856\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7187 - val_loss: 0.6845\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7172 - val_loss: 0.6832\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7157 - val_loss: 0.6817\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7142 - val_loss: 0.6804\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7128 - val_loss: 0.6791\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7112 - val_loss: 0.6777\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7097 - val_loss: 0.6766\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7083 - val_loss: 0.6752\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7069 - val_loss: 0.6740\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7054 - val_loss: 0.6724\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7039 - val_loss: 0.6713\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7025 - val_loss: 0.6703\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7011 - val_loss: 0.6688\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6997 - val_loss: 0.6677\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6983 - val_loss: 0.6663\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6969 - val_loss: 0.6650\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6955 - val_loss: 0.6639\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6941 - val_loss: 0.6626\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6927 - val_loss: 0.6614\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6914 - val_loss: 0.6599\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6899 - val_loss: 0.6589\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6885 - val_loss: 0.6575\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6871 - val_loss: 0.6562\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6856 - val_loss: 0.6546\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6843 - val_loss: 0.6534\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6829 - val_loss: 0.6521\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6815 - val_loss: 0.6509\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6801 - val_loss: 0.6496\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6787 - val_loss: 0.6484\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6773 - val_loss: 0.6472\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6760 - val_loss: 0.6457\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6746 - val_loss: 0.6444\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6732 - val_loss: 0.6429\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6718 - val_loss: 0.6417\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6704 - val_loss: 0.6404\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6690 - val_loss: 0.6390\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6676 - val_loss: 0.6376\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6662 - val_loss: 0.6364\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6647 - val_loss: 0.6347\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6634 - val_loss: 0.6334\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6619 - val_loss: 0.6320\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6605 - val_loss: 0.6308\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6590 - val_loss: 0.6293\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6575 - val_loss: 0.6282\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6560 - val_loss: 0.6264\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6546 - val_loss: 0.6251\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6531 - val_loss: 0.6237\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6517 - val_loss: 0.6222\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6502 - val_loss: 0.6209\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6488 - val_loss: 0.6194\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6473 - val_loss: 0.6180\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6518\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 2.9582 - val_loss: 1.1986\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9009 - val_loss: 0.6687\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6676 - val_loss: 0.5983\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6275 - val_loss: 0.5792\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6114 - val_loss: 0.5676\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5997 - val_loss: 0.5577\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5902 - val_loss: 0.5498\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5817 - val_loss: 0.5426\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5753 - val_loss: 0.5370\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5693 - val_loss: 0.5321\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5646 - val_loss: 0.5280\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5607 - val_loss: 0.5250\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5571 - val_loss: 0.5223\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5538 - val_loss: 0.5189\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5516 - val_loss: 0.5163\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5501 - val_loss: 0.5153\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5481 - val_loss: 0.5150\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5470 - val_loss: 0.5133\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5455 - val_loss: 0.5114\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5441 - val_loss: 0.5096\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5438 - val_loss: 0.5114\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5431 - val_loss: 0.5110\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5415 - val_loss: 0.5080\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5421 - val_loss: 0.5095\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5415 - val_loss: 0.5110\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5413 - val_loss: 0.5077\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5405 - val_loss: 0.5063\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5412 - val_loss: 0.5081\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5401 - val_loss: 0.5060\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5406 - val_loss: 0.5070\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5402 - val_loss: 0.5068\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5404 - val_loss: 0.5075\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5399 - val_loss: 0.5095\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5074\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5391 - val_loss: 0.5049\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5401 - val_loss: 0.5056\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5393 - val_loss: 0.5046\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5401 - val_loss: 0.5052\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5394 - val_loss: 0.5097\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5397 - val_loss: 0.5088\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5399 - val_loss: 0.5058\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5082\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5385 - val_loss: 0.5047\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5395 - val_loss: 0.5094\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5399 - val_loss: 0.5084\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5398 - val_loss: 0.5068\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5393 - val_loss: 0.5050\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5211\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 2.9584 - val_loss: 1.2265\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9522 - val_loss: 0.7161\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7032 - val_loss: 0.6399\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6525 - val_loss: 0.6153\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6307 - val_loss: 0.5995\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6140 - val_loss: 0.5875\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6009 - val_loss: 0.5755\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5896 - val_loss: 0.5655\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5803 - val_loss: 0.5570\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5725 - val_loss: 0.5497\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5658 - val_loss: 0.5439\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5603 - val_loss: 0.5387\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5553 - val_loss: 0.5343\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5517 - val_loss: 0.5304\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5482 - val_loss: 0.5268\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5457 - val_loss: 0.5243\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5426 - val_loss: 0.5214\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5412 - val_loss: 0.5202\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5395 - val_loss: 0.5181\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5380 - val_loss: 0.5162\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5362 - val_loss: 0.5145\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5360 - val_loss: 0.5145\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5330 - val_loss: 0.5120\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5363 - val_loss: 0.5116\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5340 - val_loss: 0.5107\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5330 - val_loss: 0.5096\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5337 - val_loss: 0.5099\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5331 - val_loss: 0.5092\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5324 - val_loss: 0.5088\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5328 - val_loss: 0.5093\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5083\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5328 - val_loss: 0.5076\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5323 - val_loss: 0.5076\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5309 - val_loss: 0.5104\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5316 - val_loss: 0.5115\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5291 - val_loss: 0.5063\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5061\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5281 - val_loss: 0.5114\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5335 - val_loss: 0.5084\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5313 - val_loss: 0.5109\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5280 - val_loss: 0.5050\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5327 - val_loss: 0.5058\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5295 - val_loss: 0.5085\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5313 - val_loss: 0.5096\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5335 - val_loss: 0.5066\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5323 - val_loss: 0.5058\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5052\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5321 - val_loss: 0.5048\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5284 - val_loss: 0.5116\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5297 - val_loss: 0.5050\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5331 - val_loss: 0.5055\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5337 - val_loss: 0.5057\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5312 - val_loss: 0.5095\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5285 - val_loss: 0.5044\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5317 - val_loss: 0.5042\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5098\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5310 - val_loss: 0.5045\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5298 - val_loss: 0.5099\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5333 - val_loss: 0.5056\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5316 - val_loss: 0.5051\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5308 - val_loss: 0.5038\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5308 - val_loss: 0.5094\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5281 - val_loss: 0.5037\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5334 - val_loss: 0.5044\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5320 - val_loss: 0.5069\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5299 - val_loss: 0.5100\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5331 - val_loss: 0.5086\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5287 - val_loss: 0.5038\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5295 - val_loss: 0.5100\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5299 - val_loss: 0.5046\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5328 - val_loss: 0.5074\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5302 - val_loss: 0.5038\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5299 - val_loss: 0.5095\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5452\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 3.6421 - val_loss: 1.2463\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9131 - val_loss: 0.6180\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6053 - val_loss: 0.5325\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5545 - val_loss: 0.5189\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5428 - val_loss: 0.5151\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5386 - val_loss: 0.5133\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5361 - val_loss: 0.5114\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5343 - val_loss: 0.5099\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5324 - val_loss: 0.5083\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5308 - val_loss: 0.5072\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5300 - val_loss: 0.5057\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5285 - val_loss: 0.5054\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5279 - val_loss: 0.5041\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5271 - val_loss: 0.5038\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5261 - val_loss: 0.5029\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5260 - val_loss: 0.5030\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5254 - val_loss: 0.5024\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5252 - val_loss: 0.5014\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5249 - val_loss: 0.5011\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5244 - val_loss: 0.5004\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5235 - val_loss: 0.5011\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.5002\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5240 - val_loss: 0.5003\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5236 - val_loss: 0.5003\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5236 - val_loss: 0.5007\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5233 - val_loss: 0.4997\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5231 - val_loss: 0.4993\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.5003\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.5004\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5231 - val_loss: 0.5005\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5228 - val_loss: 0.4993\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5002\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4991\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4991\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.4991\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4998\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5231 - val_loss: 0.4993\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4996\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4998\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.5001\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4989\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4993\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5228 - val_loss: 0.4992\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4990\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4992\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4997\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5004\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.5000\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4986\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4996\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.4998\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.4988\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5227 - val_loss: 0.4999\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5222 - val_loss: 0.4987\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5231 - val_loss: 0.4994\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4987\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4995\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5228 - val_loss: 0.4989\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.5003\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5514\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 5ms/step - loss: 3.8076 - val_loss: 2.3659\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.7529 - val_loss: 1.2318\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1526 - val_loss: 0.9739\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9673 - val_loss: 0.8459\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8522 - val_loss: 0.7599\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7748 - val_loss: 0.7028\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7249 - val_loss: 0.6646\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6916 - val_loss: 0.6390\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6687 - val_loss: 0.6204\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6519 - val_loss: 0.6062\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6386 - val_loss: 0.5950\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6278 - val_loss: 0.5853\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6186 - val_loss: 0.5774\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6107 - val_loss: 0.5705\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6036 - val_loss: 0.5642\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5972 - val_loss: 0.5585\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5912 - val_loss: 0.5531\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5857 - val_loss: 0.5481\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5802 - val_loss: 0.5437\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5753 - val_loss: 0.5392\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5705 - val_loss: 0.5350\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5657 - val_loss: 0.5310\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5612 - val_loss: 0.5273\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5569 - val_loss: 0.5232\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5526 - val_loss: 0.5195\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5486 - val_loss: 0.5160\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5442 - val_loss: 0.5124\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5407 - val_loss: 0.5091\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5367 - val_loss: 0.5059\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5331 - val_loss: 0.5026\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5296 - val_loss: 0.4995\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5261 - val_loss: 0.4966\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5227 - val_loss: 0.4937\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5195 - val_loss: 0.4912\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5165 - val_loss: 0.4886\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5133 - val_loss: 0.4860\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5105 - val_loss: 0.4839\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5077 - val_loss: 0.4815\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5049 - val_loss: 0.4793\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5024 - val_loss: 0.4772\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4999 - val_loss: 0.4752\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.4733\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4952 - val_loss: 0.4717\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4930 - val_loss: 0.4699\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4909 - val_loss: 0.4683\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4888 - val_loss: 0.4666\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4868 - val_loss: 0.4655\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4850 - val_loss: 0.4638\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4833 - val_loss: 0.4625\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4815 - val_loss: 0.4611\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4799 - val_loss: 0.4597\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4782 - val_loss: 0.4584\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4768 - val_loss: 0.4569\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4752 - val_loss: 0.4559\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4737 - val_loss: 0.4545\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4724 - val_loss: 0.4535\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4710 - val_loss: 0.4525\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4697 - val_loss: 0.4511\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4684 - val_loss: 0.4500\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4671 - val_loss: 0.4493\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4659 - val_loss: 0.4481\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4646 - val_loss: 0.4471\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4637 - val_loss: 0.4463\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4626 - val_loss: 0.4452\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4616 - val_loss: 0.4443\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4607 - val_loss: 0.4434\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4593 - val_loss: 0.4436\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4590 - val_loss: 0.4418\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4579 - val_loss: 0.4415\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4572 - val_loss: 0.4403\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4564 - val_loss: 0.4398\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4561 - val_loss: 0.4397\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4554 - val_loss: 0.4392\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4548 - val_loss: 0.4386\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4543 - val_loss: 0.4384\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4539 - val_loss: 0.4381\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4533 - val_loss: 0.4380\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4529 - val_loss: 0.4374\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4523 - val_loss: 0.4373\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4518 - val_loss: 0.4367\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4514 - val_loss: 0.4366\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4508 - val_loss: 0.4363\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4503 - val_loss: 0.4356\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4499 - val_loss: 0.4352\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4495 - val_loss: 0.4357\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4490 - val_loss: 0.4351\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4486 - val_loss: 0.4348\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4480 - val_loss: 0.4345\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4476 - val_loss: 0.4338\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4471 - val_loss: 0.4334\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4468 - val_loss: 0.4332\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4463 - val_loss: 0.4329\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4458 - val_loss: 0.4327\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4454 - val_loss: 0.4324\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4450 - val_loss: 0.4322\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4446 - val_loss: 0.4316\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4441 - val_loss: 0.4313\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4438 - val_loss: 0.4309\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4432 - val_loss: 0.4307\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4427 - val_loss: 0.4303\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4316\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 5ms/step - loss: 2.7878 - val_loss: 1.3213\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1541 - val_loss: 0.8672\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8995 - val_loss: 0.7830\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8119 - val_loss: 0.7352\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7632 - val_loss: 0.7062\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7318 - val_loss: 0.6847\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7098 - val_loss: 0.6704\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6935 - val_loss: 0.6574\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6804 - val_loss: 0.6471\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6695 - val_loss: 0.6384\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6603 - val_loss: 0.6310\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6519 - val_loss: 0.6238\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6446 - val_loss: 0.6170\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6381 - val_loss: 0.6110\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6318 - val_loss: 0.6054\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6258 - val_loss: 0.6002\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6202 - val_loss: 0.5951\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6147 - val_loss: 0.5903\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6098 - val_loss: 0.5859\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6047 - val_loss: 0.5812\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6000 - val_loss: 0.5770\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5959 - val_loss: 0.5740\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5917 - val_loss: 0.5697\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5877 - val_loss: 0.5663\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5842 - val_loss: 0.5633\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5803 - val_loss: 0.5630\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5774 - val_loss: 0.5577\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5740 - val_loss: 0.5549\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5708 - val_loss: 0.5523\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5676 - val_loss: 0.5496\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5644 - val_loss: 0.5474\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5614 - val_loss: 0.5464\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5588 - val_loss: 0.5433\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5561 - val_loss: 0.5410\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5535 - val_loss: 0.5395\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5507 - val_loss: 0.5363\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5483 - val_loss: 0.5358\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5460 - val_loss: 0.5323\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5437 - val_loss: 0.5311\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5415 - val_loss: 0.5289\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5394 - val_loss: 0.5269\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5375 - val_loss: 0.5252\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5354 - val_loss: 0.5238\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5332 - val_loss: 0.5218\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5313 - val_loss: 0.5210\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5295 - val_loss: 0.5195\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5278 - val_loss: 0.5172\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5260 - val_loss: 0.5155\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5242 - val_loss: 0.5139\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.5128\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5208 - val_loss: 0.5111\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5192 - val_loss: 0.5097\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5178 - val_loss: 0.5080\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5162 - val_loss: 0.5063\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5148 - val_loss: 0.5052\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5133 - val_loss: 0.5036\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5118 - val_loss: 0.5021\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5104 - val_loss: 0.5012\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5085 - val_loss: 0.4992\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5076 - val_loss: 0.4985\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5067 - val_loss: 0.4966\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5052 - val_loss: 0.4959\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5039 - val_loss: 0.4938\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5026 - val_loss: 0.4937\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5019 - val_loss: 0.4917\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5005 - val_loss: 0.4900\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4996 - val_loss: 0.4896\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4981 - val_loss: 0.4874\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4973 - val_loss: 0.4865\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4960 - val_loss: 0.4851\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4951 - val_loss: 0.4845\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4939 - val_loss: 0.4834\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4929 - val_loss: 0.4823\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4919 - val_loss: 0.4814\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4905 - val_loss: 0.4806\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4897 - val_loss: 0.4798\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4887 - val_loss: 0.4786\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4878 - val_loss: 0.4775\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4870 - val_loss: 0.4773\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4863 - val_loss: 0.4761\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4853 - val_loss: 0.4752\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4847 - val_loss: 0.4742\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4840 - val_loss: 0.4739\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4833 - val_loss: 0.4728\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4826 - val_loss: 0.4721\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4823 - val_loss: 0.4719\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4813 - val_loss: 0.4709\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4810 - val_loss: 0.4708\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4804 - val_loss: 0.4701\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4799 - val_loss: 0.4699\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4792 - val_loss: 0.4688\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4788 - val_loss: 0.4685\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4781 - val_loss: 0.4681\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4776 - val_loss: 0.4675\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4770 - val_loss: 0.4678\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4765 - val_loss: 0.4664\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4761 - val_loss: 0.4664\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4754 - val_loss: 0.4653\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4750 - val_loss: 0.4649\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4746 - val_loss: 0.4649\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4886\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 4.6379 - val_loss: 3.4685\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.9225 - val_loss: 2.2695\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.0197 - val_loss: 1.6768\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6095 - val_loss: 1.4446\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.4473 - val_loss: 1.3530\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3749 - val_loss: 1.3029\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3276 - val_loss: 1.2614\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.2852 - val_loss: 1.2187\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2410 - val_loss: 1.1724\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1922 - val_loss: 1.1201\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1372 - val_loss: 1.0619\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0768 - val_loss: 0.9990\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0121 - val_loss: 0.9323\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9436 - val_loss: 0.8626\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8744 - val_loss: 0.7950\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8101 - val_loss: 0.7365\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7576 - val_loss: 0.6919\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7183 - val_loss: 0.6603\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6904 - val_loss: 0.6393\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6710 - val_loss: 0.6253\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6570 - val_loss: 0.6151\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6458 - val_loss: 0.6064\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6360 - val_loss: 0.5991\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6268 - val_loss: 0.5925\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6183 - val_loss: 0.5859\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6098 - val_loss: 0.5792\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6020 - val_loss: 0.5738\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5949 - val_loss: 0.5684\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5886 - val_loss: 0.5630\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5831 - val_loss: 0.5587\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5779 - val_loss: 0.5549\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5729 - val_loss: 0.5510\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5679 - val_loss: 0.5472\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5633 - val_loss: 0.5439\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5590 - val_loss: 0.5408\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5551 - val_loss: 0.5376\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5515 - val_loss: 0.5348\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5479 - val_loss: 0.5322\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5447 - val_loss: 0.5293\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5415 - val_loss: 0.5264\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5383 - val_loss: 0.5237\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5354 - val_loss: 0.5215\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5325 - val_loss: 0.5192\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5296 - val_loss: 0.5170\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5267 - val_loss: 0.5140\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5241 - val_loss: 0.5122\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5213 - val_loss: 0.5105\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5188 - val_loss: 0.5085\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5162 - val_loss: 0.5052\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5138 - val_loss: 0.5031\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5113 - val_loss: 0.5012\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5090 - val_loss: 0.4986\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5067 - val_loss: 0.4969\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5044 - val_loss: 0.4947\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5021 - val_loss: 0.4927\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5001 - val_loss: 0.4907\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4977 - val_loss: 0.4896\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4960 - val_loss: 0.4871\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4939 - val_loss: 0.4850\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4920 - val_loss: 0.4830\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4900 - val_loss: 0.4810\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4882 - val_loss: 0.4793\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4863 - val_loss: 0.4772\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4846 - val_loss: 0.4755\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4827 - val_loss: 0.4736\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4809 - val_loss: 0.4718\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4790 - val_loss: 0.4699\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4772 - val_loss: 0.4677\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4754 - val_loss: 0.4659\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4735 - val_loss: 0.4643\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4717 - val_loss: 0.4617\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4700 - val_loss: 0.4600\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4682 - val_loss: 0.4581\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4664 - val_loss: 0.4559\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4647 - val_loss: 0.4546\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4630 - val_loss: 0.4530\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4614 - val_loss: 0.4511\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4599 - val_loss: 0.4497\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4583 - val_loss: 0.4485\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4569 - val_loss: 0.4468\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4555 - val_loss: 0.4457\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4541 - val_loss: 0.4441\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4528 - val_loss: 0.4429\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4516 - val_loss: 0.4417\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4502 - val_loss: 0.4407\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4491 - val_loss: 0.4391\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4478 - val_loss: 0.4388\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4468 - val_loss: 0.4376\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4459 - val_loss: 0.4367\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4449 - val_loss: 0.4358\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4438 - val_loss: 0.4345\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4429 - val_loss: 0.4335\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4418 - val_loss: 0.4339\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4413 - val_loss: 0.4324\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4404 - val_loss: 0.4318\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4396 - val_loss: 0.4311\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4388 - val_loss: 0.4298\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4381 - val_loss: 0.4295\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4373 - val_loss: 0.4289\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4366 - val_loss: 0.4282\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4519\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7469 - val_loss: 0.5049\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4988 - val_loss: 0.4690\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4606 - val_loss: 0.4157\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4365 - val_loss: 0.4143\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4168 - val_loss: 0.4090\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3978 - val_loss: 0.3959\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3878 - val_loss: 0.3923\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3803 - val_loss: 0.3784\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3778 - val_loss: 0.3687\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3660 - val_loss: 0.4372\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3639 - val_loss: 0.3602\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3581 - val_loss: 0.3567\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3529 - val_loss: 0.3599\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3471 - val_loss: 0.3386\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3443 - val_loss: 0.3508\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3405 - val_loss: 0.3328\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3284 - val_loss: 0.3349\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3320 - val_loss: 0.3272\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3300 - val_loss: 0.4783\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3282 - val_loss: 0.3351\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3265 - val_loss: 0.3228\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3191 - val_loss: 0.3165\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3175 - val_loss: 0.3210\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3108 - val_loss: 0.3066\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3132 - val_loss: 0.3108\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3102 - val_loss: 0.3797\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3130 - val_loss: 0.3159\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3117 - val_loss: 0.3042\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3091 - val_loss: 0.3073\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3063 - val_loss: 0.3085\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3071 - val_loss: 0.3078\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3075 - val_loss: 0.3049\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3048 - val_loss: 0.3496\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3023 - val_loss: 0.3914\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2999 - val_loss: 0.3245\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3007 - val_loss: 0.3107\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2964 - val_loss: 0.3001\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2987 - val_loss: 0.3032\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2970 - val_loss: 0.3297\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2944 - val_loss: 0.3158\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2972 - val_loss: 0.3062\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2904 - val_loss: 0.3162\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2958 - val_loss: 0.2931\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2918 - val_loss: 0.2926\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2901 - val_loss: 0.3000\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2882 - val_loss: 0.2974\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2929 - val_loss: 0.4251\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2909 - val_loss: 0.2944\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2905 - val_loss: 0.3608\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2885 - val_loss: 0.2946\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2841 - val_loss: 0.2968\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2866 - val_loss: 0.3377\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2854 - val_loss: 0.3128\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2879 - val_loss: 0.3079\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3284\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6877 - val_loss: 0.4721\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4579 - val_loss: 0.4355\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4894 - val_loss: 0.4121\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4199 - val_loss: 0.4143\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4109 - val_loss: 0.3912\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3985 - val_loss: 0.3829\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4013 - val_loss: 0.3786\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3769 - val_loss: 0.3945\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3713 - val_loss: 0.3602\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3794 - val_loss: 0.3844\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3628 - val_loss: 0.3515\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3610 - val_loss: 0.4094\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3551 - val_loss: 0.3368\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3465 - val_loss: 0.3501\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3526 - val_loss: 0.3503\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3366 - val_loss: 0.3614\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3371 - val_loss: 0.3295\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3340 - val_loss: 0.3358\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3294 - val_loss: 0.3309\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3332 - val_loss: 0.3338\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3243 - val_loss: 0.3166\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3265 - val_loss: 0.3230\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3216 - val_loss: 0.3236\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3169 - val_loss: 0.3305\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3163 - val_loss: 0.3146\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3141 - val_loss: 0.3298\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3080 - val_loss: 0.3157\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3131 - val_loss: 0.3047\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3080 - val_loss: 0.3221\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3066 - val_loss: 0.3248\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3097 - val_loss: 0.3267\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3049 - val_loss: 0.3211\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3071 - val_loss: 0.3011\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3007 - val_loss: 0.3031\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3012 - val_loss: 0.3045\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3037 - val_loss: 0.3014\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2986 - val_loss: 0.3043\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2987 - val_loss: 0.3218\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2975 - val_loss: 0.3021\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2938 - val_loss: 0.3063\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2961 - val_loss: 0.3023\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2950 - val_loss: 0.3079\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2941 - val_loss: 0.3466\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3587\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 3.5414 - val_loss: 1.4789\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0824 - val_loss: 0.7264\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6776 - val_loss: 0.5719\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5910 - val_loss: 0.5378\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5707 - val_loss: 0.5291\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5644 - val_loss: 0.5252\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5618 - val_loss: 0.5230\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5603 - val_loss: 0.5217\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5580 - val_loss: 0.5212\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5563 - val_loss: 0.5196\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5549 - val_loss: 0.5194\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5528 - val_loss: 0.5161\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5519 - val_loss: 0.5143\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5517 - val_loss: 0.5145\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5501 - val_loss: 0.5156\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5494 - val_loss: 0.5152\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5480 - val_loss: 0.5120\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5476 - val_loss: 0.5114\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5472 - val_loss: 0.5129\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5453 - val_loss: 0.5092\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5460 - val_loss: 0.5088\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5451 - val_loss: 0.5082\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5448 - val_loss: 0.5077\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5444 - val_loss: 0.5105\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5440 - val_loss: 0.5108\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5437 - val_loss: 0.5094\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5431 - val_loss: 0.5102\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5430 - val_loss: 0.5081\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5427 - val_loss: 0.5093\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5424 - val_loss: 0.5074\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5419 - val_loss: 0.5062\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5416 - val_loss: 0.5054\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5418 - val_loss: 0.5054\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5416 - val_loss: 0.5085\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5405 - val_loss: 0.5065\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5410 - val_loss: 0.5054\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5414 - val_loss: 0.5066\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5408 - val_loss: 0.5080\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5053\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5403 - val_loss: 0.5047\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5403 - val_loss: 0.5041\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5406 - val_loss: 0.5047\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5406 - val_loss: 0.5060\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5403 - val_loss: 0.5075\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5402 - val_loss: 0.5064\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5068\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5398 - val_loss: 0.5080\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5399 - val_loss: 0.5063\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5398 - val_loss: 0.5070\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5390 - val_loss: 0.5044\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5057\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5289\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 4.2388 - val_loss: 2.1510\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.5790 - val_loss: 1.0103\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9186 - val_loss: 0.7274\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7347 - val_loss: 0.6463\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6749 - val_loss: 0.6180\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6493 - val_loss: 0.6037\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6329 - val_loss: 0.5940\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6222 - val_loss: 0.5842\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6123 - val_loss: 0.5764\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6044 - val_loss: 0.5690\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5968 - val_loss: 0.5626\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5911 - val_loss: 0.5564\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5845 - val_loss: 0.5511\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5794 - val_loss: 0.5463\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5747 - val_loss: 0.5418\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5711 - val_loss: 0.5379\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5662 - val_loss: 0.5346\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5636 - val_loss: 0.5316\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5602 - val_loss: 0.5280\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5570 - val_loss: 0.5253\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5549 - val_loss: 0.5229\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5525 - val_loss: 0.5220\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5506 - val_loss: 0.5189\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5489 - val_loss: 0.5187\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5473 - val_loss: 0.5159\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5429 - val_loss: 0.5150\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5443 - val_loss: 0.5148\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5428 - val_loss: 0.5123\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5411 - val_loss: 0.5105\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5408 - val_loss: 0.5109\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5405 - val_loss: 0.5105\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5400 - val_loss: 0.5091\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5373 - val_loss: 0.5076\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5375 - val_loss: 0.5090\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5364 - val_loss: 0.5087\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5368 - val_loss: 0.5067\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5347 - val_loss: 0.5051\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5345 - val_loss: 0.5077\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5349 - val_loss: 0.5052\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5344 - val_loss: 0.5044\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5335 - val_loss: 0.5069\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5352 - val_loss: 0.5055\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5337 - val_loss: 0.5070\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5345 - val_loss: 0.5060\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5338 - val_loss: 0.5062\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5301 - val_loss: 0.5027\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5322 - val_loss: 0.5059\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5306 - val_loss: 0.5028\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5320 - val_loss: 0.5061\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5334 - val_loss: 0.5055\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5321 - val_loss: 0.5032\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5321 - val_loss: 0.5027\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5332 - val_loss: 0.5029\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5312 - val_loss: 0.5061\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5327 - val_loss: 0.5044\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5314 - val_loss: 0.5032\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5379\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 3.7967 - val_loss: 1.7492\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.2164 - val_loss: 0.7769\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6913 - val_loss: 0.5655\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5713 - val_loss: 0.5191\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5435 - val_loss: 0.5097\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5358 - val_loss: 0.5078\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5335 - val_loss: 0.5067\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5318 - val_loss: 0.5061\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5309 - val_loss: 0.5054\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5302 - val_loss: 0.5048\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5293 - val_loss: 0.5042\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5284 - val_loss: 0.5033\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5282 - val_loss: 0.5030\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5276 - val_loss: 0.5027\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5269 - val_loss: 0.5025\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5267 - val_loss: 0.5019\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5264 - val_loss: 0.5017\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5256 - val_loss: 0.5019\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5256 - val_loss: 0.5008\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5254 - val_loss: 0.5006\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5251 - val_loss: 0.5007\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5244 - val_loss: 0.5011\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5248 - val_loss: 0.5007\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5242 - val_loss: 0.4998\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5243 - val_loss: 0.4998\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.4996\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5241 - val_loss: 0.4997\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5239 - val_loss: 0.4998\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5232 - val_loss: 0.4992\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5238 - val_loss: 0.4992\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.4998\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5234 - val_loss: 0.4997\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5233 - val_loss: 0.4994\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5232 - val_loss: 0.4995\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5231 - val_loss: 0.4999\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5231 - val_loss: 0.4993\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5228 - val_loss: 0.4989\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5230 - val_loss: 0.4995\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4995\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5230 - val_loss: 0.4993\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5228 - val_loss: 0.4996\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.5001\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5229 - val_loss: 0.4995\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5228 - val_loss: 0.4992\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5226 - val_loss: 0.4989\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4994\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5224 - val_loss: 0.5001\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4995\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4996\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5226 - val_loss: 0.4989\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4995\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4996\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5224 - val_loss: 0.4997\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5227 - val_loss: 0.4995\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4995\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4988\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5225 - val_loss: 0.4993\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4989\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4993\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5222 - val_loss: 0.4997\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4999\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4995\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4992\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4992\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4989\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.4987\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4987\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.4990\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4995\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5225 - val_loss: 0.4994\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5223 - val_loss: 0.4988\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.4997\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.4992\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5223 - val_loss: 0.4995\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5222 - val_loss: 0.4996\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4992\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5221 - val_loss: 0.4996\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.5552\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 5ms/step - loss: 0.8823 - val_loss: 0.5550\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6543 - val_loss: 0.5082\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5172 - val_loss: 0.4599\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4769 - val_loss: 0.4379\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4596 - val_loss: 0.4281\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4483 - val_loss: 0.4210\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4349 - val_loss: 0.4087\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4258 - val_loss: 0.4082\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4209 - val_loss: 0.3987\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4111 - val_loss: 0.3976\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4056 - val_loss: 0.3923\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3992 - val_loss: 0.3913\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3947 - val_loss: 0.3805\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3883 - val_loss: 0.3833\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3854 - val_loss: 0.3733\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3763 - val_loss: 0.3680\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3746 - val_loss: 0.3592\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3694 - val_loss: 0.3585\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3664 - val_loss: 0.3552\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3638 - val_loss: 0.3561\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3596 - val_loss: 0.3490\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3551 - val_loss: 0.3526\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3539 - val_loss: 0.3434\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3509 - val_loss: 0.3411\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3467 - val_loss: 0.3523\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3451 - val_loss: 0.3330\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3411 - val_loss: 0.3412\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3475 - val_loss: 0.3374\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3421 - val_loss: 0.3310\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3361 - val_loss: 0.3236\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3346 - val_loss: 0.3271\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3315 - val_loss: 0.3305\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3274 - val_loss: 0.3293\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3308 - val_loss: 0.3277\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3319 - val_loss: 0.3181\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3261 - val_loss: 0.3356\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3229 - val_loss: 0.3223\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3312 - val_loss: 0.3141\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3214 - val_loss: 0.3144\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3196 - val_loss: 0.3157\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3192 - val_loss: 0.3170\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3184 - val_loss: 0.3151\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3177 - val_loss: 0.3109\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3198 - val_loss: 0.3155\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3191 - val_loss: 0.3263\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3903 - val_loss: 0.3133\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3327 - val_loss: 0.3220\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3220 - val_loss: 0.3136\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3188 - val_loss: 0.3059\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3160 - val_loss: 0.3103\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3144 - val_loss: 0.3077\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3101 - val_loss: 0.3029\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3156 - val_loss: 0.3133\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3097 - val_loss: 0.3039\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3096 - val_loss: 0.3095\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3076 - val_loss: 0.3016\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3074 - val_loss: 0.3148\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3086 - val_loss: 0.3004\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3039 - val_loss: 0.3010\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3050 - val_loss: 0.3012\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3044 - val_loss: 0.2945\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3034 - val_loss: 0.3013\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3043 - val_loss: 0.3195\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3038 - val_loss: 0.3126\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3006 - val_loss: 0.2925\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3012 - val_loss: 0.3128\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3019 - val_loss: 0.3129\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2996 - val_loss: 0.2972\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2991 - val_loss: 0.2919\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2971 - val_loss: 0.2908\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2969 - val_loss: 0.2936\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2970 - val_loss: 0.2978\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2968 - val_loss: 0.3155\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2957 - val_loss: 0.2935\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2954 - val_loss: 0.3014\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2947 - val_loss: 0.2902\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2945 - val_loss: 0.3097\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2946 - val_loss: 0.2884\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2929 - val_loss: 0.2882\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2923 - val_loss: 0.2897\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2924 - val_loss: 0.3053\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2932 - val_loss: 0.3108\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2917 - val_loss: 0.2990\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2905 - val_loss: 0.2856\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2919 - val_loss: 0.2947\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2905 - val_loss: 0.3047\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2907 - val_loss: 0.2848\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2899 - val_loss: 0.2992\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2925 - val_loss: 0.2873\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2883 - val_loss: 0.2835\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2885 - val_loss: 0.2848\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2876 - val_loss: 0.2920\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2872 - val_loss: 0.2926\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2871 - val_loss: 0.2883\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2876 - val_loss: 0.2895\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2894 - val_loss: 0.2906\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2866 - val_loss: 0.2959\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2858 - val_loss: 0.2857\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2857 - val_loss: 0.2866\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2862 - val_loss: 0.2862\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3027\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 5ms/step - loss: 0.9581 - val_loss: 0.5316\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5331 - val_loss: 0.4740\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4883 - val_loss: 0.4496\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4767 - val_loss: 0.4454\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4488 - val_loss: 0.4256\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4338 - val_loss: 0.4164\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4225 - val_loss: 0.4166\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4143 - val_loss: 0.4043\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4081 - val_loss: 0.3969\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4020 - val_loss: 0.3961\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3943 - val_loss: 0.3924\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3913 - val_loss: 0.3892\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3833 - val_loss: 0.3847\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3798 - val_loss: 0.3834\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3766 - val_loss: 0.3793\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3730 - val_loss: 0.3721\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3682 - val_loss: 0.3715\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3631 - val_loss: 0.3737\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3597 - val_loss: 0.3657\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3578 - val_loss: 0.3609\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3531 - val_loss: 0.3639\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3500 - val_loss: 0.3672\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3488 - val_loss: 0.3525\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3443 - val_loss: 0.3575\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3437 - val_loss: 0.3515\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3407 - val_loss: 0.3467\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3376 - val_loss: 0.3565\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3349 - val_loss: 0.3434\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3353 - val_loss: 0.3462\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3350 - val_loss: 0.3363\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3296 - val_loss: 0.3371\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3285 - val_loss: 0.3377\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3272 - val_loss: 0.3331\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3262 - val_loss: 0.3451\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3243 - val_loss: 0.3339\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3244 - val_loss: 0.3281\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3227 - val_loss: 0.3323\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3198 - val_loss: 0.3316\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3180 - val_loss: 0.3375\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3187 - val_loss: 0.3289\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3175 - val_loss: 0.3307\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3155 - val_loss: 0.3205\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3150 - val_loss: 0.3268\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3125 - val_loss: 0.3166\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3099 - val_loss: 0.3251\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3123 - val_loss: 0.3165\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3098 - val_loss: 0.3528\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3082 - val_loss: 0.3186\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3065 - val_loss: 0.3165\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3056 - val_loss: 0.3127\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3025 - val_loss: 0.3154\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3053 - val_loss: 0.3117\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.3199\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3052 - val_loss: 0.3088\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3041 - val_loss: 0.3044\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3003 - val_loss: 0.3065\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2988 - val_loss: 0.3140\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3014 - val_loss: 0.3055\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3000 - val_loss: 0.3026\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2995 - val_loss: 0.3088\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2953 - val_loss: 0.3065\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2945 - val_loss: 0.3032\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2942 - val_loss: 0.3269\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2939 - val_loss: 0.3036\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2924 - val_loss: 0.3014\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2923 - val_loss: 0.3027\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2906 - val_loss: 0.3046\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2895 - val_loss: 0.2963\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2912 - val_loss: 0.3168\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2890 - val_loss: 0.2959\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2893 - val_loss: 0.2948\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2892 - val_loss: 0.2996\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2867 - val_loss: 0.3164\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2883 - val_loss: 0.2976\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2896 - val_loss: 0.2920\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2920 - val_loss: 0.2951\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2935 - val_loss: 0.2932\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2864 - val_loss: 0.2952\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2840 - val_loss: 0.2977\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2826 - val_loss: 0.2976\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2813 - val_loss: 0.3005\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2804 - val_loss: 0.2989\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2821 - val_loss: 0.2948\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2798 - val_loss: 0.2953\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2798 - val_loss: 0.2899\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2776 - val_loss: 0.3103\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2779 - val_loss: 0.2880\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2799 - val_loss: 0.2893\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2777 - val_loss: 0.2901\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2772 - val_loss: 0.2872\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2761 - val_loss: 0.2902\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2759 - val_loss: 0.2945\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2765 - val_loss: 0.2913\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2752 - val_loss: 0.2902\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2778 - val_loss: 0.3006\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2751 - val_loss: 0.3021\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2768 - val_loss: 0.2938\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2803 - val_loss: 0.2840\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2738 - val_loss: 0.2932\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2750 - val_loss: 0.2867\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3973\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9465 - val_loss: 0.5817\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7175 - val_loss: 0.5023\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5010 - val_loss: 0.4974\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5225 - val_loss: 0.4381\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5852 - val_loss: 0.4335\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4510 - val_loss: 0.4169\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4246 - val_loss: 0.4079\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4131 - val_loss: 0.4043\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4076 - val_loss: 0.3985\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3988 - val_loss: 0.3923\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3931 - val_loss: 0.3836\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3867 - val_loss: 0.3818\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3819 - val_loss: 0.3762\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3790 - val_loss: 0.3727\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3726 - val_loss: 0.3738\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3721 - val_loss: 0.3769\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3667 - val_loss: 0.3627\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3630 - val_loss: 0.3572\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3592 - val_loss: 0.3556\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3568 - val_loss: 0.3555\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3533 - val_loss: 0.3496\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3507 - val_loss: 0.3532\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3483 - val_loss: 0.3550\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3480 - val_loss: 0.3430\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3430 - val_loss: 0.3400\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3411 - val_loss: 0.3412\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3394 - val_loss: 0.3410\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3368 - val_loss: 0.3313\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3346 - val_loss: 0.3400\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3330 - val_loss: 0.3565\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3312 - val_loss: 0.3324\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3283 - val_loss: 0.3316\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3268 - val_loss: 0.3367\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3255 - val_loss: 0.3309\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3258 - val_loss: 0.3220\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3229 - val_loss: 0.3291\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3215 - val_loss: 0.3249\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3184 - val_loss: 0.3192\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3181 - val_loss: 0.3179\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3182 - val_loss: 0.3227\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3160 - val_loss: 0.3167\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3138 - val_loss: 0.3192\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3144 - val_loss: 0.3137\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3129 - val_loss: 0.3242\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3116 - val_loss: 0.3237\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.3246\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3113 - val_loss: 0.3083\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3091 - val_loss: 0.3148\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3088 - val_loss: 0.3079\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3076 - val_loss: 0.3127\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3063 - val_loss: 0.3248\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3078 - val_loss: 0.3071\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3069 - val_loss: 0.3090\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3048 - val_loss: 0.3026\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3033 - val_loss: 0.3169\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3032 - val_loss: 0.3118\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3037 - val_loss: 0.3026\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3035 - val_loss: 0.3029\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3023 - val_loss: 0.3057\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3020 - val_loss: 0.3128\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3005 - val_loss: 0.3080\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.3055\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3005 - val_loss: 0.3029\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.2987 - val_loss: 0.2976\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.2992 - val_loss: 0.3147\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 2s 8ms/step - loss: 0.2978 - val_loss: 0.3090\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2957 - val_loss: 0.3028\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2969 - val_loss: 0.3047\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2966 - val_loss: 0.3378\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2960 - val_loss: 0.2970\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2952 - val_loss: 0.3124\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.2942 - val_loss: 0.3029\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2946 - val_loss: 0.3161\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2936 - val_loss: 0.2971\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2941 - val_loss: 0.3018\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2940 - val_loss: 0.3039\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2918 - val_loss: 0.2958\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2936 - val_loss: 0.3153\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2934 - val_loss: 0.2977\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2919 - val_loss: 0.3029\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2913 - val_loss: 0.2947\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2910 - val_loss: 0.2931\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2902 - val_loss: 0.2995\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2902 - val_loss: 0.2944\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2912 - val_loss: 0.3027\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2944 - val_loss: 0.3084\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2914 - val_loss: 0.3132\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2906 - val_loss: 0.2949\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2897 - val_loss: 0.3068\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2882 - val_loss: 0.2957\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2884 - val_loss: 0.3035\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2868 - val_loss: 0.2988\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3229\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 2s 6ms/step - loss: 2.9117 - val_loss: 1.5113\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3439 - val_loss: 0.9252\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9496 - val_loss: 0.7792\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8233 - val_loss: 0.7233\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7688 - val_loss: 0.6914\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7386 - val_loss: 0.6693\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7165 - val_loss: 0.6516\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6980 - val_loss: 0.6360\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6822 - val_loss: 0.6226\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6677 - val_loss: 0.6102\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6543 - val_loss: 0.5988\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6421 - val_loss: 0.5883\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6308 - val_loss: 0.5788\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6201 - val_loss: 0.5697\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6103 - val_loss: 0.5614\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6010 - val_loss: 0.5535\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5924 - val_loss: 0.5459\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5839 - val_loss: 0.5393\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5765 - val_loss: 0.5325\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5691 - val_loss: 0.5262\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5622 - val_loss: 0.5208\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5560 - val_loss: 0.5151\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5498 - val_loss: 0.5102\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5441 - val_loss: 0.5054\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5389 - val_loss: 0.5007\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5339 - val_loss: 0.4967\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5292 - val_loss: 0.4925\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5247 - val_loss: 0.4889\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5205 - val_loss: 0.4853\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5166 - val_loss: 0.4821\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5129 - val_loss: 0.4789\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5093 - val_loss: 0.4760\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5061 - val_loss: 0.4733\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5030 - val_loss: 0.4706\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5001 - val_loss: 0.4682\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.4662\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4947 - val_loss: 0.4639\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4921 - val_loss: 0.4617\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4899 - val_loss: 0.4600\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4875 - val_loss: 0.4583\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4854 - val_loss: 0.4561\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4832 - val_loss: 0.4544\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4815 - val_loss: 0.4530\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4796 - val_loss: 0.4516\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4775 - val_loss: 0.4502\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4760 - val_loss: 0.4488\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4743 - val_loss: 0.4475\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4728 - val_loss: 0.4460\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4711 - val_loss: 0.4446\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4697 - val_loss: 0.4435\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4683 - val_loss: 0.4423\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4669 - val_loss: 0.4408\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4654 - val_loss: 0.4398\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4640 - val_loss: 0.4383\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4628 - val_loss: 0.4373\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4615 - val_loss: 0.4363\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4603 - val_loss: 0.4352\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4590 - val_loss: 0.4340\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4579 - val_loss: 0.4333\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4567 - val_loss: 0.4322\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4556 - val_loss: 0.4312\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4543 - val_loss: 0.4304\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4534 - val_loss: 0.4293\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4523 - val_loss: 0.4284\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4513 - val_loss: 0.4276\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4502 - val_loss: 0.4267\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4493 - val_loss: 0.4260\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4483 - val_loss: 0.4254\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4473 - val_loss: 0.4246\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4464 - val_loss: 0.4238\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4455 - val_loss: 0.4229\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4446 - val_loss: 0.4223\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4437 - val_loss: 0.4215\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4428 - val_loss: 0.4210\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4419 - val_loss: 0.4202\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4413 - val_loss: 0.4194\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4404 - val_loss: 0.4188\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4395 - val_loss: 0.4182\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4390 - val_loss: 0.4176\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4381 - val_loss: 0.4170\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4373 - val_loss: 0.4164\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4366 - val_loss: 0.4158\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4358 - val_loss: 0.4151\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4352 - val_loss: 0.4146\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4345 - val_loss: 0.4141\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4337 - val_loss: 0.4136\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4331 - val_loss: 0.4130\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4324 - val_loss: 0.4123\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4318 - val_loss: 0.4118\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4310 - val_loss: 0.4114\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4304 - val_loss: 0.4107\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4298 - val_loss: 0.4102\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4292 - val_loss: 0.4096\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4286 - val_loss: 0.4092\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4279 - val_loss: 0.4088\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4272 - val_loss: 0.4087\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4268 - val_loss: 0.4080\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4262 - val_loss: 0.4074\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4256 - val_loss: 0.4070\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4250 - val_loss: 0.4064\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4157\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 2.7790 - val_loss: 1.2498\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0475 - val_loss: 0.7951\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7791 - val_loss: 0.6892\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7000 - val_loss: 0.6501\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6669 - val_loss: 0.6298\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6470 - val_loss: 0.6139\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6316 - val_loss: 0.6017\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6185 - val_loss: 0.5902\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6066 - val_loss: 0.5804\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5960 - val_loss: 0.5705\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5862 - val_loss: 0.5615\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5773 - val_loss: 0.5532\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5689 - val_loss: 0.5456\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5612 - val_loss: 0.5383\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5539 - val_loss: 0.5318\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5473 - val_loss: 0.5261\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5412 - val_loss: 0.5201\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5352 - val_loss: 0.5153\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5298 - val_loss: 0.5098\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5248 - val_loss: 0.5056\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5200 - val_loss: 0.5009\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5155 - val_loss: 0.4972\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5115 - val_loss: 0.4931\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5074 - val_loss: 0.4899\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5038 - val_loss: 0.4859\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5004 - val_loss: 0.4829\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4970 - val_loss: 0.4799\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4941 - val_loss: 0.4774\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4909 - val_loss: 0.4749\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4882 - val_loss: 0.4721\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4857 - val_loss: 0.4703\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4832 - val_loss: 0.4678\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4809 - val_loss: 0.4655\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4786 - val_loss: 0.4635\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4767 - val_loss: 0.4619\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4744 - val_loss: 0.4598\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4725 - val_loss: 0.4582\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4706 - val_loss: 0.4567\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4690 - val_loss: 0.4555\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4672 - val_loss: 0.4537\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4657 - val_loss: 0.4524\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4638 - val_loss: 0.4508\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4623 - val_loss: 0.4497\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4611 - val_loss: 0.4486\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4597 - val_loss: 0.4473\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4583 - val_loss: 0.4460\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4570 - val_loss: 0.4450\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4555 - val_loss: 0.4444\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4545 - val_loss: 0.4428\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4531 - val_loss: 0.4422\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4519 - val_loss: 0.4407\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4507 - val_loss: 0.4393\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4496 - val_loss: 0.4392\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4484 - val_loss: 0.4379\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4475 - val_loss: 0.4370\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4463 - val_loss: 0.4363\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4454 - val_loss: 0.4354\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4443 - val_loss: 0.4346\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4433 - val_loss: 0.4342\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4423 - val_loss: 0.4329\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4414 - val_loss: 0.4321\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4404 - val_loss: 0.4319\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4394 - val_loss: 0.4310\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4385 - val_loss: 0.4299\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4375 - val_loss: 0.4292\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4368 - val_loss: 0.4289\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4360 - val_loss: 0.4279\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4350 - val_loss: 0.4271\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4342 - val_loss: 0.4265\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4333 - val_loss: 0.4264\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4324 - val_loss: 0.4250\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4317 - val_loss: 0.4243\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4308 - val_loss: 0.4240\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4302 - val_loss: 0.4236\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4294 - val_loss: 0.4230\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4287 - val_loss: 0.4226\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4279 - val_loss: 0.4215\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4270 - val_loss: 0.4217\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4266 - val_loss: 0.4208\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4258 - val_loss: 0.4198\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4250 - val_loss: 0.4195\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4242 - val_loss: 0.4191\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4237 - val_loss: 0.4186\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4229 - val_loss: 0.4178\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4221 - val_loss: 0.4180\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4216 - val_loss: 0.4173\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4209 - val_loss: 0.4163\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4202 - val_loss: 0.4160\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4196 - val_loss: 0.4152\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4188 - val_loss: 0.4156\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4182 - val_loss: 0.4142\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4177 - val_loss: 0.4137\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4170 - val_loss: 0.4130\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4164 - val_loss: 0.4125\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4158 - val_loss: 0.4128\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4151 - val_loss: 0.4115\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4146 - val_loss: 0.4118\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4139 - val_loss: 0.4108\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4133 - val_loss: 0.4106\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4125 - val_loss: 0.4098\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4380\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 3.5683 - val_loss: 1.6609\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3529 - val_loss: 0.8867\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8959 - val_loss: 0.7527\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7804 - val_loss: 0.7092\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7381 - val_loss: 0.6836\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7126 - val_loss: 0.6647\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6926 - val_loss: 0.6473\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6748 - val_loss: 0.6319\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6584 - val_loss: 0.6178\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6435 - val_loss: 0.6048\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6297 - val_loss: 0.5923\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6170 - val_loss: 0.5813\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6050 - val_loss: 0.5705\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5942 - val_loss: 0.5613\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5840 - val_loss: 0.5525\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5745 - val_loss: 0.5442\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5660 - val_loss: 0.5373\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5581 - val_loss: 0.5302\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5508 - val_loss: 0.5240\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5441 - val_loss: 0.5180\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5376 - val_loss: 0.5126\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5315 - val_loss: 0.5083\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5263 - val_loss: 0.5032\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5212 - val_loss: 0.4988\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5164 - val_loss: 0.4946\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5120 - val_loss: 0.4909\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5078 - val_loss: 0.4876\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5040 - val_loss: 0.4843\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5004 - val_loss: 0.4812\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4970 - val_loss: 0.4782\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4936 - val_loss: 0.4760\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4907 - val_loss: 0.4728\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4877 - val_loss: 0.4705\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4851 - val_loss: 0.4683\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4824 - val_loss: 0.4661\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4800 - val_loss: 0.4640\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4776 - val_loss: 0.4616\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4754 - val_loss: 0.4599\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4731 - val_loss: 0.4580\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4711 - val_loss: 0.4562\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4691 - val_loss: 0.4544\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4671 - val_loss: 0.4528\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4652 - val_loss: 0.4511\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4635 - val_loss: 0.4498\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4617 - val_loss: 0.4486\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4601 - val_loss: 0.4466\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4584 - val_loss: 0.4451\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4567 - val_loss: 0.4440\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4553 - val_loss: 0.4428\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4537 - val_loss: 0.4414\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4522 - val_loss: 0.4403\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4510 - val_loss: 0.4389\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4496 - val_loss: 0.4379\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4484 - val_loss: 0.4367\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4472 - val_loss: 0.4355\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4459 - val_loss: 0.4347\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4448 - val_loss: 0.4336\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4437 - val_loss: 0.4324\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4426 - val_loss: 0.4315\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4415 - val_loss: 0.4306\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4403 - val_loss: 0.4298\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4394 - val_loss: 0.4285\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4383 - val_loss: 0.4277\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4374 - val_loss: 0.4269\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4364 - val_loss: 0.4262\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4355 - val_loss: 0.4257\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4346 - val_loss: 0.4248\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4337 - val_loss: 0.4239\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4328 - val_loss: 0.4228\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4319 - val_loss: 0.4222\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4311 - val_loss: 0.4216\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4302 - val_loss: 0.4210\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4293 - val_loss: 0.4200\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4286 - val_loss: 0.4194\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4277 - val_loss: 0.4189\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4270 - val_loss: 0.4181\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4261 - val_loss: 0.4177\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4254 - val_loss: 0.4169\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4246 - val_loss: 0.4162\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4238 - val_loss: 0.4156\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4230 - val_loss: 0.4148\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4223 - val_loss: 0.4145\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4216 - val_loss: 0.4137\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4209 - val_loss: 0.4132\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4201 - val_loss: 0.4129\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4195 - val_loss: 0.4120\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4188 - val_loss: 0.4117\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4181 - val_loss: 0.4110\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.4106\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4166 - val_loss: 0.4098\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4160 - val_loss: 0.4094\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4154 - val_loss: 0.4091\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4089\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4140 - val_loss: 0.4081\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4133 - val_loss: 0.4080\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4129 - val_loss: 0.4071\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4067\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4116 - val_loss: 0.4062\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4110 - val_loss: 0.4056\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4053\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4304\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 3.3800 - val_loss: 1.8828\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.5035 - val_loss: 1.0681\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0358 - val_loss: 0.8614\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8947 - val_loss: 0.7970\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8402 - val_loss: 0.7667\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8120 - val_loss: 0.7471\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7927 - val_loss: 0.7315\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7771 - val_loss: 0.7179\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7632 - val_loss: 0.7054\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7506 - val_loss: 0.6939\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7386 - val_loss: 0.6834\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7276 - val_loss: 0.6731\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7170 - val_loss: 0.6636\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7069 - val_loss: 0.6544\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6974 - val_loss: 0.6458\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6883 - val_loss: 0.6376\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6797 - val_loss: 0.6297\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6716 - val_loss: 0.6223\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6636 - val_loss: 0.6154\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6563 - val_loss: 0.6086\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6491 - val_loss: 0.6020\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6423 - val_loss: 0.5958\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6357 - val_loss: 0.5899\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6295 - val_loss: 0.5842\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6235 - val_loss: 0.5789\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6178 - val_loss: 0.5737\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6123 - val_loss: 0.5689\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6070 - val_loss: 0.5641\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6020 - val_loss: 0.5597\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5970 - val_loss: 0.5553\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5925 - val_loss: 0.5511\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5880 - val_loss: 0.5471\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5836 - val_loss: 0.5433\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5795 - val_loss: 0.5396\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5755 - val_loss: 0.5362\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5718 - val_loss: 0.5328\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5682 - val_loss: 0.5297\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5647 - val_loss: 0.5265\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5614 - val_loss: 0.5236\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5582 - val_loss: 0.5207\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5552 - val_loss: 0.5181\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5522 - val_loss: 0.5154\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5494 - val_loss: 0.5130\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5466 - val_loss: 0.5108\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5441 - val_loss: 0.5085\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5415 - val_loss: 0.5062\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5392 - val_loss: 0.5041\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5369 - val_loss: 0.5021\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5347 - val_loss: 0.5001\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5327 - val_loss: 0.4984\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5307 - val_loss: 0.4966\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5287 - val_loss: 0.4948\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5268 - val_loss: 0.4931\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5250 - val_loss: 0.4914\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5233 - val_loss: 0.4899\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5216 - val_loss: 0.4884\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5201 - val_loss: 0.4873\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5184 - val_loss: 0.4859\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5167 - val_loss: 0.4843\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5154 - val_loss: 0.4830\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5139 - val_loss: 0.4818\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5126 - val_loss: 0.4806\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5113 - val_loss: 0.4795\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5100 - val_loss: 0.4784\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5088 - val_loss: 0.4774\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5075 - val_loss: 0.4764\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5062 - val_loss: 0.4755\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5051 - val_loss: 0.4745\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5039 - val_loss: 0.4734\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5027 - val_loss: 0.4723\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5018 - val_loss: 0.4715\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5006 - val_loss: 0.4705\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4995 - val_loss: 0.4695\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4986 - val_loss: 0.4686\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.4677\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4965 - val_loss: 0.4669\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4956 - val_loss: 0.4664\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4945 - val_loss: 0.4654\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4937 - val_loss: 0.4649\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4927 - val_loss: 0.4642\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4917 - val_loss: 0.4632\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4910 - val_loss: 0.4625\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4900 - val_loss: 0.4617\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4892 - val_loss: 0.4612\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4883 - val_loss: 0.4605\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4875 - val_loss: 0.4598\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4866 - val_loss: 0.4593\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4859 - val_loss: 0.4585\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4851 - val_loss: 0.4580\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4843 - val_loss: 0.4571\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4836 - val_loss: 0.4567\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4827 - val_loss: 0.4558\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4820 - val_loss: 0.4555\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4814 - val_loss: 0.4549\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4806 - val_loss: 0.4540\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4800 - val_loss: 0.4534\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4794 - val_loss: 0.4529\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4787 - val_loss: 0.4525\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4780 - val_loss: 0.4518\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4774 - val_loss: 0.4513\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4593\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 3.4803 - val_loss: 2.0017\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.6124 - val_loss: 1.1411\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0969 - val_loss: 0.9048\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9273 - val_loss: 0.8229\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8543 - val_loss: 0.7831\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8124 - val_loss: 0.7562\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7829 - val_loss: 0.7348\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7593 - val_loss: 0.7165\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7394 - val_loss: 0.6998\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7220 - val_loss: 0.6852\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7064 - val_loss: 0.6720\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6923 - val_loss: 0.6596\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6796 - val_loss: 0.6485\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6679 - val_loss: 0.6377\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6572 - val_loss: 0.6281\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6473 - val_loss: 0.6193\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6381 - val_loss: 0.6108\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6295 - val_loss: 0.6030\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6214 - val_loss: 0.5957\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6139 - val_loss: 0.5889\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6069 - val_loss: 0.5825\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6004 - val_loss: 0.5764\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5942 - val_loss: 0.5707\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5884 - val_loss: 0.5652\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5829 - val_loss: 0.5599\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5777 - val_loss: 0.5552\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5727 - val_loss: 0.5506\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5680 - val_loss: 0.5462\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5634 - val_loss: 0.5421\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5591 - val_loss: 0.5379\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5550 - val_loss: 0.5340\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5510 - val_loss: 0.5303\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5473 - val_loss: 0.5269\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5437 - val_loss: 0.5236\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5402 - val_loss: 0.5203\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5369 - val_loss: 0.5172\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5336 - val_loss: 0.5146\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5307 - val_loss: 0.5118\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5278 - val_loss: 0.5090\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5250 - val_loss: 0.5063\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5224 - val_loss: 0.5038\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5198 - val_loss: 0.5014\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5174 - val_loss: 0.4993\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5150 - val_loss: 0.4971\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5127 - val_loss: 0.4949\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5106 - val_loss: 0.4931\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5084 - val_loss: 0.4909\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5063 - val_loss: 0.4890\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5044 - val_loss: 0.4873\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5026 - val_loss: 0.4857\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5008 - val_loss: 0.4841\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4991 - val_loss: 0.4825\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4974 - val_loss: 0.4808\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4958 - val_loss: 0.4794\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4943 - val_loss: 0.4781\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4928 - val_loss: 0.4768\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.4755\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4899 - val_loss: 0.4742\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4887 - val_loss: 0.4732\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4873 - val_loss: 0.4718\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4860 - val_loss: 0.4706\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.4695\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4837 - val_loss: 0.4687\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4826 - val_loss: 0.4678\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4814 - val_loss: 0.4666\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4804 - val_loss: 0.4655\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4795 - val_loss: 0.4649\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4784 - val_loss: 0.4638\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4773 - val_loss: 0.4630\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4766 - val_loss: 0.4623\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4755 - val_loss: 0.4614\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4746 - val_loss: 0.4605\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4738 - val_loss: 0.4600\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4729 - val_loss: 0.4592\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4721 - val_loss: 0.4584\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4712 - val_loss: 0.4577\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4704 - val_loss: 0.4569\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4697 - val_loss: 0.4563\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4689 - val_loss: 0.4557\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4682 - val_loss: 0.4553\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4674 - val_loss: 0.4544\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4665 - val_loss: 0.4538\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4660 - val_loss: 0.4534\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4652 - val_loss: 0.4527\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4646 - val_loss: 0.4525\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4638 - val_loss: 0.4516\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4632 - val_loss: 0.4510\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4624 - val_loss: 0.4503\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4621 - val_loss: 0.4500\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4613 - val_loss: 0.4493\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4607 - val_loss: 0.4488\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4600 - val_loss: 0.4487\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4595 - val_loss: 0.4482\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4589 - val_loss: 0.4475\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4584 - val_loss: 0.4472\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4577 - val_loss: 0.4465\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4572 - val_loss: 0.4460\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4567 - val_loss: 0.4456\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4561 - val_loss: 0.4454\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4555 - val_loss: 0.4449\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4750\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 4ms/step - loss: 3.3183 - val_loss: 1.8425\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.5661 - val_loss: 1.0547\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.0159 - val_loss: 0.8112\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.8358 - val_loss: 0.7345\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7690 - val_loss: 0.7016\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7376 - val_loss: 0.6814\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7173 - val_loss: 0.6660\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7013 - val_loss: 0.6530\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6876 - val_loss: 0.6413\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6752 - val_loss: 0.6306\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6637 - val_loss: 0.6205\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6532 - val_loss: 0.6111\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6432 - val_loss: 0.6023\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6338 - val_loss: 0.5943\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6251 - val_loss: 0.5865\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6167 - val_loss: 0.5792\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6088 - val_loss: 0.5722\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6014 - val_loss: 0.5657\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5946 - val_loss: 0.5598\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5879 - val_loss: 0.5540\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5819 - val_loss: 0.5488\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5759 - val_loss: 0.5436\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5706 - val_loss: 0.5391\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5654 - val_loss: 0.5342\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5605 - val_loss: 0.5299\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5559 - val_loss: 0.5257\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5515 - val_loss: 0.5218\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5473 - val_loss: 0.5182\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5432 - val_loss: 0.5150\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5396 - val_loss: 0.5117\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5360 - val_loss: 0.5087\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5327 - val_loss: 0.5058\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5295 - val_loss: 0.5032\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.5006\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5237 - val_loss: 0.4981\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5210 - val_loss: 0.4958\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5185 - val_loss: 0.4936\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5161 - val_loss: 0.4917\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5138 - val_loss: 0.4899\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5117 - val_loss: 0.4880\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5097 - val_loss: 0.4863\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5077 - val_loss: 0.4849\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5059 - val_loss: 0.4833\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5040 - val_loss: 0.4817\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5025 - val_loss: 0.4804\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5009 - val_loss: 0.4792\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.4777\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4978 - val_loss: 0.4767\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4964 - val_loss: 0.4754\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4950 - val_loss: 0.4742\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4938 - val_loss: 0.4732\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4926 - val_loss: 0.4723\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4913 - val_loss: 0.4712\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4901 - val_loss: 0.4702\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4890 - val_loss: 0.4693\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4880 - val_loss: 0.4683\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4869 - val_loss: 0.4676\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4858 - val_loss: 0.4667\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4849 - val_loss: 0.4659\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4838 - val_loss: 0.4652\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4828 - val_loss: 0.4643\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4819 - val_loss: 0.4635\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4809 - val_loss: 0.4629\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4801 - val_loss: 0.4622\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4792 - val_loss: 0.4616\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4784 - val_loss: 0.4609\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4776 - val_loss: 0.4604\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4769 - val_loss: 0.4598\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4762 - val_loss: 0.4592\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4755 - val_loss: 0.4586\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4748 - val_loss: 0.4579\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4741 - val_loss: 0.4575\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4734 - val_loss: 0.4567\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4729 - val_loss: 0.4562\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4722 - val_loss: 0.4557\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4717 - val_loss: 0.4553\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4710 - val_loss: 0.4547\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4704 - val_loss: 0.4543\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4699 - val_loss: 0.4538\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4692 - val_loss: 0.4532\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4687 - val_loss: 0.4527\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4681 - val_loss: 0.4524\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4676 - val_loss: 0.4520\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4671 - val_loss: 0.4515\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4666 - val_loss: 0.4511\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4661 - val_loss: 0.4507\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4656 - val_loss: 0.4504\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4651 - val_loss: 0.4499\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4647 - val_loss: 0.4495\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4642 - val_loss: 0.4491\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4637 - val_loss: 0.4488\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4633 - val_loss: 0.4485\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4629 - val_loss: 0.4482\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4624 - val_loss: 0.4477\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4620 - val_loss: 0.4475\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4616 - val_loss: 0.4472\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4613 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4609 - val_loss: 0.4464\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4604 - val_loss: 0.4461\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4601 - val_loss: 0.4459\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4826\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0083 - val_loss: 0.5478\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5566 - val_loss: 0.4817\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5048 - val_loss: 0.4579\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4778 - val_loss: 0.4421\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4595 - val_loss: 0.4271\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4461 - val_loss: 0.4203\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4350 - val_loss: 0.4109\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4265 - val_loss: 0.4052\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4203 - val_loss: 0.4000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4122 - val_loss: 0.4071\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4063 - val_loss: 0.3923\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4023 - val_loss: 0.3956\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3992 - val_loss: 0.3872\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4005 - val_loss: 0.3882\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3899 - val_loss: 0.3805\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3850 - val_loss: 0.3754\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3829 - val_loss: 0.3772\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3786 - val_loss: 0.3753\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3740 - val_loss: 0.3714\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3729 - val_loss: 0.3668\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3704 - val_loss: 0.3685\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3678 - val_loss: 0.3680\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3643 - val_loss: 0.3610\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3611 - val_loss: 0.3592\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3643 - val_loss: 0.3561\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3582 - val_loss: 0.3536\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3579 - val_loss: 0.3532\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3518 - val_loss: 0.3508\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3511 - val_loss: 0.3483\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3548 - val_loss: 0.3495\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3512 - val_loss: 0.3480\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3449 - val_loss: 0.3428\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3416 - val_loss: 0.3418\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3443 - val_loss: 0.3412\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3425 - val_loss: 0.3404\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3396 - val_loss: 0.3372\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3376 - val_loss: 0.3375\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3357 - val_loss: 0.3476\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3361 - val_loss: 0.3423\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3340 - val_loss: 0.3331\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3328 - val_loss: 0.3327\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3306 - val_loss: 0.3283\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3301 - val_loss: 0.3359\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3310 - val_loss: 0.3282\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3271 - val_loss: 0.3272\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3308 - val_loss: 0.3357\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3229 - val_loss: 0.3456\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3253 - val_loss: 0.3240\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3261 - val_loss: 0.3389\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3211 - val_loss: 0.3245\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3235 - val_loss: 0.3237\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3199 - val_loss: 0.3247\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3286 - val_loss: 0.3209\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3221 - val_loss: 0.3204\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3196 - val_loss: 0.3381\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3173 - val_loss: 0.3251\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3165 - val_loss: 0.3196\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3144 - val_loss: 0.3208\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3138 - val_loss: 0.3171\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3163 - val_loss: 0.3199\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3134 - val_loss: 0.3325\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3132 - val_loss: 0.3137\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3122 - val_loss: 0.3168\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3125 - val_loss: 0.3151\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3114 - val_loss: 0.3144\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3108 - val_loss: 0.3155\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3089 - val_loss: 0.3236\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3099 - val_loss: 0.3360\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3083 - val_loss: 0.3132\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3065 - val_loss: 0.3076\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3070 - val_loss: 0.3162\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3063 - val_loss: 0.3192\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3066 - val_loss: 0.3088\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3057 - val_loss: 0.3057\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3058 - val_loss: 0.3098\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3056 - val_loss: 0.3187\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3053 - val_loss: 0.3135\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3036 - val_loss: 0.3096\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3046 - val_loss: 0.3207\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3042 - val_loss: 0.3070\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3019 - val_loss: 0.3167\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3014 - val_loss: 0.3058\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3038 - val_loss: 0.3044\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3002 - val_loss: 0.3040\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2998 - val_loss: 0.3013\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2996 - val_loss: 0.3003\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3007 - val_loss: 0.3084\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2975 - val_loss: 0.3082\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.3037\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2985 - val_loss: 0.3100\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2975 - val_loss: 0.3035\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2988 - val_loss: 0.3052\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2972 - val_loss: 0.2996\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2967 - val_loss: 0.2988\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2947 - val_loss: 0.3247\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2954 - val_loss: 0.2956\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2952 - val_loss: 0.3051\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2939 - val_loss: 0.3174\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2932 - val_loss: 0.2961\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2928 - val_loss: 0.3105\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.3167\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0339 - val_loss: 0.5449\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5424 - val_loss: 0.5093\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4997 - val_loss: 0.4657\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4732 - val_loss: 0.4504\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4573 - val_loss: 0.4385\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4446 - val_loss: 0.4315\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4325 - val_loss: 0.4180\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4241 - val_loss: 0.4210\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4158 - val_loss: 0.4123\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4097 - val_loss: 0.4010\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.3959\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3965 - val_loss: 0.4041\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3930 - val_loss: 0.3882\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3878 - val_loss: 0.3832\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3832 - val_loss: 0.3830\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3794 - val_loss: 0.3809\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3739 - val_loss: 0.3809\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3715 - val_loss: 0.3890\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3685 - val_loss: 0.3693\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3656 - val_loss: 0.3679\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3618 - val_loss: 0.3697\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3591 - val_loss: 0.3684\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3583 - val_loss: 0.3582\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3530 - val_loss: 0.3639\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3517 - val_loss: 0.3590\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3500 - val_loss: 0.3550\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3472 - val_loss: 0.3570\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3445 - val_loss: 0.3520\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3434 - val_loss: 0.3499\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3403 - val_loss: 0.3517\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3394 - val_loss: 0.3435\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3388 - val_loss: 0.3470\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3355 - val_loss: 0.3427\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3342 - val_loss: 0.3550\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3318 - val_loss: 0.3428\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3298 - val_loss: 0.3391\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3296 - val_loss: 0.3364\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3281 - val_loss: 0.3346\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3277 - val_loss: 0.3479\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3262 - val_loss: 0.3384\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3242 - val_loss: 0.3303\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3232 - val_loss: 0.3359\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3216 - val_loss: 0.3429\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3205 - val_loss: 0.3368\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3212 - val_loss: 0.3259\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3190 - val_loss: 0.3404\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3194 - val_loss: 0.3287\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3179 - val_loss: 0.3334\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3160 - val_loss: 0.3297\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3146 - val_loss: 0.3344\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3136 - val_loss: 0.3204\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3117 - val_loss: 0.3285\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3126 - val_loss: 0.3254\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3112 - val_loss: 0.3271\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3090 - val_loss: 0.3242\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3102 - val_loss: 0.3207\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3082 - val_loss: 0.3159\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3070 - val_loss: 0.3271\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3076 - val_loss: 0.3247\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3074 - val_loss: 0.3239\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3073 - val_loss: 0.3162\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3054 - val_loss: 0.3126\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3059 - val_loss: 0.3242\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3062 - val_loss: 0.3161\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3054 - val_loss: 0.3143\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3033 - val_loss: 0.3240\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3024 - val_loss: 0.3175\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3035 - val_loss: 0.3160\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3042 - val_loss: 0.3180\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3047 - val_loss: 0.3120\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3085 - val_loss: 0.3167\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3086 - val_loss: 0.3154\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3088 - val_loss: 0.3282\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3097 - val_loss: 0.3084\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3077 - val_loss: 0.4182\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3194 - val_loss: 0.3113\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3085 - val_loss: 0.3286\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3005 - val_loss: 0.3113\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2988 - val_loss: 0.3094\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2986 - val_loss: 0.3136\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2983 - val_loss: 0.3168\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2963 - val_loss: 0.3139\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.2941 - val_loss: 0.3129\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2963 - val_loss: 0.3107\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3403\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1068 - val_loss: 0.5959\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5919 - val_loss: 0.5191\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5220 - val_loss: 0.4820\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4873 - val_loss: 0.4567\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4649 - val_loss: 0.4594\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4514 - val_loss: 0.4315\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4408 - val_loss: 0.4220\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4316 - val_loss: 0.4162\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4228 - val_loss: 0.4112\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4179 - val_loss: 0.4080\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4110 - val_loss: 0.4067\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4059 - val_loss: 0.3988\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4007 - val_loss: 0.3994\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4120 - val_loss: 0.3954\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3935 - val_loss: 0.3912\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3910 - val_loss: 0.3891\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3877 - val_loss: 0.3869\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3832 - val_loss: 0.3810\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3783 - val_loss: 0.3806\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3831 - val_loss: 0.3788\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3719 - val_loss: 0.3731\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3678 - val_loss: 0.3732\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3785 - val_loss: 0.3709\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3630 - val_loss: 0.3652\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3590 - val_loss: 0.3642\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3559 - val_loss: 0.3661\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3570 - val_loss: 0.3675\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3535 - val_loss: 0.3564\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3497 - val_loss: 0.3580\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3500 - val_loss: 0.3538\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3885 - val_loss: 0.3506\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3504 - val_loss: 0.3522\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3452 - val_loss: 0.3471\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3401 - val_loss: 0.3489\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3382 - val_loss: 0.3494\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3361 - val_loss: 0.3435\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3351 - val_loss: 0.3508\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3351 - val_loss: 0.3399\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3322 - val_loss: 0.3404\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3317 - val_loss: 0.3377\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3297 - val_loss: 0.3402\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3295 - val_loss: 0.3364\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3253 - val_loss: 0.3372\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3284 - val_loss: 0.3487\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3246 - val_loss: 0.3333\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3503 - val_loss: 0.3333\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3221 - val_loss: 0.3300\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3229 - val_loss: 0.3303\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3213 - val_loss: 0.3307\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3172 - val_loss: 0.3361\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3180 - val_loss: 0.3364\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3155 - val_loss: 0.3368\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3154 - val_loss: 0.3280\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3157 - val_loss: 0.3319\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3152 - val_loss: 0.3354\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3137 - val_loss: 0.3224\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3111 - val_loss: 0.3308\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3098 - val_loss: 0.3278\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3123 - val_loss: 0.3231\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3094 - val_loss: 0.3236\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3087 - val_loss: 0.3201\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3059 - val_loss: 0.3310\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3086 - val_loss: 0.3201\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3068 - val_loss: 0.3194\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3196 - val_loss: 0.3619\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3078 - val_loss: 0.3210\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3039 - val_loss: 0.3192\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3016 - val_loss: 0.3203\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3029 - val_loss: 0.3130\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3014 - val_loss: 0.3224\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3003 - val_loss: 0.3137\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3031 - val_loss: 0.3090\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3005 - val_loss: 0.3094\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2980 - val_loss: 0.3108\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2989 - val_loss: 0.3148\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2968 - val_loss: 0.3053\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2940 - val_loss: 0.3117\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2954 - val_loss: 0.3077\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2938 - val_loss: 0.3062\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2936 - val_loss: 0.3208\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2940 - val_loss: 0.3089\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2945 - val_loss: 0.3108\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2998 - val_loss: 0.3163\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2945 - val_loss: 0.3053\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2951 - val_loss: 0.3063\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.2917 - val_loss: 0.3067\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3235\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 3.4348 - val_loss: 1.6353\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3438 - val_loss: 0.9622\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9226 - val_loss: 0.7710\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7902 - val_loss: 0.7068\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7435 - val_loss: 0.6755\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7164 - val_loss: 0.6559\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6966 - val_loss: 0.6407\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6801 - val_loss: 0.6270\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6656 - val_loss: 0.6152\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6534 - val_loss: 0.6051\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6416 - val_loss: 0.5961\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6308 - val_loss: 0.5859\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6210 - val_loss: 0.5782\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6117 - val_loss: 0.5699\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6028 - val_loss: 0.5630\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5946 - val_loss: 0.5559\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5868 - val_loss: 0.5491\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5795 - val_loss: 0.5430\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5728 - val_loss: 0.5371\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5663 - val_loss: 0.5315\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5601 - val_loss: 0.5263\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5542 - val_loss: 0.5221\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5490 - val_loss: 0.5164\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5434 - val_loss: 0.5123\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5388 - val_loss: 0.5079\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5341 - val_loss: 0.5038\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5297 - val_loss: 0.5001\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5256 - val_loss: 0.4966\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5217 - val_loss: 0.4932\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5181 - val_loss: 0.4899\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5148 - val_loss: 0.4869\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5116 - val_loss: 0.4840\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5086 - val_loss: 0.4811\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5056 - val_loss: 0.4789\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5028 - val_loss: 0.4762\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5002 - val_loss: 0.4738\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4978 - val_loss: 0.4715\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4953 - val_loss: 0.4694\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4931 - val_loss: 0.4674\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4906 - val_loss: 0.4652\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4886 - val_loss: 0.4637\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4867 - val_loss: 0.4616\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4846 - val_loss: 0.4598\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4828 - val_loss: 0.4584\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4811 - val_loss: 0.4566\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4795 - val_loss: 0.4552\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4779 - val_loss: 0.4538\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4766 - val_loss: 0.4525\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4752 - val_loss: 0.4512\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4737 - val_loss: 0.4508\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4724 - val_loss: 0.4494\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4713 - val_loss: 0.4481\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4700 - val_loss: 0.4470\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4690 - val_loss: 0.4462\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4678 - val_loss: 0.4452\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4667 - val_loss: 0.4445\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4656 - val_loss: 0.4430\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4649 - val_loss: 0.4426\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4636 - val_loss: 0.4413\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4628 - val_loss: 0.4410\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4618 - val_loss: 0.4400\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4610 - val_loss: 0.4394\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4599 - val_loss: 0.4391\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4591 - val_loss: 0.4371\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4582 - val_loss: 0.4371\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4574 - val_loss: 0.4359\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4566 - val_loss: 0.4351\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4557 - val_loss: 0.4352\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4552 - val_loss: 0.4335\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4545 - val_loss: 0.4325\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4534 - val_loss: 0.4325\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4526 - val_loss: 0.4314\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4519 - val_loss: 0.4308\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4510 - val_loss: 0.4305\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4504 - val_loss: 0.4296\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4497 - val_loss: 0.4287\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4490 - val_loss: 0.4282\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4481 - val_loss: 0.4275\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4474 - val_loss: 0.4268\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4466 - val_loss: 0.4264\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4459 - val_loss: 0.4260\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4453 - val_loss: 0.4251\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4445 - val_loss: 0.4242\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4435 - val_loss: 0.4239\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4432 - val_loss: 0.4231\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4422 - val_loss: 0.4230\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4419 - val_loss: 0.4221\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4411 - val_loss: 0.4213\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4403 - val_loss: 0.4208\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4400 - val_loss: 0.4202\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4392 - val_loss: 0.4194\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4386 - val_loss: 0.4192\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4382 - val_loss: 0.4183\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4373 - val_loss: 0.4187\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4368 - val_loss: 0.4174\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4363 - val_loss: 0.4170\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4358 - val_loss: 0.4164\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4353 - val_loss: 0.4157\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4345 - val_loss: 0.4157\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4342 - val_loss: 0.4153\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4216\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 2.7509 - val_loss: 1.4207\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.2069 - val_loss: 0.9547\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9148 - val_loss: 0.7977\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7982 - val_loss: 0.7271\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7381 - val_loss: 0.6850\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7009 - val_loss: 0.6571\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6757 - val_loss: 0.6378\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6579 - val_loss: 0.6242\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6439 - val_loss: 0.6136\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6330 - val_loss: 0.6037\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6237 - val_loss: 0.5956\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6157 - val_loss: 0.5889\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6089 - val_loss: 0.5814\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6022 - val_loss: 0.5770\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5966 - val_loss: 0.5686\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5912 - val_loss: 0.5644\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5858 - val_loss: 0.5590\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5808 - val_loss: 0.5533\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5767 - val_loss: 0.5494\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5721 - val_loss: 0.5464\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5682 - val_loss: 0.5412\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5642 - val_loss: 0.5389\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5605 - val_loss: 0.5342\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5568 - val_loss: 0.5306\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5530 - val_loss: 0.5268\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5495 - val_loss: 0.5237\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5460 - val_loss: 0.5203\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5425 - val_loss: 0.5176\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5391 - val_loss: 0.5148\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5355 - val_loss: 0.5108\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5322 - val_loss: 0.5106\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5295 - val_loss: 0.5068\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5262 - val_loss: 0.5043\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5233 - val_loss: 0.5000\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5201 - val_loss: 0.4969\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5172 - val_loss: 0.4943\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5143 - val_loss: 0.4917\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5112 - val_loss: 0.4883\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5084 - val_loss: 0.4875\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5055 - val_loss: 0.4840\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5030 - val_loss: 0.4816\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5001 - val_loss: 0.4789\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4972 - val_loss: 0.4772\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4946 - val_loss: 0.4738\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4919 - val_loss: 0.4717\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4893 - val_loss: 0.4697\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4868 - val_loss: 0.4678\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4841 - val_loss: 0.4647\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4817 - val_loss: 0.4630\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4792 - val_loss: 0.4599\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4769 - val_loss: 0.4580\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4746 - val_loss: 0.4574\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4724 - val_loss: 0.4552\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4704 - val_loss: 0.4531\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4685 - val_loss: 0.4520\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4666 - val_loss: 0.4501\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4647 - val_loss: 0.4494\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4627 - val_loss: 0.4469\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4613 - val_loss: 0.4463\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4598 - val_loss: 0.4451\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4584 - val_loss: 0.4436\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4568 - val_loss: 0.4435\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4555 - val_loss: 0.4417\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4542 - val_loss: 0.4415\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4529 - val_loss: 0.4403\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4517 - val_loss: 0.4385\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4505 - val_loss: 0.4382\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4494 - val_loss: 0.4373\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4485 - val_loss: 0.4360\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4476 - val_loss: 0.4357\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4467 - val_loss: 0.4351\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4459 - val_loss: 0.4339\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4450 - val_loss: 0.4332\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4443 - val_loss: 0.4328\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4436 - val_loss: 0.4324\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4429 - val_loss: 0.4314\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4422 - val_loss: 0.4311\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4415 - val_loss: 0.4310\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4409 - val_loss: 0.4301\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4401 - val_loss: 0.4294\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4396 - val_loss: 0.4288\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4392 - val_loss: 0.4288\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4387 - val_loss: 0.4291\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4382 - val_loss: 0.4276\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4376 - val_loss: 0.4278\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4372 - val_loss: 0.4275\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4367 - val_loss: 0.4277\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4362 - val_loss: 0.4263\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4357 - val_loss: 0.4256\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4355 - val_loss: 0.4253\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4351 - val_loss: 0.4251\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4346 - val_loss: 0.4256\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4343 - val_loss: 0.4248\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4338 - val_loss: 0.4253\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4333 - val_loss: 0.4250\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4331 - val_loss: 0.4238\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4327 - val_loss: 0.4233\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4325 - val_loss: 0.4236\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4320 - val_loss: 0.4240\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4314 - val_loss: 0.4246\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4458\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 5ms/step - loss: 3.3361 - val_loss: 1.6782\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.5224 - val_loss: 1.0731\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0156 - val_loss: 0.8006\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7764 - val_loss: 0.6710\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6738 - val_loss: 0.6161\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6306 - val_loss: 0.5904\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6097 - val_loss: 0.5764\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5968 - val_loss: 0.5657\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5868 - val_loss: 0.5571\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5781 - val_loss: 0.5502\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5709 - val_loss: 0.5428\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5642 - val_loss: 0.5364\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5582 - val_loss: 0.5311\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5520 - val_loss: 0.5252\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5469 - val_loss: 0.5206\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5417 - val_loss: 0.5161\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5374 - val_loss: 0.5122\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5331 - val_loss: 0.5088\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5291 - val_loss: 0.5047\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5250 - val_loss: 0.5014\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5214 - val_loss: 0.4989\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5176 - val_loss: 0.4958\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5146 - val_loss: 0.4941\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5116 - val_loss: 0.4917\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5088 - val_loss: 0.4894\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5065 - val_loss: 0.4876\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5046 - val_loss: 0.4858\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5025 - val_loss: 0.4844\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5005 - val_loss: 0.4825\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4988 - val_loss: 0.4812\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4970 - val_loss: 0.4803\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4955 - val_loss: 0.4786\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4941 - val_loss: 0.4776\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4927 - val_loss: 0.4765\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4914 - val_loss: 0.4756\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4903 - val_loss: 0.4742\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4890 - val_loss: 0.4745\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4884 - val_loss: 0.4731\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4873 - val_loss: 0.4716\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4864 - val_loss: 0.4715\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4856 - val_loss: 0.4702\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4846 - val_loss: 0.4692\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4839 - val_loss: 0.4689\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4832 - val_loss: 0.4679\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4823 - val_loss: 0.4672\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4817 - val_loss: 0.4666\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4807 - val_loss: 0.4670\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4804 - val_loss: 0.4654\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4797 - val_loss: 0.4647\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4792 - val_loss: 0.4640\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4785 - val_loss: 0.4635\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4779 - val_loss: 0.4629\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4773 - val_loss: 0.4621\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4765 - val_loss: 0.4616\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4762 - val_loss: 0.4612\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4755 - val_loss: 0.4606\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4750 - val_loss: 0.4605\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4743 - val_loss: 0.4598\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4739 - val_loss: 0.4590\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4731 - val_loss: 0.4585\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4726 - val_loss: 0.4585\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4720 - val_loss: 0.4576\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4715 - val_loss: 0.4569\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4709 - val_loss: 0.4565\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4705 - val_loss: 0.4564\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4697 - val_loss: 0.4557\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4694 - val_loss: 0.4555\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4688 - val_loss: 0.4555\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4683 - val_loss: 0.4547\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4678 - val_loss: 0.4542\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4673 - val_loss: 0.4537\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4668 - val_loss: 0.4541\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4665 - val_loss: 0.4532\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4659 - val_loss: 0.4526\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4655 - val_loss: 0.4522\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4650 - val_loss: 0.4518\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4646 - val_loss: 0.4516\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4641 - val_loss: 0.4522\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4635 - val_loss: 0.4513\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4633 - val_loss: 0.4503\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4627 - val_loss: 0.4499\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4622 - val_loss: 0.4499\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4617 - val_loss: 0.4494\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4611 - val_loss: 0.4485\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4607 - val_loss: 0.4486\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4601 - val_loss: 0.4477\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4596 - val_loss: 0.4470\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4589 - val_loss: 0.4464\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4584 - val_loss: 0.4461\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.4578 - val_loss: 0.4457\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4573 - val_loss: 0.4451\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4568 - val_loss: 0.4446\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4562 - val_loss: 0.4439\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4557 - val_loss: 0.4437\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4552 - val_loss: 0.4431\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4547 - val_loss: 0.4429\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4542 - val_loss: 0.4421\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4537 - val_loss: 0.4414\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4529 - val_loss: 0.4409\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4526 - val_loss: 0.4405\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4724\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [-0.59213199 -0.53922822 -0.4573759          nan -0.54067796 -0.3409718\n",
      " -0.42803406 -0.47229101 -0.32681732 -0.44658858]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 0.8175 - val_loss: 0.5385\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5595 - val_loss: 0.4787\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4937 - val_loss: 0.4514\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4669 - val_loss: 0.4469\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4481 - val_loss: 0.4283\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4356 - val_loss: 0.4155\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4286 - val_loss: 0.4089\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4144 - val_loss: 0.4009\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4063 - val_loss: 0.3944\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3992 - val_loss: 0.3917\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4113 - val_loss: 0.3866\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3835 - val_loss: 0.3770\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3805 - val_loss: 0.3737\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3741 - val_loss: 0.3682\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3687 - val_loss: 0.3784\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3655 - val_loss: 0.3700\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3603 - val_loss: 0.3580\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3547 - val_loss: 0.3552\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3521 - val_loss: 0.3514\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3555 - val_loss: 0.3478\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3572 - val_loss: 0.3482\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3442 - val_loss: 0.3398\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3418 - val_loss: 0.3456\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3396 - val_loss: 0.3384\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3392 - val_loss: 0.3406\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3347 - val_loss: 0.3316\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3370 - val_loss: 0.3344\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3325 - val_loss: 0.3282\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3307 - val_loss: 0.3378\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3276 - val_loss: 0.3288\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3285 - val_loss: 0.3385\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3230 - val_loss: 0.3260\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3270 - val_loss: 0.3239\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3224 - val_loss: 0.3241\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3206 - val_loss: 0.3281\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3199 - val_loss: 0.3235\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3190 - val_loss: 0.3206\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3174 - val_loss: 0.3152\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3166 - val_loss: 0.3124\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 3s 9ms/step - loss: 0.3126 - val_loss: 0.3216\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3120 - val_loss: 0.3139\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3157 - val_loss: 0.3126\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3113 - val_loss: 0.3191\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3110 - val_loss: 0.3087\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3106 - val_loss: 0.3219\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3079 - val_loss: 0.3126\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3071 - val_loss: 0.3032\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3053 - val_loss: 0.3037\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.3058 - val_loss: 0.3038\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3058 - val_loss: 0.3028\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3025 - val_loss: 0.3132\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.3024 - val_loss: 0.2978\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3057 - val_loss: 0.3081\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 3s 8ms/step - loss: 0.3039 - val_loss: 0.3081\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3005 - val_loss: 0.3068\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3016 - val_loss: 0.3089\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3173 - val_loss: 0.3042\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3050 - val_loss: 0.3005\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2998 - val_loss: 0.2959\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2978 - val_loss: 0.2987\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.2983 - val_loss: 0.2977\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.2987 - val_loss: 0.2958\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2944 - val_loss: 0.3012\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2939 - val_loss: 0.2944\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2947 - val_loss: 0.2958\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2938 - val_loss: 0.2926\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2939 - val_loss: 0.2978\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.2929 - val_loss: 0.2943\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2894 - val_loss: 0.2917\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2930 - val_loss: 0.2891\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2918 - val_loss: 0.2926\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2940 - val_loss: 0.2961\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2912 - val_loss: 0.3011\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2905 - val_loss: 0.2923\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2899 - val_loss: 0.2978\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2905 - val_loss: 0.2943\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2891 - val_loss: 0.2907\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2904 - val_loss: 0.2908\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2896 - val_loss: 0.2929\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.2884 - val_loss: 0.2912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001C9F31CAD00>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001C9F31CA8E0>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.006997198455624844, 'n_hidden': 2, 'n_neurons': 44}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3268173237641652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is very time consuming and not the best use of time. Other techniques can help us explore a search space much more efficient than randomly. The coreidea is that when a region of space turns out to be good, it should be explored more. Such techniques take care of the zooming process for us and lead to better solutions in less time.\n",
    "- Hyperopt\n",
    "- Hyperas\n",
    "- Keras Tuner\n",
    "- Scikit-Optimize\n",
    "- Spearmint\n",
    "- Hyperband\n",
    "- Sklearn-Deap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use many hidden layers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex problems, deep networks have a much higher parameter efficiency than shallow ones. This means that they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data is often structured in a hierarchical way, and deep neural networks automatically take advantage of this fact\n",
    "- Lower hidden layers model low-level structures (eg: line segments of various shapes and orientations)\n",
    "- Intermediate hidden layers combine these low-level structures to model intermediate-level structures (eg: squares, circles)\n",
    "- Highst hidden layers and output layer combine these intermediate structures to model high-level structures (eg: faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage of using DNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converge faster to a good solution\n",
    "- Improves ability to generalize to new datasets\n",
    "- Make use of **transfer learning** - reusing lower layers of the first network. Instead of randomly initializing weights and biases of the first few layers, we can initialize them to the values of the weights and biases of the lower layers of the first network. This way, the network will not have to learn from scratch all the low-level structures that occur in most pictures and only have to learn higher-level structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For many problems, start with just one or two hidden layers\n",
    "- For complex problems, can ramp up the number of hidden layers until we start overfitting the training set\n",
    "- Complex tasks such as image classification or speech recognition require networks with dozens of layers and large amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Number of Neurons per Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of neurons in input and output layer is determined by the type of input and output our task requires. (Eg: MNIST task requires 28x28=784 input neurons and 10 output neurons)\n",
    "\n",
    "- For hidden layers, it is common to size them to form a pyramid (fewer and fewer neurons at each layer). This is beacuse many low-level features can coalesce into far fewer high-level features.\n",
    "\n",
    "- We can try increasing the number of neurons gradually until the network starts overfitting. In pratice, it is simpler and more efficient to pick a model with more layers and neurons than we need then use early stopping and other regularization techniques to prevent it from overfitting.\n",
    "\n",
    "- In general, increasing number of layers instead of number of neurons can attain better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most important hyperparmeter\n",
    "- In general, the most optimal learning rate is about half of the maximum learning rate (i.e the learning rate above which the training algorithm diverges)\n",
    "- A good way to find good learning rate is to train the model for a few hundred iterations, starting with very low learning rate (eg $10^{-5}$) and gradually increase it to large very (eg: 10). This is done by multiplying the learning rate by a constant factor at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing a better optimizer than Mini-batch Gradient Descent is quite important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reseachers and practitioners recommend using largest batch size that can fit in GPU RAM. \n",
    "- But large batch size model may not generalize as well as model trained with small batch size.\n",
    "- Using small batches (from 2 to 32) is more preferable because small batches led to better models in less training time.\n",
    "- But it is possible to use very large batch sizes using various techniques such as warming up the learning rate.\n",
    "- Our strategy is to try use large batch size, using learning rate warmup and if training is unstable or final performance is disappointing, then we try using a small batch size instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general ReLU activation function is a good default for all hidden layers. \n",
    "- For output layer, it depends on our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 Number of Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In most case, the number of training iterations does not actually need to be tweaked.\n",
    "- We can just use EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5db5e69cbba06ac89a1590ec73231baad4b2563d81e8260f8e57174d6de7b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
