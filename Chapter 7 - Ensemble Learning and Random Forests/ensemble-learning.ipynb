{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Leaning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that if we aggegate the predictions of a group of predictors such as classifiers or regressors, we will often get better predictions than with the best individual predictor. A group of predictors is called an *ensemble* and this technique is called Ensemble Learning. An Ensemble Learning algorithm is called an Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of Ensemble Method is the Random Forest. We train a group of Decision Tree classifiers, each on a different random subset of the training set and take the majority vote class as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.1](images/figure7.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an even better classifier, we can aggregate the predictions of each classifier and predict the class that gets the most votes. This is called a hard voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.2](images/figure7.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even if each classifier is a weak learner (does only slightly better than randomly guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse\n",
    "\n",
    "- Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors hence improving the ensemble's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf_clf', RandomForestClassifier()),\n",
       "                             ('svc', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf),\n",
    "                ('rf_clf', rf_clf),\n",
    "                ('svc', svm_clf)],\n",
    "    voting=\"hard\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indivdual classifier's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilties, we can predict the class with the highest class probability, averaged over all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft voting classifier often achieves higher performance than hard voting because it gives more weight to highly confident votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(probability=True, random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42) \n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bagging (Bootstrap Aggregating)**: Sampling with replacement \n",
    "- **Pasting**: Sampling without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.4](images/figure7.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble can make a prediction for a new instance by aggregating the predictions of all predcitors.  \n",
    "- Classification: Majority Vote\n",
    "- Regression: Average value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating reduces both bias and variance. The net result is that the ensemble has a similar bias but a lower variance than a single predcitor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Classifier automatically performs soft voting instead of hard voting if the base classifier can estimate probabiltiies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier( \n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    max_samples=100, bootstrap=True, n_jobs=-1) # for pasting, bootstrap=False. n_jobs=-1 uses all CPU cores\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances without the need for a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred) # close enough to the oob score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36507937, 0.63492063],\n",
       "       [0.38043478, 0.61956522],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14942529, 0.85057471],\n",
       "       [0.32960894, 0.67039106],\n",
       "       [0.01522843, 0.98477157],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.73023256, 0.26976744],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.8423913 , 0.1576087 ],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.07142857, 0.92857143],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98039216, 0.01960784],\n",
       "       [0.96089385, 0.03910615],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.04891304, 0.95108696],\n",
       "       [0.37755102, 0.62244898],\n",
       "       [0.92771084, 0.07228916],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97350993, 0.02649007],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68648649, 0.31351351],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18324607, 0.81675393],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41116751, 0.58883249],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.25128205, 0.74871795],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02808989, 0.97191011],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.89830508, 0.10169492],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [0.96585366, 0.03414634],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05464481, 0.94535519],\n",
       "       [0.97860963, 0.02139037],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.77011494, 0.22988506],\n",
       "       [0.43715847, 0.56284153],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.69767442, 0.30232558],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84530387, 0.15469613],\n",
       "       [1.        , 0.        ],\n",
       "       [0.63687151, 0.36312849],\n",
       "       [0.14634146, 0.85365854],\n",
       "       [0.66161616, 0.33838384],\n",
       "       [0.9047619 , 0.0952381 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.17877095, 0.82122905],\n",
       "       [0.86979167, 0.13020833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03465347, 0.96534653],\n",
       "       [0.03108808, 0.96891192],\n",
       "       [0.32386364, 0.67613636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.85263158, 0.14736842],\n",
       "       [0.00483092, 0.99516908],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23728814, 0.76271186],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93969849, 0.06030151],\n",
       "       [0.79227053, 0.20772947],\n",
       "       [0.0049505 , 0.9950495 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20689655, 0.79310345],\n",
       "       [0.61827957, 0.38172043],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05202312, 0.94797688],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01694915, 0.98305085],\n",
       "       [0.99390244, 0.00609756],\n",
       "       [0.26237624, 0.73762376],\n",
       "       [0.54347826, 0.45652174],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [0.9950495 , 0.0049505 ],\n",
       "       [0.28571429, 0.71428571],\n",
       "       [0.86868687, 0.13131313],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85227273, 0.14772727],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98181818, 0.01818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.953125  , 0.046875  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01538462, 0.98461538],\n",
       "       [0.27918782, 0.72081218],\n",
       "       [0.95833333, 0.04166667],\n",
       "       [0.28494624, 0.71505376],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70760234, 0.29239766],\n",
       "       [0.36627907, 0.63372093],\n",
       "       [0.45454545, 0.54545455],\n",
       "       [0.83977901, 0.16022099],\n",
       "       [0.94623656, 0.05376344],\n",
       "       [0.06629834, 0.93370166],\n",
       "       [0.84065934, 0.15934066],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0199005 , 0.9800995 ],\n",
       "       [0.95897436, 0.04102564],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03414634, 0.96585366],\n",
       "       [0.00510204, 0.99489796],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9558011 , 0.0441989 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32369942, 0.67630058],\n",
       "       [0.2392638 , 0.7607362 ],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25698324, 0.74301676],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01219512, 0.98780488],\n",
       "       [0.63636364, 0.36363636],\n",
       "       [0.91666667, 0.08333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05128205, 0.94871795],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03821656, 0.96178344],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03867403, 0.96132597],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.95505618, 0.04494382],\n",
       "       [0.74331551, 0.25668449],\n",
       "       [0.60294118, 0.39705882],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12777778, 0.87222222],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92553191, 0.07446809],\n",
       "       [0.96842105, 0.03157895],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.        , 1.        ],\n",
       "       [0.35869565, 0.64130435],\n",
       "       [0.9039548 , 0.0960452 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00995025, 0.99004975],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92655367, 0.07344633],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25925926, 0.74074074],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98342541, 0.01657459],\n",
       "       [0.77777778, 0.22222222],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12068966, 0.87931034],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.02604167, 0.97395833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05376344, 0.94623656],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76608187, 0.23391813],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90954774, 0.09045226],\n",
       "       [1.        , 0.        ],\n",
       "       [0.15662651, 0.84337349],\n",
       "       [0.22916667, 0.77083333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.1827957 , 0.8172043 ],\n",
       "       [0.96315789, 0.03684211],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98224852, 0.01775148],\n",
       "       [0.        , 1.        ],\n",
       "       [0.52542373, 0.47457627],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14619883, 0.85380117],\n",
       "       [0.0862069 , 0.9137931 ],\n",
       "       [0.97714286, 0.02285714],\n",
       "       [0.04      , 0.96      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38728324, 0.61271676],\n",
       "       [0.1299435 , 0.8700565 ],\n",
       "       [0.55675676, 0.44324324],\n",
       "       [0.60427807, 0.39572193],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58602151, 0.41397849],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23295455, 0.76704545],\n",
       "       [0.83248731, 0.16751269],\n",
       "       [0.07      , 0.93      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82080925, 0.17919075],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12290503, 0.87709497],\n",
       "       [0.01219512, 0.98780488],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.90229885, 0.09770115],\n",
       "       [0.15217391, 0.84782609],\n",
       "       [0.96703297, 0.03296703],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.58045977, 0.41954023],\n",
       "       [0.07009346, 0.92990654],\n",
       "       [0.98429319, 0.01570681],\n",
       "       [0.80729167, 0.19270833],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95212766, 0.04787234],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22916667, 0.77083333],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87165775, 0.12834225],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7173913 , 0.2826087 ],\n",
       "       [0.96174863, 0.03825137],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70175439, 0.29824561],\n",
       "       [0.57837838, 0.42162162],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.90547264, 0.09452736],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.85106383, 0.14893617],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78804348, 0.21195652],\n",
       "       [0.0875    , 0.9125    ],\n",
       "       [0.47643979, 0.52356021],\n",
       "       [0.22994652, 0.77005348],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88888889, 0.11111111],\n",
       "       [0.85632184, 0.14367816],\n",
       "       [0.00483092, 0.99516908],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02525253, 0.97474747],\n",
       "       [0.97237569, 0.02762431],\n",
       "       [0.92670157, 0.07329843],\n",
       "       [1.        , 0.        ],\n",
       "       [0.56565657, 0.43434343],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.02898551, 0.97101449],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98876404, 0.01123596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08823529, 0.91176471],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99497487, 0.00502513],\n",
       "       [0.00613497, 0.99386503],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14367816, 0.85632184],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44382022, 0.55617978],\n",
       "       [0.05294118, 0.94705882],\n",
       "       [0.25988701, 0.74011299],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98958333, 0.01041667],\n",
       "       [0.22404372, 0.77595628],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97948718, 0.02051282],\n",
       "       [0.32642487, 0.67357513],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03389831, 0.96610169],\n",
       "       [0.99038462, 0.00961538],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.72413793, 0.27586207]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier class supports sampling the features. Sampling is controlled by two hyperparameters: `max_features` and `bootstrap-features`\n",
    "\n",
    "- **Random Patches**: Sampling both training instances and features\n",
    "- **Random Subspaces**: Keeping all training instances but sampling features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is an ensemble of Decision Trees, generally trained via the bagging method.\n",
    "\n",
    "Random Forest algorithm produces extra randomness when growing trees.    \n",
    "- At each node, only a random subset of features is considered for splitting.\n",
    "- This results in greater tree diversity which trades for higher bias for a lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Extremely Randomized Trees (Extra-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make trees even more random by using random thresholds for each feature rather than searching for the best possible thresholds.\n",
    "\n",
    "- This technique trades more bias for a lower variance.\n",
    "- Much faster algorithm to train than regular Random Forests because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are very handy to get a quick understanding of what features matter, if we need to perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.11249225099876375\n",
      "sepal width (cm) 0.02311928828251033\n",
      "petal length (cm) 0.4410304643639577\n",
      "petal width (cm) 0.4233579963547682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important features are the petal length (44%) and width (42%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11249225, 0.02311929, 0.44103046, 0.423358  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting**: refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of boosting methods: \n",
    "- AdaBoost (Adaptive Boosting)\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost**: Pay more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost can be used for classification and regression   \n",
    "- AdaBoostClassifier()\n",
    "- AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.8](images/figure7.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the AdaBoost classifier,  \n",
    "- algorithm first rains a base classifier (such as decision tree) and uses it to make predictions on the training set.\n",
    "- then the algorithm increases the relative weight of misclassified training instances.\n",
    "- then it trains a second classifier, using the updated weights and make predictions on the training set and updates weights and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback to this sequential learning technique is that it cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. Hence it does not scale as well as bagging or pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn uses a multiclass version of AdaBoost called SAMME (Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "- Scikit-Learn can use a variant of SAMME called SAMME.R (R=\"real\") which relies on class probabilities rather than predictions and generally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we train an AdaBoost classifier based on 200 Decision stumps (max_depth=1). Decision stump tree is composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If AdaBoost ensemble is overfitting, reduce number of estimators or more strongly regularizeing the base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**: Sequentially adding predictors to an ensemble, each one corresponding its predecessor. However instead of tweaking the instance weight at every iteration like AdaBoost, Gradient Boosting tries to fit the new predictor to the residual errors made by the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting also works with regression tasks  \n",
    "- Gradient Tree Boosting\n",
    "- Gradient Boosted Regression Trees (GBRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a second DecisionTreeRegressor on the residual errors made by the first predcitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a third DecisionTreeRegressor on the residual errors made by the fsecondirst predcitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an ensemble containing three trees, we can make predictions on a new instance by adding up the predictions of all the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Finding optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=85)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, len(errors) + 1), errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Error\", fontsize=16)\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping with some patience (interrupts training only after there's no improvement for 5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 0.002750279033345716\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizd implementation of Gradient Boosting. \n",
    "- It aims to be extremely fast, scalable and portable. \n",
    "- XGBoost automatically take care of early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.22055\n",
      "[1]\tvalidation_0-rmse:0.16547\n",
      "[2]\tvalidation_0-rmse:0.12243\n",
      "[3]\tvalidation_0-rmse:0.10044\n",
      "[4]\tvalidation_0-rmse:0.08467\n",
      "[5]\tvalidation_0-rmse:0.07344\n",
      "[6]\tvalidation_0-rmse:0.06728\n",
      "[7]\tvalidation_0-rmse:0.06383\n",
      "[8]\tvalidation_0-rmse:0.06125\n",
      "[9]\tvalidation_0-rmse:0.05959\n",
      "[10]\tvalidation_0-rmse:0.05902\n",
      "[11]\tvalidation_0-rmse:0.05852\n",
      "[12]\tvalidation_0-rmse:0.05844\n",
      "[13]\tvalidation_0-rmse:0.05801\n",
      "[14]\tvalidation_0-rmse:0.05747\n",
      "[15]\tvalidation_0-rmse:0.05772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\xgboost\\sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], \n",
    "            early_stopping_rounds=2)\n",
    "\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stacking (Stacked Generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the idea that instead of using trivial functions such as hard voting to aggregate the predictions of all predictors in an ensemble, we train a model to perform this aggregation.\n",
    "- Each of the predictors predicts a different value (3.1, 2.7, 2.9), and the final predcitor (blender or metal learner) takes these predictions and as inputs and makes the final prediction (3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.12](images/figure7.12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the blender, we use hold-out set.  \n",
    "- First we split training data into two subsets. First subset is used to train the predictors in the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.13](images/figure7.13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use second subset as the hold-out set where we evaluate on.\n",
    "- We have three predicted values.\n",
    "- We can create a new training set using these predicted values as input features and keeping the target values.\n",
    "- The blender is trained on this new training set so it leans to predict the target value, given the first layer's predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure7.14](images/figure7.14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb5db5e69cbba06ac89a1590ec73231baad4b2563d81e8260f8e57174d6de7b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
